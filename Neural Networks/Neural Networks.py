# -*- coding: utf-8 -*-
"""Chapter_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XYAOczTF1GmWf-0Q-8YteeKI6OpMf_eC

## Initial Setup
"""

import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

import seaborn as sns
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

np.random.seed(42)
torch.manual_seed(42)

"""## Activation Functions

"""

def sigmoid(z):
  """
  Sigmoid activation function: σ(z) = 1 / (1 + e^(-z))
  Maps any real value to the range (0,1)
  """

  return 1/(1+np.exp(-z))

def relu(z):
  """
  Rectified Linear Unit: ReLU(z) = max(0, z)
  Returns z if positive, 0 otherwise
  """
  return(np.maximum(z,0))


def tanh(z):
  """
  Hyperbolic tangent activation: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
  Maps any real value to the range (-1,1)
  """

  return ((np.exp(z) - np.exp(-z))/(np.exp(z)+np.exp(-z)))



print(round(tanh(5),15) == round(np.tanh(5),15))
print(round(tanh(5),16) == round(np.tanh(5),16)) ## np.tanh has more floating points

"""### Visualization"""

z = np.linspace(-10, 10, 1000)

plt.figure(figsize=(15,4))

# Sigmoid
plt.subplot(1,3,1) # *nrows*, *ncols*, *index: 1 row, 3 cols, position 1
plt.plot(z,sigmoid(z),linewidth=2)
plt.grid(True)
plt.xlabel("z = wx+b")
plt.ylabel("sigmoid(z)")
plt.title("Sigmoid Function")

# ReLU
plt.subplot(1,3,2)
plt.plot(z, relu(z), linewidth=2)
plt.grid(True)
plt.xlabel("z = wx+b")
plt.ylabel("ReLU(z)")
plt.title("ReLU Function")

# Hypebolic Tangent
plt.subplot(1,3,3)
plt.plot(z, tanh(z), linewidth=2)
plt.grid(True)
plt.xlabel("z = wx+b")
plt.ylabel("tanh(z)")
plt.title("Hyperbolic Tangent Function")

plt.tight_layout() # Adjust the padding between and around subplots


plt.show()

"""## Derivatives of Activation Functions"""

def sigmoid_derivative(z):
  """Derivative of sigmoid: σ'(z) = σ(z)(1-σ(z))"""
  s = sigmoid(z)

  return(s * (1-s))

def tanh_derivative(z):
  """Derivative of tanh: tanh'(z) = 1 - tanh²(z)"""

  return(1 - np.tanh(z)**2)

def relu_derivative(z):
  """Derivative of ReLU: 1 if z > 0, 0 otherwise"""

  return(z > 0).astype(float)

plt.figure(figsize=(15,4)) # 5 each graph

# Sigmoid derivative
plt.subplot(1,3,1) # *nrows*, *ncols*, *index: 1 row, 3 cols, position 1
plt.plot(z, sigmoid_derivative(z), linewidth=2)
plt.title("Sigmoid Derivative")
plt.grid(True)


# ReLU derivative
plt.subplot(1,3,2)
plt.plot(z, relu_derivative(z), linewidth=2)
plt.title("ReLU Derivative")
plt.grid(True)


# tanh derivative
plt.subplot(1,3,3)
plt.plot(z, tanh_derivative(z), linewidth=2)
plt.title("Hyperbolic Tangent Derivative")
plt.grid(True)

plt.tight_layout()
plt.show()

"""#Neural Unit"""

class NeuralUnit:
  """
  Single Neural Unit that computes: y = activation(w.x + b)
  """

  def __init__(self, n_inputs, activation='sigmoid'):
    self.weights = np.random.randn(n_inputs) * 0.1 # Must not be zero, if not weights learn nothing. Diff from LogReg
    self.bias = 0.0

    # set activation function
    self.activation_name = activation

    if activation == 'sigmoid':
      self.activation = sigmoid
      self.activation_derivative = sigmoid_derivative

    elif activation == 'tanh':
      self.activation = tanh
      self.activation_derivative = tanh_derivative

    elif activation == 'relu':
      self.activation = relu
      self.activation_derivative = relu_derivative


  def forward(self, x):
    """Forward pass: compute output given input x"""

    # Linear combination: z = w·x + b
    self.z = np.dot(x, self.weights) + self.bias

    # Activation function
    self.output = self.activation(self.z)

    return(self.output)


  def __repr__(self):
    return f"NeuralUnit(weights={self.weights}, bias={self.bias:.3f}, activation={self.activation_name})"

neural_unit = NeuralUnit(n_inputs=10, activation = 'sigmoid')

x = np.random.randn(10)

z = neural_unit.forward(x)

print(f"Input: {x}")
print(f"Weights: {neural_unit.weights}")
print(f"Bias: {neural_unit.bias}")
print(f"z = w·x + b = {neural_unit.z:.3f}")
print(f"Input shape: {x.shape}")
print(f"Output: {z}")

"""# The XOR Problem

## Perceptron
### Demonstrating that a single perceptron cannot solve XOR
"""

def perceptron(x, w, b):
  """Simple perceptron with step activation"""
  z = np.dot(x,w) + b

  return(1 if z > 0 else 0 )


# XOR Truth Table
X_xor = np.array([[0,0], [0,1], [1,0], [1,1]]) # the 4 possible combinations

y_xor = np.array([0,1,1,0]) # XOR outputs

# AND Truth Table
y_and = np.array([0,0,0,1]) # AND outputs

# OR Truth Table
y_or = np.array([0,1,1,1]) # OR outputs


# Visualize problem
fig, axes = plt.subplots(1,3, figsize=(15,5) ) # 1 row, 3 cols

for ax, y, title in zip(axes, [y_and, y_or, y_xor], ['AND', 'OR', 'XOR']):
  # Plot points
  for i in range(4):
    color = 'red' if y[i] == 1 else 'blue'
    marker = 'o' if y[i] == 1 else 'x'
    ax.scatter(X_xor[i, 0], X_xor[i, 1], c=color, s=200, marker=marker)

  ax.set_xlim(-.5,1.5)
  ax.set_ylim(-.5,1.5)
  ax.set_ylabel('x2')
  ax.set_title(f'{title} Function')
  ax.grid(True, alpha=0.3)

  # Adding Decision Boundaries
  if title=='AND':
    # Decision boundary: x1 + x2 - 1.5 = 0
    x_line = np.linspace(-0.5, 1.5, 100)
    y_line = 1.5 - x_line
    ax.plot(x_line, y_line, 'g--', label='Decision boundary')
  elif title == 'OR':
    # Decision boundary: x1 + x2 - 0.5 = 0
    x_line = np.linspace(-0.5, 1.5, 100)
    y_line = 0.5 - x_line
    ax.plot(x_line, y_line, 'g--', label='Decision boundary')

  if title != 'XOR':
    ax.legend()

plt.show()

"""## Notes about `@` vs `np.dot()` vs `*`

The `@` operator in Python/NumPy is the **matrix multiplication operator**:


```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
v = np.array([1, 2])

# Matrix multiplication
A @ B       # Same as np.matmul(A, B)
np.dot(A, B)  # Also matrix multiplication for 2D arrays

# Element-wise multiplication  
A * B       # Different! Multiplies corresponding elements
```

## How `@` Behaves with Different Dimensions

### 1. **Matrix × Matrix** → Matrix multiplication (Row × Column)
```python
[[1, 2]    [[5, 6]     [[19, 22]
 [3, 4]] @  [7, 8]]  =  [43, 50]]
```

### 2. **Matrix × Vector** → Matrix-vector multiplication
```python
[[1, 2]    [5]     [1×5 + 2×6]     [17]
 [3, 4]] @ [6]  =  [3×5 + 4×6]  =  [39]
```

### 3. **Vector × Vector** → Dot product (scalar)
```python
[1, 2] @ [3, 4] = 1×3 + 2×4 = 11  # Returns scalar!
```

### 4. **Inner vs Outer Products**
**Row × Column: (1,n) @ (n,1) → scalar (dot/inner product)**  
**Column × Row: (m,1) @ (1,n) → (m,n) matrix (outer product)**

## Solving XOR with Neural Network
"""

class XORNetwork:
  """
  2-layer network to solve the XOR Problem
  2 inputs -> 2 hidden units -> 1 output
  """

  def __init__(self):
    # Hidden layer weights (example from the book originally from Bengio)

    """
    NumPy's @ operator (or np.dot) automatically treats the 1D array x as a column vector:
    [[1, 1]    [[0]     [[1×0 + 1×0]     [[0]
    [1, 1]] @  [0]]  =  [1×0 + 1×0]]  =  [0]]

    For input [0, 0]:
    Row 1: 1×0 + 1×0 = 0
    Row 2: 1×0 + 1×0 = 0
    Result: [0, 0]

    For input [1, 1]:
    Row 1: 1×1 + 1×1 = 2
    Row 2: 1×1 + 1×1 = 2
    Result: [2, 2]
    """

    self.W1 = np.array([[1,1],   # weights for h1
                        [1,1]])  # weights for h2

    self.b1 = np.array([0,-1])

    self.W2 = np.array([[1,-2]])
    self.b2 = np.array([0])

  def forward(self,x):
    # Hidden layer with ReLU

    self.z1 = np.dot(self.W1,x) + self.b1  # NumPy's @ operator (or np.dot) automatically treats the 1D array x as a column vector
    print(f"z1 = {self.W1}@{x} + {self.b1}")

    # W1 @ x: (2,2) @ (2,) → (2,) - returns a 1D array
    # Example: [[1,1],[1,1]] @ [1,1] → [2, 2] (vector)


    self.h = relu(self.z1)
    print(f"\nh = {self.h}")

    # Output layer (linear)
    self.z2 = np.dot(self.W2, self.h) + self.b2
    print(f"z2 = {self.W2}@{self.h} + {self.b2}")

    # W2 @ h: (1,2) @ (2,) → (1,) - returns a 1D array with one element
    # Example: [[1,-2]] @ [2,1] → [0] (


    print(f"z2 = {self.z2}")
    self.y = self.z2[0]   #  We use the [0] to extract the scalar from the array
    print("\n")
    return(self.y)

  def predict(self,x):
    """Binary prediction with threshold at 0"""

    return (self.forward(x) > 0).astype(int)

"""### Test XOR network

"""

xor_net = XORNetwork()

print("XOR Network Predictions:")
print("-" * 30)
for i, x in enumerate(X_xor):
  prediction = xor_net.predict(x)
  print(f"Input: {x} -> Predicted: {prediction}, True: {y_xor[i]}\n\n")

# Visualize hidden layer representations
hidden_representations = []
for x in X_xor:
  xor_net.forward(x)
  hidden_representations.append(xor_net.h)

hidden_representations = np.array(hidden_representations)

plt.figure(figsize=(10, 5))

# Original space
plt.subplot(1, 2, 1)
for i in range(4):
  color = 'red' if y_xor[i] == 1 else 'blue'
  marker = 'o' if y_xor[i] == 1 else 'x'
  plt.scatter(X_xor[i, 0], X_xor[i, 1], c=color, s=200, marker=marker)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Original Input Space')
plt.grid(True, alpha=0.3)

# Hidden layer space
plt.subplot(1, 2, 2)
for i in range(4):
  color = 'red' if y_xor[i] == 1 else 'blue'
  marker = 'o' if y_xor[i] == 1 else 'x'
  plt.scatter(hidden_representations[i, 0], hidden_representations[i, 1],c=color, s=200, marker=marker)
  plt.annotate(f'({X_xor[i,0]},{X_xor[i,1]})', (hidden_representations[i, 0], hidden_representations[i, 1]), xytext=(5, 5), textcoords='offset points')

h1_range = np.linspace(-0.5, 2.5, 100)
h2_boundary = h1_range / 2 -.25 # Shifted -0.25 boundary for clearer visualization

plt.plot(h1_range, h2_boundary, 'g--', label='Decision boundary: h2 = h1/2 -0.25')

plt.xlabel('h1')
plt.ylabel('h2')
plt.title('Hidden Layer Representation')
plt.grid(True, alpha=0.3)
plt.xlim(-0.5, 2.5)
plt.ylim(-0.5, 1.5)
plt.legend()

plt.tight_layout()
plt.show()

"""## Feedforward Neural Networks"""

class FeedforwardNetwork:
  """
  A 2-layer Feedforward Neural Network for Binary classification
  input -> hidden layer -> output


  Short recall: for matrix multiplication: A @ B,
  the number of Columns in A must equal the number of Rows in B.

  # Layer 1: input → hidden
  z1 = W1 @ X + b1    # X will be of shape ( |V| × embedding_dimension )
  # (hidden, input) @ (input, 1) = (hidden, 1)

  # Layer 2: hidden → output
  z2 = W2 @ h + b2
  # (output, hidden) @ (hidden, 1) = (output, 1)

  """

  def __init__(self, input_size, hidden_size, output_size=1, activation='sigmoid', learning_rate=0.1):
    ### Initializa weights as small random vlaues
    ### While random bias initialization works (in this example actually works much better than the zeros), the standard practice is to initialize biases to zero

    self.W1 = np.random.randn(hidden_size, input_size) * 0.1
    self.b1 = np.random.randn(hidden_size, 1)

    self.W2 = np.random.randn(output_size, hidden_size) * 0.1
    self.b2 = np.random.randn(output_size, 1)

    self.learning_rate = learning_rate

    if activation == 'sigmoid':
      self.activation = sigmoid
      self.activation_derivative = sigmoid_derivative

    elif activation == 'tanh':
      self.activation = tanh
      self.activation_derivative = tanh_derivative

    elif activation == 'relu':
      self.activation = relu
      self.activation_derivative = relu_derivative

  def forward(self,X):

    # Hidden layer
    self.z1 =  self.W1 @ X + self.b1  ## W1 first then X because for matrix multiplication: A @ B, the number of columns in A must equal the number of rows in B

    self.a1 = self.activation(self.z1) # using a1,a2 as in the book


    # Output layer (sigmoid for binary classification)
    self.z2 =  self.W2 @ self.a1 + self.b2
    self.a2 = sigmoid(self.z2) # Using sigmoid for the final activation to have probs, softmax in multinomial classif

    return self.a2


  def compute_loss(self, y_pred, y_true):
      """Binary cross-enropy loss"""

      m = y_true.shape[1] # num of samples
      epsilon = 1e-7 # to avoid log(0)

      loss = -(1/m) * np.sum(  y_true   * np.log( y_pred + epsilon )        # the term cancelled when not True
                          + ((1-y_true) * np.log( 1-y_pred + epsilon ))) # the term cancelled when True

      return loss

  def backward(self, X,y_true, y_pred):
    """Backward propagation for computing the gradients"""

    m = X.shape[1] # num of samples

    # Output layer gradients
    dz2 = y_pred - y_true # derivative of the Loss w.r.t. (with respect to) z2, the deriv of the Loss = -[ylog(σ(z)) + (1-y)log(1-σ(z))]  is  y_pred - y_true
    print(f"dz2 = y_pred - y_true")
    print(f"    = {y_pred[:, :3]} - {y_true[:, :3]} (showing first 3 samples)")
    print(f"    = {dz2[:, :3]}")
    print(f"Shape: {dz2.shape} (output_size × m)\n")

    dW2 = (1/m) * np.dot(dz2, self.a1.T)
    print(f"dW2 = (1/{m}) × dz2 @ a1.T")
    print(f"    = (1/{m}) × {dz2.shape} @ {self.a1.T.shape}")
    print(f"    = {dW2.shape} (output_size × hidden_size)")
    print(f"Example: dW2[0,0] = {dW2[0,0]:.4f}\n")

    db2 = (1/m) * np.sum(dz2, axis = 1, keepdims=True)
    print(f"db2 = (1/{m}) × sum(dz2, axis=1)")
    print(f"    = Average error across all {m} samples")
    print(f"    = {db2.T} (shape: {db2.shape})\n")

    # Hidden layer gradients
    da1 = np.dot(self.W2.T, dz2)
    print(f"da1 = W2.T @ dz2")
    print(f"    = {self.W2.T.shape} @ {dz2.shape}")
    print(f"    = {da1.shape} (hidden_size × m)")
    print(f"Backpropagating error to hidden layer\n")

    dz1 = da1 * self.activation_derivative(self.z1)
    print(f"dz1 = da1 * activation_derivative(z1)")
    print(f"    = {da1[:2, :3]} * {self.activation_derivative(self.z1)[:2, :3]} (sample)")
    print(f"    = {dz1[:2, :3]} (showing 2×3 sample)")
    print(f"Shape: {dz1.shape}\n")

    dW1 = (1/m) * np.dot(dz1, X.T)
    print(f"dW1 = (1/{m}) × dz1 @ X.T")
    print(f"    = (1/{m}) × {dz1.shape} @ {X.T.shape}")
    print(f"    = {dW1.shape} (hidden_size × input_size)")
    print(f"Example: dW1[0,0] = {dW1[0,0]:.4f}\n")

    db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)
    print(f"db1 = (1/{m}) × sum(dz1, axis=1)")
    print(f"    = {db1.T} (shape: {db1.shape})")
    print(f"\n" + "="*50 + "\n")

    return(dW1, db1, dW2, db2)


  def update_parameters(self, dW1, db1, dW2, db2):
    """ Update weights and biases using the gradients"""
    self.W1 -= self.learning_rate * dW1
    self.b1 -= self.learning_rate * db1
    self.W2 -= self.learning_rate * dW2
    self.b2 -= self.learning_rate * db2

  def train(self, X, y, epochs = 1000, verbose=True):
    """ Training the network"""

    losses = []

    for epoch in range(epochs):

      # Forward propagation
      y_pred = self.forward(X)

      # Compute Loss
      loss = self.compute_loss(y_pred, y)
      losses.append(loss)

      # Calc gradients with backward propagation
      dW1, db1, dW2, db2 = self.backward( X,y, y_pred)

      # Update parameters
      self.update_parameters(dW1, db1, dW2, db2)

      if verbose and epoch %100 == 0:
        print(f"Epoch {epoch}, Loss:{loss:.4f}")

    return losses


  def predict(self, X):
    """Make the predictions"""

    probabilities = self.forward(X)
    return((probabilities > 0.5).astype(int))

"""### Testing on XOR Problem

"""

# Prepare XOR data for our network
X_xor_nn = X_xor.T  # Shape: (2, 4) - features x samples
y_xor_nn = y_xor.reshape(1, -1)  # Shape: (1, 4)

# Create and train Feedforward NN
xor_network = FeedforwardNetwork(input_size=2,
                                 hidden_size=4,
                                 activation='tanh',
                                 learning_rate=0.5)

print("Training XOR Network...")
losses = xor_network.train(X_xor_nn, y_xor_nn, epochs=1000, verbose=False)

### Plot training loss
plt.figure(figsize=(8, 5))
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Binary Cross-Entropy Loss')
plt.title('Training Loss for XOR Problem')
plt.grid(True, alpha=0.3)
plt.show()

# Test predictions
predictions = xor_network.predict(X_xor_nn)
print("\nXOR Network Results:")
print("-" * 30)
for i in range(4):
  print(f"Input: {X_xor[i]} -> Predicted: {predictions[0,i]}, True: {y_xor[i]}")

"""## PyTorch Implementation of Feedforward Networks

"""

class PyTorchFeedforward(nn.Module):
  """PyTorch Implementation of Feedforward Networks"""

  def __init__(self, input_size, hidden_size, output_size=1, activation='relu'):
    super(PyTorchFeedforward, self).__init__()

    # Def fully connected layers
    self.fc1 = nn.Linear(input_size, hidden_size)
    self.fc2 = nn.Linear(hidden_size, output_size)

    # Def activation functs
    if activation == 'relu':
        self.activation = nn.ReLU()
    elif activation == 'tanh':
        self.activation = nn.Tanh()
    elif activation == 'sigmoid':
        self.activation = nn.Sigmoid()

  def forward(self, X):
    # Using "a1" and "a2" for the layers outputs as in the book
    z1 = self.fc1(X)
    a1 = self.activation(z1)

    z2 = self.fc2(a1)
    a2 = torch.sigmoid(z2)

    return(a2)

# Convert XOR data to PyTorch tensors
X_xor_torch = torch.FloatTensor(X_xor)
y_xor_torch = torch.FloatTensor(y_xor).reshape(-1, 1)

# Create model
torch_model = PyTorchFeedforward(input_size=2, hidden_size=4, activation='tanh')
print(torch_model)

## See params
print([i for i in torch_model.parameters()])

# Define Loss and Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(torch_model.parameters(), lr=0.01)

print(criterion)
print(optimizer)

# Training
print("Training PyTorch XOR Network")
torch_losses = []

for epoch in range(1000):
  # Forward pass
  y_pred = torch_model.forward(X_xor_torch)
  print("\ny_pred: ", y_pred, "\n")

  # Loss
  loss = criterion(y_pred, y_xor_torch)
  print("loss: ", loss, "\n")
  print("-"*50)

  # Backward pass and optimization
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  torch_losses.append(loss.item())

  if epoch % 100 == 0:
    print(f'Epoch [{epoch}/1000], Loss: {loss.item():.4f}')

# Evaluate
with torch.no_grad():
  predictions = torch_model(X_xor_torch)
  print("X_xor_torch: ", X_xor_torch,"\n")
  print("predictions: ", predictions,"\n")

  predicted_classes = (predictions > 0.5).float()
  print("predicted_classes: ", predicted_classes,"\n")

  accuracy = (predicted_classes == y_xor_torch).float().mean() ### tensor([[True],[True],[True],[True]]) -> 1.,1.,1.,1.
  print(f'\nAccuracy: {accuracy:.2%}')