# -*- coding: utf-8 -*-
"""Chapter_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttzOxJg4Re0lkrpoUTdNI06EFIb4e-_7
"""

!pip install gensim spacy transformers chromadb sentence-transformers -q
!python -m spacy download en_core_web_md -q
!pip install torchinfo torchviz -q
!pip install PyPDF2 pdfplumber

# ## In Colab: Restart session to avoid issues, specially with gensim

import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import gensim
from gensim.models import Word2Vec

import spacy

from transformers import AutoTokenizer, AutoModel
import torch
from torchinfo import summary
from torchviz import make_dot

import chromadb
from chromadb.utils import embedding_functions

import textwrap

from sentence_transformers import SentenceTransformer
import time

import nltk
# Download NLTK data quietly
nltk.download('punkt', quiet=True)  # Punkt tokenizer for sentence splitting
nltk.download('brown', quiet=True)  # Brown corpus for training data
from nltk.corpus import brown # for Word2Vec training with gensim

"""##Word-Document and Word-Word Matrices"""

# Example documents (Shakespeare plays)
documents = {
    "As You Like It": "battle good fool wit love forest magic",
    "Twelfth Night": "good fool wit love comedy mistaken identity",
    "Julius Caesar": "battle battle battle good fool war rome politics",
    "Henry V": "battle battle battle battle good wit war king england"
}

def create_term_document_matrix(documents):
  """ Creates term-document matrix
  Rows represent words (terms)
  Columns represent docs
  Cells have the frequencies (counts)"""

  all_the_words = set()

  for doc in documents.values():
    all_the_words.update(doc.split())

  vocab = sorted(all_the_words)
  print(vocab)

  matrix = []

  for word in vocab:
    row = []

    for doc_name, doc_text in documents.items():
      count = doc_text.split().count(word)
      row.append(count)

    matrix.append(row)

  print(matrix)


  df = pd.DataFrame(matrix,
                    index=vocab,
                    columns=list(documents.keys()))

  return df

term_doc_matrix = create_term_document_matrix(documents)
print("\n", term_doc_matrix)

"""## TF-IDF

"""

def compute_tf_idf(term_doc_matrix):
  """Compute Term Frequency × Inverse Document Frequency"""

  matrix = term_doc_matrix.values # Convert to numpy.ndarray to apply np.where easily

  n_docs = matrix.shape[1]

  print(np.where(matrix>0, 1,0))

  # Term freq
  tf = np.where(matrix > 0, 1 + np.log10(matrix + 1e-10), 0) # Raw counts can be misleading (100 occurrences isn't 100x more important than 1), for taht reason we compress it with the log10. Also, added small epsilon to avoid log(0) and warning
  print("\nTerm Freq:\n", tf, "\n")

  # Doc freq (words appearing on how many docs)
  df = np.sum(matrix>0, axis=1)
  print("\nDoc Freq:\n", df, "\n")

  # Inverse doc freq
  idf = np.log10(n_docs/df)
  print("\nInverse Doc Freq without log(10):\n", n_docs/df, "\n")

  print("\nInverse Doc Freq (with log(10)):\n", idf, "\n")


  # TF-IDF
  idf = idf[:, np.newaxis] # Flatten idf
  print("\nBroadcasted Inverse Doc Freq to dimensionality (n,1):\n", idf, "\n")

  tf_idf = tf*idf
  print("\nTF-IDF:\n", tf_idf, "\n")

  return pd.DataFrame(tf_idf, index=term_doc_matrix.index, columns=term_doc_matrix.columns)

tf_idf_matrix = compute_tf_idf(term_doc_matrix)
tf_idf_matrix

"""## Word-Word Co-occurrence (Association)

"""

def create_word_cooccurrence_matrix(documents, window_size=2):
  cooccurrence = defaultdict(lambda: defaultdict(int))

  for doc in documents.values():
    words = doc.split()

    for i, target_word in enumerate(words):# print(i,target_word)

      start = max(0, i - window_size) # Sets the start of the context window so it is not below 0
      end = min(len(words), i + window_size + 1) # Sets the end of the context window so it is not above the last word
      print("Word ", target_word, "has a window with indexes range ", start, "-", end, "\n")

      for j in range(start,end):
        if i != j:                  # Not counting the word with itself

          context_word = words[j]
          cooccurrence[target_word][context_word] += 1

      print("cooccurrence: ", cooccurrence)


  # all_words = []
  # for doc in documents.values():
  #   for word in doc.split(): all_words.append(word)
  # sorted(set(all_words))

  all_words = sorted(set(word for doc in documents.values() for word in doc.split(" ")  )) # Same as above but in a set comprehension
  print("\nall_words: ", all_words)

  matrix = []
  for target_word in all_words:
    row = []
    for ctxt_word in all_words:
      row.append(cooccurrence[target_word][ctxt_word])
    matrix.append(row)

  return pd.DataFrame(matrix, index = all_words, columns = all_words)

cooc_matrix = create_word_cooccurrence_matrix(documents, window_size=2)
cooc_matrix

"""## PPMI (Positive Pointwise Mutual Information)"""

def compute_ppmi(cooc_matrix, alpha = 0.75):
  """
  Compute PPMI matrix from the Co-occurrence matrix.
  alpha = 0.75: Levy et al. (2015) found that a setting of α = 0.75 improved performance of embeddings on a wide range of tasks
  0.75 increases the probability assigned to rare contexts, and hence lowers their PMI (Pα(c) > P(c) when c is rare).
  """

  matrix = cooc_matrix.values.astype(float)

  total = np.sum(matrix)


  # Joint probabilities P(w,c)
  # Element-wise division by scalar
  p_wc = matrix / total
  print("\n p_wc: \n", p_wc, "\n")

  # Getting marginal probabilities
  p_w = np.sum(matrix, axis=1) / total
  p_c = np.sum(matrix, axis=0) / total # Although it is the same bcs it is a symmetric co-occurrence matrix


  # Smoothing and re-normalizing
  p_c_alpha = np.power(p_c, alpha)
  p_c_alpha = p_c_alpha / np.sum(p_c_alpha)

  for i,j,k in zip(cooc_matrix.index, p_w, p_c_alpha):print(i,j,round(k,4)) ## p_w = p_c because it is a symmetric matrix, with the power to alpha that changes


  # Calculate PMI
  epsilon = 1e-10 #Added small epsilon to avoid division by zero and log(0)

  pmi = np.log2((p_wc + epsilon)/
              (p_w[:,np.newaxis] * p_c_alpha[np.newaxis,:]+epsilon)) # Broadcasting p_w to dimension (1,n)
  print("\n PMI: \n", pmi, "\n")

  # Convert to PPMI (turn negatives to zero)
  ppmi = np.maximum(0,pmi)
  return pd.DataFrame(ppmi, index=cooc_matrix.index, columns=cooc_matrix.columns)



ppmi_matrix = compute_ppmi(cooc_matrix, alpha = 0.75)
ppmi_matrix

"""## Cosine Similarity"""

def cosine_similarity(vec1, vec2):
  """
  Compute cosine similarity between two vectors
  Dot product divide by the product of the Norms of both to normalize them
  cosine(v1, v2) = (v1 . v2) / (|v1| x |v1|)
  """

  dot_prod = np.dot(vec1, vec2)

  magnitude1 = np.sqrt(np.sum(vec1**2))
  magnitude2 = np.sqrt(np.sum(vec2**2))

  # Prevent div by zero
  if magnitude1 == 0 or magnitude2 == 0: return 0

  return np.dot(vec1,vec2)/ (magnitude1 * magnitude2)

"""### Usecase for Cosine Similarity"""

def find_similar_words(word,matrix,top_n=3):
  """Finds the most similar words to a target word."""

  if word not in matrix.index:
    return([])

  target_vector = matrix.loc[word].values # Convert to pandas series
  similarities = []

  for other_word in matrix.index:
    if other_word != word:
      other_vector = matrix.loc[other_word].values
      sim = cosine_similarity(target_vector,other_vector)
      similarities.append((other_word, sim))

  similarities.sort(key=lambda x:x[1], reverse=True)  # Sort by similarity in descending order

  return similarities[:top_n]

find_similar_words('battle', tf_idf_matrix, 3)

"""##Simple Word2Vec Implementation (Skip-gram concept)"""

def sigmoid(x):
    """Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))"""
    return 1 / (1 + np.exp(-x))

class SkipGram:
  """Simplified Skipgram"""

  def __init__(self, vocab_size, embedding_dim=10):
    # Random initialization multiplied by small value
    self.W = np.random.randn(vocab_size, embedding_dim) * 0.1
    self.C = np.random.randn(vocab_size, embedding_dim) * 0.1
    self.vocab_size = vocab_size

  def forward(self, target_idx, context_idx):
    "Conpute P(context_word|target_word)"

    # Get embeddings by indexing into matrices
    target_embedding = self.W[target_idx]
    context_embedding = self.C[context_idx]

    # Dot product and sigmoid
    dot_product = np.dot(target_embedding, context_embedding)
    probability = sigmoid(dot_product)

    return probability

  def train_pair(self, target_idx, context_idx, label, learning_rate=0.01):
    """
    Train on a single target-context pair.
    label: 1 for positive (real context), 0 for negative (noise)
    """

    # Forward pass
    prob = self.forward(target_idx, context_idx)

    gradient  = (prob-label) # derivative of loss w.r.t. activation

    # Store original values before updating
    w_original = self.W[target_idx].copy()
    c_original = self.C[context_idx].copy()

    # Update embeddings with gradient descent
    # ∂L/∂W[target] = gradient * C[context]
    self.W[target_idx] -= learning_rate * gradient * c_original
    self.C[context_idx] -= learning_rate * gradient * w_original

    print("")
    print(self.W[target_idx], self.C[target_idx] )
    print("")

"""### Usecase of SkipGram"""

vocab = ['battle', 'good', 'fool', 'wit', 'love']

vocab_to_idx = {word: idx for idx,word in enumerate(vocab)}
print(vocab_to_idx)

model = SkipGram(len(vocab), embedding_dim=5)

print("\nTraining Skip-gram:")
print("Initial embedding for 'battle':", model.W[vocab_to_idx['battle']].round(3))


for _ in range(20):
  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['good'], 1) # Positive example: 'battle' appears with 'good'
  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['love'], 0) # Negative example: 'battle' doesn't appear with 'love'

print("Updated embedding for 'battle':", model.W[vocab_to_idx['battle']].round(3))

"""###  Visualization of Embeddings with t-SNE"""

def visualize_embeddings(embeddings, labels):
  """Visualize high-dimensional embedding in two dimensions with t-Distributed Stochastic Neighbor Embedding"""

  # Reducing dimensionality to 2 with t-SNE
  if embeddings.shape[1] > 2 and embeddings.shape[0] > 5: # Checking that there are enough samples
    # Adjust perplexity based on number of samples
    ppl = min(30,embeddings.shape[0]-1) # 30 is the default, but must be < n_samples
    tsne = TSNE(n_components = 2, random_state=42, perplexity=ppl)
    embeddings_2d = tsne.fit_transform(embeddings)
  else:
    embeddings_2d = embeddings[:,:2] # Takes just 2 first dimensions

  print(embeddings_2d)

  plt.figure(figsize=(10,8))
  plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1])

  # Adding labels
  for i,label in enumerate(labels):
    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), xytext=(5, 5), textcoords='offset points')

  plt.title("Word Embeddings")
  plt.xlabel("Dim 1")
  plt.ylabel("Dim 2")
  plt.grid(True, alpha=.3)
  plt.show()

if ppmi_matrix.shape[0]>0:
  print("Visualizing word embeddings from PPMI matrix")
  embeddings = ppmi_matrix.values
  labels = ppmi_matrix.index.tolist()
  visualize_embeddings(embeddings, labels)

"""### Word2Vec (with Gensim lib instead, common approach)

"""

# Using brown corpus
sentences = brown.sents()#[:10000]
print(sentences[:1000])

# Training Word2Vec
model = Word2Vec(
    sentences = sentences,
    vector_size = 100, # Embedding dimensionality
    window = 5,
    min_count = 5, # Ignoring words with frequency < 5
    workers = 4, # Parallel threads
    sg = 1, # Skip-gram (1) or Continuous Bag-of-Words (0)
    negative = 5, # Negative samples per each positive
    epochs = 10 # iters over the whole corpus
)

print("\nMost similar words to 'man':")
similar_words = model.wv.most_similar('man', topn=5)
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")
### Interesting results

print("\nMost similar words to 'Christ':")
similar_words = model.wv.most_similar('Christ', topn=5)
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")

# Word analogies
print("\nWord analogy: father - man + woman = ?")
# positive: words to add, negative: words to subtract
result = model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)
print(f"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})")
# Probably needs more data in the corpus

# Word analogies
print("\nWord analogy: queen - woman + man = ?")
# positive: words to add, negative: words to subtract
result = model.wv.most_similar(positive=['queen', 'man'], negative=['woman'], topn=1)
print(f"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})")

"""### Visualization in 2D with PCA"""

def plot_word_vectors(model, words):
  """Plot word vectors in 2-D with Princpal Component Analysis"""

  word_vectors = np.array([model.wv[word] for word in words if word in model.wv])
  word_labels = [word for word in words if word in model.wv]

  print("word_vectors shpae:", word_vectors.shape)
  print("word_vectors[:5,:5]: \n", str(word_vectors[:5,:5]))
  print("word_labels: \n", word_labels)


  # Reducing dimensionality to just 2-D with PCA
  pca = PCA(n_components = 2)
  vectors_2d = pca.fit_transform(word_vectors)

  print("vectors_2d with PCA:\n", vectors_2d)

  # Plot
  plt.figure(figsize=(10,8))
  plt.scatter(vectors_2d[:,0], vectors_2d[:,1])
  plt.title("Word2Vec Embeddings with PCA projection into 2-D")
  plt.grid(True, alpha =.3)

  for i, word in enumerate(word_labels):
    plt.annotate(word, xy=(vectors_2d[i,0], vectors_2d[i,1]), xytext=(5, 5), textcoords='offset points')

  plt.show()

words_to_plot = ['brave', 'fear', 'devil', 'hero' ,'monster','doctor','faith', 'eternal', 'health','king', 'queen', 'man', 'woman', 'prince', 'princess', 'boy', 'girl', 'father', 'mother', 'god', 'noise_asdfasdf', 'power', 'money',  'glory', 'God', 'Son', 'Jesus', 'Christ']
plot_word_vectors(model, words_to_plot)

"""## TF-IDF with Scikit-learn"""

# Sample documents
documents = [
    "The quick brown fox jumps over the lazy dog",
    "A quick brown dog outpaces a quick fox",
    "The lazy cat sleeps all day long",
    "Machine learning is fascinating and powerful",
    "Deep learning revolutionized natural language processing",
    "Natural language processing helps computers understand human language"
]

# TF-IDF vectorizer
vectorizer = TfidfVectorizer(
    max_features = 100, # Max vocab size
    ngram_range = (1,2), # Use unigrams and bigrams (1-word and 2-word phrases)
    stop_words = 'english', # Removing English stopwords
    lowercase = True, # Converts to lowercase
    use_idf = True,   # IDF weighting
    smooth_idf = True, # Add-1 (Laplace) Smoothing
    sublinear_tf = True # Uses log(tf) instead of just tf
)

# vectorizer

# Fit and trasnform docs
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print("tfidf_matrix[1]\n")
for i in tfidf_matrix[1]: print(i)

print("feature_names:\n ",feature_names)

tfidf_df = pd.DataFrame(
  tfidf_matrix.toarray(),
  columns=feature_names,
  index=[f"Doc{i}" for i in range(len(documents))]
)

print("\ntfidf_df.iloc[1][:20]\n")
print(tfidf_df.iloc[1][:20])

print("TF-IDF Matrix shape:", tfidf_matrix.shape)
print("\nTop terms by TF-IDF score in first document:")
doc_tfidf = tfidf_df.iloc[0]  # First doc
top_terms = doc_tfidf.nlargest(5)  # Get 5 largest values
for term, score in top_terms.items():
  print(f"  {term}: {score:.3f}")

"""### Document similarity on the tfidf_matrix

"""

from sklearn.metrics.pairwise import cosine_similarity # Cosine similarity from the lib

print("\nDocument similarity matrix:")
similarity_matrix = cosine_similarity(tfidf_matrix)
similarity_df = pd.DataFrame(
  similarity_matrix,
  index=[f"Doc{i}" for i in range(len(documents))],
  columns=[f"Doc{i}" for i in range(len(documents))]
)
print(similarity_df.round(3))

def find_similar_documents(doc_id, similarity_matrix, documents, top_n=2):
  """Find documents most similar to a given document."""
  similarities = similarity_matrix[doc_id]
  print("\nsimilarities:\n", similarities, "\n")

  similar_indices = np.argsort(similarities)[::-1][1:top_n+1] # argsort() returns indices that would sort array
  print("\nnp.argsort(similarities):", np.argsort(similarities), "\n")

  print(f"\nDocuments similar to: '{documents[doc_id]}'")
  for idx in similar_indices:
    print(f"  Similarity {similarities[idx]:.3f}: '{documents[idx]}'")


find_similar_documents(4, similarity_matrix, documents)

"""# Embeddings with SpaCy (a bit more modern)

### Written in Cython, spaCy is optimized for performance, enabling fast processing of large volumes of text data.
"""

## !python -m spacy download en_core_web_md -q


# Load the model
nlp = spacy.load("en_core_web_md")
print(dir(nlp))

# Process text and get word vectors
text = "The quick brown fox jumps over the lazy dog"

doc = nlp(text)
print(doc)

print("spaCy Word Vectors:")
print(f"Vector dimension: {doc[0].vector.shape[0]}")  # First token's vector dimension


# Word similarity using spaCy
# Process individual words
word1 = nlp("king")
word2 = nlp("queen")
word3 = nlp("car")

print(f"\nWord similarities:")
# .similarity() method computes cosine similarity
print(f"  king - queen: {word1.similarity(word2):.3f}")
print(f"  king - car: {word1.similarity(word3):.3f}")

"""## Contextual Embeddings with Transformers (more modern)"""

print("Loading BERT") # Requires using the HF_TOKEN, I have it loaded on colab
model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name) # AutoTokenizer automatically loads the correct tokenizer for that model

model = AutoModel.from_pretrained(model_name) # AutoModel loads the model architecture and weights

# for i in dir(model):print(i)

# Key architecture details
print(f"Hidden size: {model.config.hidden_size}")
print(f"Num layers: {model.config.num_hidden_layers}")
print(f"Num attention heads: {model.config.num_attention_heads}")
print(f"Vocab size: {model.config.vocab_size}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

print("\n========== LISTING THE LAYERS ========\n")
# List all layers
for name, module in model.named_modules():
    print(f"{name}: {module.__class__.__name__}")
print("\n==================\n")

print("\n")
print(model)

print("\n=== MODEL SUMMARY ===\n")
# Create actual input tensors for BERT
dummy_tokens = torch.randint(0, 1000, (1, 128))  # Random token IDs
summary(model, input_data={'input_ids': dummy_tokens},
        col_names=['input_size', 'output_size', 'num_params'],
        depth=3)
print("\n==================\n")

# Making downloadable viz of the arch of the model
print("\n=== CREATING COMPUTATION GRAPH ===\n")
dummy_input = tokenizer("Hello world", return_tensors="pt")
outputs = model(**dummy_input)
graph = make_dot(outputs.last_hidden_state.mean(), params=dict(model.named_parameters()))
graph.render("bert_graph", format="png")
print("Graph saved as bert_graph.png")

def get_bert_embeddings(text):
  """ Get BERT embeddings for a text."""

  # tokenize and prepare inputs
  inputs = tokenizer(
    text,
    return_tensors="pt", # python tensors
    padding = True, # pads to same length
    truncation = True, # Truncates to max length
    max_length = 512
  )


  # Get token IDs and convert back to tokens
  token_ids = inputs['input_ids'][0]  # Get first sequence
  tokens = tokenizer.convert_ids_to_tokens(token_ids)
  for i in range(min(20, len(tokens))):
    print(f"{i}: {token_ids[i]:5d} -> '{tokens[i]}'")


  with torch.no_grad(): # inference mode to get embeddings, no need on computing gradients
    outputs = model(**inputs) # unpacking dicts as kwards


  # Extracting embeddings
  sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy() # last_hidden_state shape: (batch_size, sequence_length, hidden_size). The [:, 0, :] selects CLS token (sentence representation)
  print("\nsentence_embedding[:,:10]\n")
  print(sentence_embedding[:,:10])
  print("\n")

  # Mean Pooling: averaging all the token embeddigns
  mean_embedding = outputs.last_hidden_state.mean(dim=1).numpy()
  print("\nmean_embedding[:,:10]\n")
  print(mean_embedding[:,:10])
  print("\n")

  return sentence_embedding[0], mean_embedding[0]


# Test sentences showcasing different linguistic phenomena
sentences = [
  # Polysemy: "bank" (financial vs. river)
  "The bank is by the river",
  "I need to go to the bank to deposit money",
  "The river bank is muddy",
  "She works at the central bank",
  "The boat docked at the river bank",
  "My bank account is overdrawn",

  # Polysemy: "bat" (animal vs. sports equipment)
  "The bat flew out of the cave at dusk",
  "She swung the bat and hit a home run",
  "Bats sleep hanging upside down",
  "He bought a new cricket bat",
  "The vampire bat feeds on blood",

  # Context changes meaning: "light"
  "The feather is very light",
  "Please turn on the light",
  "She wore a light blue dress",
  "Light travels faster than sound",
  "This suitcase feels light",
  "The room needs more light",

  # Synonyms in different contexts
  "The doctor prescribed medicine for my cold",
  "The physician recommended medication for my illness",
  "The surgeon performed the operation",
  "The medic treated the wounded soldier",

  # Idioms vs. literal meaning
  "It's raining cats and dogs outside",
  "The cats and dogs are playing together",
  "Break a leg at your performance tonight",
  "He literally broke his leg skiing",

  # Technical vs. common usage
  "I need to debug this Python code",
  "The bug crawled across the keyboard",
  "The software has a critical bug",
  "I found a bug in my salad"
]

embeddings = []

for sentence in sentences:
  cls_emb, mean_emb = get_bert_embeddings(sentence)
  embeddings.append(mean_emb)
  print(f"\nSentence: '{sentence}', with shape: {mean_emb.shape}. First 5 values: {mean_emb[:5].round(3)}.\n")

# Compute similarities
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings)

print("\nContextual similarity matrix:\n")
for i in range(len(sentences)):
  for j in range(i+1, len(sentences)):
    print(f" '{sentences[i]}' vs '{sentences[j]}':"
          f"{similarity_matrix[i,j]:.4f}" )

"""##ChromaDB for Embedding Storage and Retrieval

"""

# Sample documents to index
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks with multiple layers",
    "Natural language processing helps computers understand text",
    "Computer vision enables machines to interpret visual information",
    "Reinforcement learning trains agents through rewards and penalties",
    "Transfer learning reuses knowledge from pre-trained models",
    "Unsupervised learning finds patterns without labeled data",
    "Supervised learning requires labeled training examples"
]

client = chromadb.PersistentClient(path="/content/chroma_db")  # Initialize persisting client

# Create embedding function
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
      model_name = "all-MiniLM-L6-v2")

# Create or get collections
try:
  collection = client.get_collection(
      name="document_embeddings",
      embedding_function = sentence_transformer_ef
      )
  print("Loading existing collection")
except Exception as e:
  collection = client.create_collection(
      name="document_embeddings",
      embedding_function = sentence_transformer_ef,
      metadata = {"description": "Document embedding for semantic search"}
  )
  print(f"Creating new collection: {e}")

# Check existence of docs before addign
existing_count = collection.count()
if existing_count == 0:
  # Adding if empty
  collection.add(         # In collection.add() uses plurals for the params
    documents = documents,
    metadatas = [{"source": f"doc_{i}", "category":"AI/ML"}
                for i in range(len(documents))],
    ids = [f"doc_{i}" for i in range(len(documents))]
  )
  print(f"Added {len(documents)} documents")
else:
  print(f"Collection already has {existing_count} documents")


# Perform semantic search
print("\nSemantic Search:\n")
query = "How do neural networks learn from examples?"
results = collection.query(    # query() finds most similar documents
    query_texts=[query],
    n_results = 3
)

for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
  print(f"{i+1}. Distance: {distance:.4f}. {doc}")

print("\n")

# Advanced search with metadata filtering
print("\n\nFiltered search (category='AI/ML'):\n")
filtered_results = collection.query(
    query_texts = ["What is learning without labels?"],
    n_results = 2,
    where = {"category": "AI/ML"} # Filtering by metadata
)

for doc, dist in zip(filtered_results['documents'][0], filtered_results['distances'][0]) : print(f"{dist:.4f} - {doc}")

"""## Comparing Different Embedding Methods"""

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import time
import spacy

# Load required models
st_model = SentenceTransformer('all-MiniLM-L6-v2')

# Requires using the HF_TOKEN, I have it loaded on colab
model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name) # AutoTokenizer automatically loads the correct tokenizer for that model

model = AutoModel.from_pretrained(model_name) # AutoModel loads the model architecture and weights


# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except:
    !python -m spacy download en_core_web_sm
    nlp = spacy.load("en_core_web_sm")

# RENAME Word2Vec model to avoid conflict
from gensim.models import Word2Vec
sample_sentences = [["machine", "learning", "is", "great"],
                   ["computers", "process", "information", "fast"]]
w2v_model = Word2Vec(sentences=sample_sentences, vector_size=100, window=5, min_count=1)  # Changed name!

def compare_embedding_methods(text):
  """Comparing different embedding methods for the same text"""

  results = {}

  # TF-IDF
  sample_docs = [text, "Just another document", "And one more as yapa"]
  tfidf_vec = TfidfVectorizer(max_features = 50) # Dimensions
  tfidf_matrix = tfidf_vec.fit_transform(sample_docs)
  results['TF-IDF'] = {
      'dimension': tfidf_matrix.shape[1],
      'type': 'sparse',
      'values': tfidf_matrix[0].toarray()[0][:5] # Just first 5 vals
  }


  # Word2Vec
  words = text.lower().split()
  valid_words = [w for w in words if w in w2v_model.wv] # Only existing words in the vocabulary
  if not valid_words:
    print(f"Warning: No valid words found for Word2Vec from: {words}")
  print("valid_words", valid_words)
  if valid_words:
    # Average word vectors
    w2v_embedding = np.mean([w2v_model.wv[w] for w in valid_words], axis = 0)
    results['Word2Vec'] = {
      'dimension': len(w2v_embedding),
      'type': 'dense',
      'values': w2v_embedding[:5] # Just first 5 vals
  }


  # spaCy
  spacy_doc = nlp(text)
  results['spaCy'] = {
    'dimension': len(spacy_doc.vector),
    'type': 'dense',
    'values': spacy_doc.vector[:5]
  }

  st_embedding = st_model.encode(text)
  results['Sentence-Transformer'] = {
    'dimension': len(st_embedding),
    'type': 'dense',
    'values': st_embedding[:5]
  }

  cls_emb, mean_emb = get_bert_embeddings(text)

  results['BERT'] = {
    'dimension': len(mean_emb),
    'type':'dense, contextual',
    'values': mean_emb[:5]
  }

  return results




# Compare diff methods
test_text = "Machine learning transforms how computers process information"
print(f"Comparing embeddings for: '{test_text}'\n")

comparison = compare_embedding_methods(test_text)
for method, info in comparison.items():
  print(f"{method}:")
  print(f"  Dimension: {info['dimension']}")
  print(f"  Type: {info['type']}")
  print(f"  First 5 values: {np.array(info['values']).round(3)}")
  print()

"""## Usecase: PDF Question Answering with Semantic Search

### Download chapter from the book
"""

# !wget https://web.stanford.edu/~jurafsky/slp3/6.pdf # Easiest way

# More python way
import requests
url = "https://web.stanford.edu/~jurafsky/slp3/6.pdf"

resp = requests.get(url)

with open("6.pdf", "wb") as fh:
  fh.write(resp.content)

"""### Specific libs for this usecase"""

import PyPDF2
import pdfplumber # Better for complex pdfs
import re
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import textwrap

"""### Creating Class

"""

class PDFSemanticSearch:
  """Semantic Search Engine for PDF documents."""

  def __init__(self, embedding_model='all-MiniLM-L6-v2'):
    self.model = SentenceTransformer(embedding_model)
    self. paragraphs = []
    self.embeddings = None
    self.metadata = []
    self.full_text = "" # Keeping the complete text for reference
    self.sections = []

  def extract_text_from_pdf(self,pdf_path):
    """PDF text extraction with multiple methods""" # using just PyPDF2 was not enough
    print(f"Reading PDF: {pdf_path}")
    all_text = []

    try:
      # Method 1: pdfplumber, better for complex layouts
      with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
          text = page.extract_text() or ""
          if text.strip():
            all_text.append({
                'text': text,
                'page': page_num + 1,
                'method': 'pdfplumber'
            })

    except Exception as e:
      print(f"pdfplumber failed :( \nError: {e} ")

    # Method 2: PyPDF2 as fallback if required
    if not all_text:
      print("Falling back to PyPDF2")
      with open(pdf_path, 'rb') as fh:
        pdf_reader = PyPDF2.PdfReader(fh)
        for page_num in range(len(pdf_reader.pages)):
          page = pdf_reader.pages[page_num]
          text = page.extract_text()
          if text.strip():
            all_text.append({
                'text': text,
                'page': page_num + 1,
                'method': 'PyPDF2'
            })

    # Store full text for reference
    self.full_text = "\n\n".join([page['text'] for page in all_text ])
    return(all_text)


  def clean_text(self, text):
    """Clean and normalize the extracted text."""

    # Common pdf extraction issues
    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text) # Adding space btwn camelCase
    text = re.sub(r'(?<=[a-zA-Z])(?=[0-9])', ' ', text) # Add space between letter and number
    text = re.sub(r'(?<=[0-9])(?=[a-zA-Z])', ' ', text)
    text = re.sub(r'([a-z])([.!?,;:])([a-zA-Z])', r'\1\2 \3', text)  # Fix words that are concatenated with punctuation
    text = re.sub(r'\s+', ' ' , text) # Normalize whitespaces
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text) # Fixing hyphenated words
    text = re.sub(r'([.!?])\s*\n', r'\1 ', text)  # Fix sentence breaks

    # Remove excessive spaces but preserving paragraphs structure
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
      line = line.strip()
      if line:
        cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)


  def split_into_chunks(self, pdf_text, chunk_size=500, overlap=50):
    """
    Split text into overlapping chunks for context preservation.
    chunk_size: target size of each chunk in chars.
    overlap: number of chars to overlap between chunks.
    """
    chunks = []


    for page_data in pdf_text:
      text = self.clean_text(page_data['text'])
      page_num = page_data['page']

      # First try to split by paragraphs
      paragraphs = re.split(r'\n\s*\n|\n(?=[A-Z])', text)

      current_chunk = ""
      for para in paragraphs:
        para = para.strip()

        # if the paragraph is too long, split it further
        if len(para) > chunk_size:
          #split by sentencse
          sentences = re.split(r'(?<=[.!?])\s+', para)
          for sent in sentences:
            if len(current_chunk) + len(sent) > chunk_size and current_chunk:
              chunks.append({
                  'text': current_chunk.strip(),
                  'page': page_num,
                  'length': len(current_chunk)
              })
              # Keep overlap
              current_chunk = current_chunk[-overlap:] + " " + sent
            else:
              current_chunk += " " + sent

        # if adding paragraph does not exceed the chunk size
        elif len(current_chunk) + len(para) <= chunk_size:
          current_chunk += "\n" + para if current_chunk else para


        # If exceeds the chunk size, save current chunk and start a new one
        else:
          if current_chunk:
            chunks.append({
                'text': current_chunk.strip(),
                'page': page_num,
                'length': len(current_chunk)
            })
          current_chunk = para

      # Last chunk
      if current_chunk.strip():
          chunks.append({
              'text': current_chunk.strip(),
              'page': page_num,
              'length': len(current_chunk)
          })

    return chunks


  def extract_sections(self, text):
    """ Extract section headers and structure from text."""

    # Common section patterns for pdfs
    section_patterns = [
      r'^(\d+\.?\d*)\s+([A-Z][A-Za-z\s]+)$',  # Example: "6.1 Lexical Semantics"
      r'^([A-Z][A-Za-z\s]+):$',  # Example "Introduction:"
      r'^#{1,3}\s*(.+)$',  #  ###Markdown style headers
    ]

    sections = []
    for pattern in section_patterns:
      matches = re.finditer(pattern, text, re.MULTILINE) # re.MULTILINE modifies the behavior of the ^ (caret) and $ (dollar sign) anchors, acting also after the new lines \n
      for match in matches:
        sections.append({
          'title': match.group(0).strip(),
          'start': match.start(),
          'level': match.group(1) if match.lastindex > 1 else '0'
        })

    return(sorted(sections, key=lambda x: x['start']))



  def index_pdf(self, pdf_path, chunk_size=500):
    """Extract and index chunks from the pdf with processing"""

    # Extract text from the pdf
    pdf_text = self.extract_text_from_pdf(pdf_path)

    if not pdf_text:
      raise ValueError("No text could be extracted from the PDF.")


    # Extract sections for better context
    self.sections = self.extract_sections(self.full_text)
    print(f"Found {len(self.sections)} sections in the document")

    # Split into chunks
    chunk_data = self.split_into_chunks(pdf_text, chunk_size=chunk_size)

    # Extract text and metadata
    self.paragraphs = [c['text'] for c in chunk_data]
    self.metadata = []

    for i, chunk in enumerate(chunk_data):
      # Find which section this chunk belongs to
      chunk_start = self.full_text.find(chunk['text'][:50])
      section = "Unknown"
      for s in reversed(self.sections):
        if s['start'] <= chunk_start:
          section = s['title']
          break

      self.metadata.append({
          'page': chunk['page'],
          'length': chunk['length'],
          'section': section,
          'index': i
      })

    print(f"\nCreated {len(self.paragraphs)} chunks")
    print(f"Average chunk size: {np.mean([m['length'] for m in self.metadata]):.0f} characters")
    print("Creating embeddings...")

    # Create the embeddings
    self.embeddings = self.model.encode(
        self.paragraphs,
        show_progress_bar = True,
        batch_size = 32,
        convert_to_numpy=True
    )

    print("Indexing complete!")

  def search(self, query, top_k=5, threshold=0.25, rerank=True):
    """Search with optional reranking"""

    if self.embeddings is None:
      raise ValueError("No documents indexed. Use index_pdf() first.")


    # Encode query
    query_embedding = self.model.encode([query], convert_to_numpy=True) # np.argsort(similarities) - Gets indices that would sort the array (ascending). top_k*2 candidates if reranking is enabled (to have more options for reranking), so if top_k=5 and rerank=True, it gets the indices of the 10 most similar chunks, which will later be reranked and trimmed to 5.

    # Compute similarities
    similarities = cosine_similarity(query_embedding, self.embeddings)[0]

    # Get top candidates
    top_indices = np.argsort(similarities)[::-1][:top_k*2 if rerank else top_k]

    results = []
    for idx in top_indices:
      score = similarities[idx]
      if score >= threshold:
        results.append({
            'paragraph': self.paragraphs[idx],
            'score': score,
            'page': self.metadata[idx]['page'],
            'section': self.metadata[idx]['section'],
            'index':idx
        })

    # Rerank based on additional factors
    if rerank and results:
      results = self._rerank_results(results, query)

    return(results[:top_k])

  def _rerank_results(self, results, query):
    """Rerank results based on additional criteria: exact matches in the text/section/title."""

    query_terms = set(query.lower().split())  # LowerCase and split to match exactly the query with the text

    for result in results:
      text_lower = result['paragraph'].lower() # LowerCase to match exactly the query with the text

      # Boost score for exact term matches
      term_matches = sum(1 for term in query_terms if term in text_lower)

      # Boost for title/section matches
      section_boost = 0.1 if any(term in result['section'].lower()
                                for term in query_terms) else 0

      # Combine scores
      result['final_score'] = result['score'] + (term_matches * 0.05) + section_boost

    # Sort by final score
    return sorted(results, key=lambda x: x['final_score'], reverse=True)

  def answer_question(self, question, top_k=3, show_context=True):
    """ Question answering"""

    print(f"\n{'='*80}")
    print(f"Question: {question}")
    print(f"{'='*80}\n")

    # Searching for relevant chunks
    results = self.search(question, top_k=top_k)

    if not results:
      print("No relevant content found. Review your question, pal.")
      return None

    print(f"Found {len(results)} relevant passages:\n")

    # Display results
    for i, result in enumerate(results, 1):
      print(f"Result {i} (Page {result['page']}, Score: {result['score']:.3f})")
      print(f"Section: {result['section']}")
      print("-" * 60)

      # Format and display text
      text = result['paragraph']

      # Highlight query terms
      highlighted_text = self._highlight_terms(text, question)

      # Wrap text for better readability
      wrapped_lines = textwrap.wrap(highlighted_text, width=80)
      for line in wrapped_lines:
          print(line)

      print("\n")

      # Show surrounding context if requested
      if show_context and i==1: # For the top result only
        self._show_context(result['index'])


    return(results)

  def _highlight_terms(self, text, query):
    """Highlight query terms in text."""
    # Extract important terms from query
    stop_words = {'what', 'is', 'the', 'how', 'does', 'are', 'in', 'of', 'to', 'a', 'an'}
    query_terms = [term.lower() for term in query.split()
                  if term.lower() not in stop_words and len(term) > 2]

    # Highlight each term
    for term in query_terms:
      # Case-insensitive replacement with word boundaries
      pattern = re.compile(rf'\b{re.escape(term)}\b', re.IGNORECASE)
      text = pattern.sub(lambda m: f"**{m.group().upper()}**", text)

    return text

  def _show_context(self, index, window=1):
    """Show surrounding context for a result."""
    print("\n Extended Context:")
    print("-" * 60)

    # Get surrounding chunks
    start_idx = max(0, index - window)
    end_idx = min(len(self.paragraphs), index + window + 1)

    for i in range(start_idx, end_idx):
      if i == index:
        print(">>> MAIN PASSAGE <<<")
      else:
        print(f"[Context from page {self.metadata[i]['page']}]")

      wrapped = textwrap.wrap(self.paragraphs[i], width=80)

      for line in wrapped[:3]:  # Show first 3 lines of context
        print(line)
      if len(wrapped) > 3:
        print("...")
      print()

  def get_section_summary(self):
    """Get a summary of document sections."""
    if not self.sections:
      return "No sections found."

    print("\n Document Structure:")
    print("=" * 60)

    current_level = None
    for section in self.sections[:20]:  # Show first 20 sections
      level = section.get('level', '0')
      indent = "  " * (len(level.split('.')) - 1)
      print(f"{indent}{section['title']}")

    if len(self.sections) > 20:
      print(f"... and {len(self.sections) - 20} more sections")



# Create enhanced search engine
pdf_search = PDFSemanticSearch()

# Index the PDF
pdf_path = "6.pdf"
pdf_search.index_pdf(pdf_path, chunk_size=600)

# Show document structure
pdf_search.get_section_summary()

# Interactive mode with improved features
def enhanced_interactive_qa():
  """Enhanced interactive Q&A with helpful features."""
  print("\n" + "="*80)
  print(" Enhanced PDF Q&A System")
  print(" Commands: 'help', 'sections', 'search [term]', 'quit'")
  print("="*80)

  while True:
    user_input = input("\nAsk a question (or command): ").strip()

    if user_input.lower() in ['quit', 'exit', 'q']:
      print("Goodbye!")
      break

    elif user_input.lower() == 'help':
      print("\nAvailable commands:")
      print(" - Ask any question about the content")
      print(" - 'sections': Show document structure")
      print(" - 'search [term]': Search for specific term")
      print(" - 'quit': Exit the system")

    elif user_input.lower() == 'sections':
      pdf_search.get_section_summary()

    elif user_input.lower().startswith('search '):
      search_term = user_input[7:]
      print(f"\nSearching for '{search_term}'...")
      results = pdf_search.search(search_term, top_k=3)
      if results:
        print(f"Found {len(results)} results containing '{search_term}'")
        for i, r in enumerate(results, 1):
          print(f"\n{i}. Page {r['page']}, Section: {r['section']}")
          print(f" Preview: {r['paragraph'][:100]}...")

    elif user_input:
      pdf_search.answer_question(user_input)

    else:
      print("Please enter a question or command.")

# Run enhanced interactive mode
enhanced_interactive_qa()

# Example questions about Chapter 6 content
questions = [
    "What is the distributional hypothesis?",
    "How does Word2Vec skip-gram work?",
    "What is the difference between sparse and dense embeddings?",
    "How do we compute cosine similarity between vectors?",
    "What is PPMI and how is it calculated?",
    "What are the problems with bias in embeddings?",
    "How does TF-IDF weighting work?"
]