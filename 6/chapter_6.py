# -*- coding: utf-8 -*-
"""Chapter_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttzOxJg4Re0lkrpoUTdNI06EFIb4e-_7
"""

!pip install gensim spacy transformers chromadb sentence-transformers -q
# !python -m spacy download en_core_web_md -q


# ## In Colab: Restart session to avoid issues, specially with gensim

import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import gensim
from gensim.models import Word2Vec

import spacy

from transformers import AutoTokenizer, AutoModel
import torch
from torchinfo import summary
from torchviz import make_dot

import nltk
# Download NLTK data quietly
nltk.download('punkt', quiet=True)  # Punkt tokenizer for sentence splitting
nltk.download('brown', quiet=True)  # Brown corpus for training data
from nltk.corpus import brown # for Word2Vec training with gensim



"""##Word-Document and Word-Word Matrices"""

# Example documents (Shakespeare plays)
documents = {
    "As You Like It": "battle good fool wit love forest magic",
    "Twelfth Night": "good fool wit love comedy mistaken identity",
    "Julius Caesar": "battle battle battle good fool war rome politics",
    "Henry V": "battle battle battle battle good wit war king england"
}

def create_term_document_matrix(documents):
  """ Creates term-document matrix
  Rows represent words (terms)
  Columns represent docs
  Cells have the frequencies (counts)"""

  all_the_words = set()

  for doc in documents.values():
    all_the_words.update(doc.split())

  vocab = sorted(all_the_words)
  print(vocab)

  matrix = []

  for word in vocab:
    row = []

    for doc_name, doc_text in documents.items():
      count = doc_text.split().count(word)
      row.append(count)

    matrix.append(row)

  print(matrix)


  df = pd.DataFrame(matrix,
                    index=vocab,
                    columns=list(documents.keys()))

  return df

term_doc_matrix = create_term_document_matrix(documents)
print("\n", term_doc_matrix)

"""## TF-IDF

"""

def compute_tf_idf(term_doc_matrix):
  """Compute Term Frequency × Inverse Document Frequency"""

  matrix = term_doc_matrix.values # Convert to numpy.ndarray to apply np.where easily

  n_docs = matrix.shape[1]

  print(np.where(matrix>0, 1,0))

  # Term freq
  tf = np.where(matrix > 0, 1 + np.log10(matrix + 1e-10), 0) # Raw counts can be misleading (100 occurrences isn't 100x more important than 1), for taht reason we compress it with the log10. Also, added small epsilon to avoid log(0) and warning
  print("\nTerm Freq:\n", tf, "\n")

  # Doc freq (words appearing on how many docs)
  df = np.sum(matrix>0, axis=1)
  print("\nDoc Freq:\n", df, "\n")

  # Inverse doc freq
  idf = np.log10(n_docs/df)
  print("\nInverse Doc Freq without log(10):\n", n_docs/df, "\n")

  print("\nInverse Doc Freq (with log(10)):\n", idf, "\n")


  # TF-IDF
  idf = idf[:, np.newaxis] # Flatten idf
  print("\nBroadcasted Inverse Doc Freq to dimensionality (n,1):\n", idf, "\n")

  tf_idf = tf*idf
  print("\nTF-IDF:\n", idf, "\n")

  return pd.DataFrame(tf_idf, index=term_doc_matrix.index, columns=term_doc_matrix.columns)

tf_idf_matrix = compute_tf_idf(term_doc_matrix)
tf_idf_matrix

"""## Word-Word Co-occurrence (Association)

"""

def create_word_cooccurrence_matrix(documents, window_size=2):
  cooccurrence = defaultdict(lambda: defaultdict(int))

  for doc in documents.values():
    words = doc.split()

    for i, target_word in enumerate(words):# print(i,target_word)

      start = max(0, i - window_size) # Sets the start of the context window so it is not below 0
      end = min(len(words), i + window_size + 1) # Sets the end of the context window so it is not above the last word
      print("Word ", target_word, "has a window with indexes range ", start, "-", end, "\n")

      for j in range(start,end):
        if i != j:                  # Not counting the word with itself

          context_word = words[j]
          cooccurrence[target_word][context_word] += 1

      print("cooccurrence: ", cooccurrence)


  # all_words = []
  # for doc in documents.values():
  #   for word in doc.split(): all_words.append(word)
  # sorted(set(all_words))

  all_words = sorted(set(word for doc in documents.values() for word in doc.split(" ")  )) # Same as above but in a set comprehension
  print("\nall_words: ", all_words)

  matrix = []
  for target_word in all_words:
    row = []
    for ctxt_word in all_words:
      row.append(cooccurrence[target_word][ctxt_word])
    matrix.append(row)

  return pd.DataFrame(matrix, index = all_words, columns = all_words)

cooc_matrix = create_word_cooccurrence_matrix(documents, window_size=2)
cooc_matrix

"""## PPMI (Positive Pointwise Mutual Information)"""

def compute_ppmi(cooc_matrix, alpha = 0.75):
  """
  Compute PPMI matrix from the Co-occurrence matrix.
  alpha = 0.75: Levy et al. (2015) found that a setting of α = 0.75 improved performance of embeddings on a wide range of tasks
  0.75 increases the probability assigned to rare contexts, and hence lowers their PMI (Pα(c) > P(c) when c is rare).
  """

  matrix = cooc_matrix.values.astype(float)

  total = np.sum(matrix)


  # Joint probabilities P(w,c)
  # Element-wise division by scalar
  p_wc = matrix / total
  print("\n p_wc: \n", p_wc, "\n")

  # Getting marginal probabilities
  p_w = np.sum(matrix, axis=1) / total
  p_c = np.sum(matrix, axis=0) / total # Although it is the same bcs it is a symmetric co-occurrence matrix


  # Smoothing and re-normalizing
  p_c_alpha = np.power(p_c, alpha)
  p_c_alpha = p_c_alpha / np.sum(p_c_alpha)

  for i,j,k in zip(cooc_matrix.index, p_w, p_c_alpha):print(i,j,round(k,4)) ## p_w = p_c because it is a symmetric matrix, with the power to alpha that changes


  # Calculate PMI
  epsilon = 1e-10 #Added small epsilon to avoid division by zero and log(0)

  pmi = np.log2((p_wc + epsilon)/
              (p_w[:,np.newaxis] * p_c_alpha[np.newaxis,:]+epsilon)) # Broadcasting p_w to dimension (1,n)
  print("\n PMI: \n", pmi, "\n")

  # Convert to PPMI (turn negatives to zero)
  ppmi = np.maximum(0,pmi)
  return pd.DataFrame(ppmi, index=cooc_matrix.index, columns=cooc_matrix.columns)



ppmi_matrix = compute_ppmi(cooc_matrix, alpha = 0.75)
ppmi_matrix

"""## Cosine Similarity"""

def cosine_similarity(vec1, vec2):
  """
  Compute cosine similarity between two vectors
  Dot product divide by the product of the Norms of both to normalize them
  cosine(v1, v2) = (v1 . v2) / (|v1| x |v1|)
  """

  dot_prod = np.dot(vec1, vec2)

  magnitude1 = np.sqrt(np.sum(vec1**2))
  magnitude2 = np.sqrt(np.sum(vec2**2))

  # Prevent div by zero
  if magnitude1 == 0 or magnitude2 == 0: return 0

  return np.dot(vec1,vec2)/ (magnitude1 * magnitude2)

"""### Usecase for Cosine Similarity"""

def find_similar_words(word,matrix,top_n=3):
  """Finds the most similar words to a target word."""

  if word not in matrix.index:
    return([])

  target_vector = matrix.loc[word].values # Convert to pandas series
  similarities = []

  for other_word in matrix.index:
    if other_word != word:
      other_vector = matrix.loc[other_word].values
      sim = cosine_similarity(target_vector,other_vector)
      similarities.append((other_word, sim))

  similarities.sort(key=lambda x:x[1], reverse=True)  # Sort by similarity in descending order

  return similarities[:top_n]

find_similar_words('battle', tf_idf_matrix, 3)

"""##Simple Word2Vec Implementation (Skip-gram concept)"""

def sigmoid(x):
    """Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))"""
    return 1 / (1 + np.exp(-x))

class SkipGram:
  """Simplified Skipgram"""

  def __init__(self, vocab_size, embedding_dim=10):
    # Random initialization multiplied by small value
    self.W = np.random.randn(vocab_size, embedding_dim) * 0.1
    self.C = np.random.randn(vocab_size, embedding_dim) * 0.1
    self.vocab_size = vocab_size

  def forward(self, target_idx, context_idx):
    "Conpute P(context_word|target_word)"

    # Get embeddings by indexing into matrices
    target_embedding = self.W[target_idx]
    context_embedding = self.W[context_idx]

    # Dot product and sigmoid
    dot_product = np.dot(target_embedding, context_embedding)
    probability = sigmoid(dot_product)

    return probability

  def train_pair(self, target_idx, context_idx, label, learning_rate=0.01):
    """
    Train on a single target-context pair.
    label: 1 for positive (real context), 0 for negative (noise)
    """

    # Forward pass
    prob = self.forward(target_idx, context_idx)

    gradient  = (prob-label) # derivative of loss w.r.t. activation

    # Update embeddings with gradient descent
    self.W[target_idx] -= learning_rate * gradient * self.C[context_idx]
    self.C[target_idx] -= learning_rate * gradient * self.W[target_idx]

    print("")
    print(self.W[target_idx], self.C[target_idx] )
    print("")

"""### Usecase of SkipGram"""

vocab = ['battle', 'good', 'fool', 'wit', 'love']

vocab_to_idx = {word: idx for idx,word in enumerate(vocab)}
print(vocab_to_idx)

model = SkipGram(len(vocab), embedding_dim=5)

print("\nTraining Skip-gram:")
print("Initial embedding for 'battle':", model.W[vocab_to_idx['battle']].round(3))


for _ in range(20):
  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['good'], 1) # Positive example: 'battle' appears with 'good'
  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['love'], 0) # Negative example: 'battle' doesn't appear with 'love'

print("Updated embedding for 'battle':", model.W[vocab_to_idx['battle']].round(3))

"""###  Visualization of Embeddings with t-SNE"""

def visualize_embeddings(embeddings, labels):
  """Visualize high-dimensional embedding in two dimensions with t-Distributed Stochastic Neighbor Embedding"""

  # Reducing dimensionality to 2 with t-SNE
  if embeddings.shape[1] > 2 and embeddings.shape[0] > 5: # Checking that there are enough samples
    # Adjust perplexity based on number of samples
    ppl = min(30,embeddings.shape[0]-1) # 30 is the default, but must be < n_samples
    tsne = TSNE(n_components = 2, random_state=42, perplexity=ppl)
    embeddings_2d = tsne.fit_transform(embeddings)
  else:
    embeddings_2d = embeddings[:,:2] # Takes just 2 first dimensions

  print(embeddings_2d)

  plt.figure(figsize=(10,8))
  plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1])

  # Adding labels
  for i,label in enumerate(labels):
    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), xytext=(5, 5), textcoords='offset points')

  plt.title("Word Embeddings")
  plt.xlabel("Dim 1")
  plt.ylabel("Dim 2")
  plt.grid(True, alpha=.3)
  plt.show()

if ppmi_matrix.shape[0]>0:
  print("Visualizing word embeddings from PPMI matrix")
  embeddings = ppmi_matrix.values
  labels = ppmi_matrix.index.tolist()
  visualize_embeddings(embeddings, labels)

"""### Word2Vec (with Gensim lib instead, common approach)

"""

# Using brown corpus
sentences = brown.sents()#[:10000]
print(sentences[:1000])

# Training Word2Vec
model = Word2Vec(
    sentences = sentences,
    vector_size = 100, # Embedding dimensionality
    window = 5,
    min_count = 5, # Ignoring words with frequency < 5
    workers = 4, # Parallel threads
    sg = 1, # Skip-gram (1) or Continuous Bag-of-Words (0)
    negative = 5, # Negative samples per each positive
    epochs = 10 # iters over the whole corpus
)

print("\nMost similar words to 'man':")
similar_words = model.wv.most_similar('man', topn=5)
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")
### Interesting results

print("\nMost similar words to 'Christ':")
similar_words = model.wv.most_similar('Christ', topn=5)
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")

# Word analogies
print("\nWord analogy: father - man + woman = ?")
# positive: words to add, negative: words to subtract
result = model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)
print(f"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})")
# Probably needs more data in the corpus

# Word analogies
print("\nWord analogy: queen - woman + man = ?")
# positive: words to add, negative: words to subtract
result = model.wv.most_similar(positive=['queen', 'man'], negative=['woman'], topn=1)
print(f"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})")

"""### Visualization in 2D with PCA"""

def plot_word_vectors(model, words):
  """Plot word vectors in 2-D with Princpal Component Analysis"""

  word_vectors = np.array([model.wv[word] for word in words if word in model.wv])
  word_labels = [word for word in words if word in model.wv]

  print("word_vectors shpae:", word_vectors.shape)
  print("word_vectors[:5,:5]: \n", str(word_vectors[:5,:5]))
  print("word_labels: \n", word_labels)


  # Reducing dimensionality to just 2-D with PCA
  pca = PCA(n_components = 2)
  vectors_2d = pca.fit_transform(word_vectors)

  print("vectors_2d with PCA:\n", vectors_2d)

  # Plot
  plt.figure(figsize=(10,8))
  plt.scatter(vectors_2d[:,0], vectors_2d[:,1])
  plt.title("Word2Vec Embeddings with PCA projection into 2-D")
  plt.grid(True, alpha =.3)

  for i, word in enumerate(word_labels):
    plt.annotate(word, xy=(vectors_2d[i,0], vectors_2d[i,1]), xytext=(5, 5), textcoords='offset points')

  plt.show()

words_to_plot = ['brave', 'fear', 'devil', 'hero' ,'monster','doctor','faith', 'eternal', 'health','king', 'queen', 'man', 'woman', 'prince', 'princess', 'boy', 'girl', 'father', 'mother', 'god', 'noise_asdfasdf', 'power', 'money',  'glory', 'God', 'Son', 'Jesus', 'Christ']
plot_word_vectors(model, words_to_plot)

"""## TF-IDF with Scikit-learn"""

# Sample documents
documents = [
    "The quick brown fox jumps over the lazy dog",
    "A quick brown dog outpaces a quick fox",
    "The lazy cat sleeps all day long",
    "Machine learning is fascinating and powerful",
    "Deep learning revolutionized natural language processing",
    "Natural language processing helps computers understand human language"
]

# TF-IDF vectorizer
vectorizer = TfidfVectorizer(
    max_features = 100, # Max vocab size
    ngram_range = (1,2), # Use unigrams and bigrams (1-word and 2-word phrases)
    stop_words = 'english', # Removing English stopwords
    lowercase = True, # Converts to lowercase
    use_idf = True,   # IDF weighting
    smooth_idf = True, # Add-1 (Laplace) Smoothing
    sublinear_tf = True # Uses log(tf) instead of just tf
)

# vectorizer

# Fit and trasnform docs
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print("tfidf_matrix[1]\n")
for i in tfidf_matrix[1]: print(i)

print("feature_names:\n ",feature_names)

tfidf_df = pd.DataFrame(
  tfidf_matrix.toarray(),
  columns=feature_names,
  index=[f"Doc{i}" for i in range(len(documents))]
)

print("\ntfidf_df.iloc[1][:20]\n")
print(tfidf_df.iloc[1][:20])

print("TF-IDF Matrix shape:", tfidf_matrix.shape)
print("\nTop terms by TF-IDF score in first document:")
doc_tfidf = tfidf_df.iloc[0]  # First doc
top_terms = doc_tfidf.nlargest(5)  # Get 5 largest values
for term, score in top_terms.items():
  print(f"  {term}: {score:.3f}")

"""### Document similarity on the tfidf_matrix

"""

from sklearn.metrics.pairwise import cosine_similarity # Cosine similarity from the lib

print("\nDocument similarity matrix:")
similarity_matrix = cosine_similarity(tfidf_matrix)
similarity_df = pd.DataFrame(
  similarity_matrix,
  index=[f"Doc{i}" for i in range(len(documents))],
  columns=[f"Doc{i}" for i in range(len(documents))]
)
print(similarity_df.round(3))

def find_similar_documents(doc_id, similarity_matrix, documents, top_n=2):
  """Find documents most similar to a given document."""
  similarities = similarity_matrix[doc_id]
  print("\nsimilarities:\n", similarities, "\n")

  similar_indices = np.argsort(similarities)[::-1][1:top_n+1] # argsort() returns indices that would sort array
  print("\nnp.argsort(similarities):", np.argsort(similarities), "\n")

  print(f"\nDocuments similar to: '{documents[doc_id]}'")
  for idx in similar_indices:
    print(f"  Similarity {similarities[idx]:.3f}: '{documents[idx]}'")


find_similar_documents(4, similarity_matrix, documents)

"""# Embeddings with SpaCy (a bit more modern)

### Written in Cython, spaCy is optimized for performance, enabling fast processing of large volumes of text data.
"""

## !python -m spacy download en_core_web_md -q


# Load the model
nlp = spacy.load("en_core_web_md")
print(dir(nlp))

# Process text and get word vectors
text = "The quick brown fox jumps over the lazy dog"

doc = nlp(text)
print(doc)

print("spaCy Word Vectors:")
print(f"Vector dimension: {doc[0].vector.shape[0]}")  # First token's vector dimension


# Word similarity using spaCy
# Process individual words
word1 = nlp("king")
word2 = nlp("queen")
word3 = nlp("car")

print(f"\nWord similarities:")
# .similarity() method computes cosine similarity
print(f"  king - queen: {word1.similarity(word2):.3f}")
print(f"  king - car: {word1.similarity(word3):.3f}")

"""## Contextual Embeddings with Transformers (more modern)"""

print("Loading BERT") # Requires using the HF_TOKEN, I have it loaded on colab
model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name) # AutoTokenizer automatically loads the correct tokenizer for that model

model = AutoModel.from_pretrained(model_name) # AutoModel loads the model architecture and weights

# for i in dir(model):print(i)

# Key architecture details
print(f"Hidden size: {model.config.hidden_size}")
print(f"Num layers: {model.config.num_hidden_layers}")
print(f"Num attention heads: {model.config.num_attention_heads}")
print(f"Vocab size: {model.config.vocab_size}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

print("\n========== LISTING THE LAYERS ========\n")
# List all layers
for name, module in model.named_modules():
    print(f"{name}: {module.__class__.__name__}")
print("\n==================\n")

print("\n")
print(model)

print("\n=== MODEL SUMMARY ===\n")
# Create actual input tensors for BERT
dummy_tokens = torch.randint(0, 1000, (1, 128))  # Random token IDs
summary(model, input_data={'input_ids': dummy_tokens},
        col_names=['input_size', 'output_size', 'num_params'],
        depth=3)
print("\n==================\n")

# Making downloadable viz of the arch of the model
print("\n=== CREATING COMPUTATION GRAPH ===\n")
dummy_input = tokenizer("Hello world", return_tensors="pt")
outputs = model(**dummy_input)
graph = make_dot(outputs.last_hidden_state.mean(), params=dict(model.named_parameters()))
graph.render("bert_graph", format="png")
print("Graph saved as bert_graph.png")

def get_bert_embeddings(text):
  """ Get BERT embeddings for a text."""

  # tokenize and prepare inputs
  inputs = tokenizer(
      text,
      return_tensors="pt", # python tensors
      padding = True, # pads to same length
      truncation = True, # Truncates to max length
      max_length = 512
  )


  # Get token IDs and convert back to tokens
  token_ids = inputs['input_ids'][0]  # Get first sequence
  tokens = tokenizer.convert_ids_to_tokens(token_ids)
  for i in range(min(20, len(tokens))):
    print(f"{i}: {token_ids[i]:5d} -> '{tokens[i]}'")


  with torch.no_grad(): # inference mode to get embeddings, no need on computing gradients
    outputs = model(**inputs) # unpacking dicts as kwards


  # Extracting embeddings
  sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy() # last_hidden_state shape: (batch_size, sequence_length, hidden_size). The [:, 0, :] selects CLS token (sentence representation)
  print("\nsentence_embedding[:,:10]\n")
  print(sentence_embedding[:,:10])
  print("\n")

  # Mean Pooling: averaging all the token embeddigns
  mean_embedding = outputs.last_hidden_state.mean(dim=1).numpy()
  print("\nmean_embedding[:,:10]\n")
  print(mean_embedding[:,:10])
  print("\n")

  return sentence_embedding[0], mean_embedding[0]


sentences = [
    "The bank is by the river",
    "I need to go to the bank to deposit money",
    "The river bank is muddy"
]

embeddings = []

for sentence in sentences:
  cls_emb, mean_emb = get_bert_embeddings(sentence)
  embeddings.append(mean_emb)
  print(f"\nSentence: '{sentence}', with shape: {mean_emb.shape}. First 5 values: {mean_emb[:5].round(3)}.\n")

# Compute similarities
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings)

print("\nContextual similarity matrix:\n")
for i in range(len(sentences)):
  for j in range(i+1, len(sentences)):
    print(f" '{sentences[i]}' vs '{sentences[j]}':"
          f"{similarity_matrix[i,j]:.4f}" )



