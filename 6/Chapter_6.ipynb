{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "821b2afcebfe429bafe4f83f69686a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5da19b9789f4c97aeb9c812c769398f",
              "IPY_MODEL_4a9a02fb07ef451581eb023360794c4e",
              "IPY_MODEL_a34e3c1c709b4fdbb30e4685da572431"
            ],
            "layout": "IPY_MODEL_026792d645844e7ba444f3cf7ee0a3d7"
          }
        },
        "b5da19b9789f4c97aeb9c812c769398f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e366c223ce58479b95964dfdd620470d",
            "placeholder": "​",
            "style": "IPY_MODEL_0afdbd025e9844bdb11598e5a78d0bbe",
            "value": "Batches: 100%"
          }
        },
        "4a9a02fb07ef451581eb023360794c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e596e5dfeb994d6194ba19237c0fd974",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccb776b391184dd4b7b22eec6f0b83ae",
            "value": 7
          }
        },
        "a34e3c1c709b4fdbb30e4685da572431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6810611fd2e345c28504c32a8eaa39dd",
            "placeholder": "​",
            "style": "IPY_MODEL_7c723ed4be8046c4ab93a67f41bbf1ae",
            "value": " 7/7 [00:23&lt;00:00,  2.87s/it]"
          }
        },
        "026792d645844e7ba444f3cf7ee0a3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e366c223ce58479b95964dfdd620470d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0afdbd025e9844bdb11598e5a78d0bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e596e5dfeb994d6194ba19237c0fd974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb776b391184dd4b7b22eec6f0b83ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6810611fd2e345c28504c32a8eaa39dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c723ed4be8046c4ab93a67f41bbf1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim spacy transformers chromadb sentence-transformers -q\n",
        "# !python -m spacy download en_core_web_md -q\n",
        "!pip install PyPDF2 pdfplumber\n",
        "\n",
        "# ## In Colab: Restart session to avoid issues, specially with gensim"
      ],
      "metadata": {
        "id": "nHyB5iIaJB-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b052ae0-902b-4374-ce44-ec517d54b551"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m912.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, PyPDF2, pdfminer.six, pdfplumber\n",
            "Successfully installed PyPDF2-3.0.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import spacy\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torchviz import make_dot\n",
        "\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "import textwrap\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "# Download NLTK data quietly\n",
        "nltk.download('punkt', quiet=True)  # Punkt tokenizer for sentence splitting\n",
        "nltk.download('brown', quiet=True)  # Brown corpus for training data\n",
        "from nltk.corpus import brown # for Word2Vec training with gensim\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0F0wpwgKlkUI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "1d325a15-5fa0-433f-843d-ac2571a16c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchinfo'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-297952289.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word-Document and Word-Word Matrices"
      ],
      "metadata": {
        "id": "FQ7kAcxdlhB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azfXOpePHlb9"
      },
      "outputs": [],
      "source": [
        "# Example documents (Shakespeare plays)\n",
        "documents = {\n",
        "    \"As You Like It\": \"battle good fool wit love forest magic\",\n",
        "    \"Twelfth Night\": \"good fool wit love comedy mistaken identity\",\n",
        "    \"Julius Caesar\": \"battle battle battle good fool war rome politics\",\n",
        "    \"Henry V\": \"battle battle battle battle good wit war king england\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_term_document_matrix(documents):\n",
        "  \"\"\" Creates term-document matrix\n",
        "  Rows represent words (terms)\n",
        "  Columns represent docs\n",
        "  Cells have the frequencies (counts)\"\"\"\n",
        "\n",
        "  all_the_words = set()\n",
        "\n",
        "  for doc in documents.values():\n",
        "    all_the_words.update(doc.split())\n",
        "\n",
        "  vocab = sorted(all_the_words)\n",
        "  print(vocab)\n",
        "\n",
        "  matrix = []\n",
        "\n",
        "  for word in vocab:\n",
        "    row = []\n",
        "\n",
        "    for doc_name, doc_text in documents.items():\n",
        "      count = doc_text.split().count(word)\n",
        "      row.append(count)\n",
        "\n",
        "    matrix.append(row)\n",
        "\n",
        "  print(matrix)\n",
        "\n",
        "\n",
        "  df = pd.DataFrame(matrix,\n",
        "                    index=vocab,\n",
        "                    columns=list(documents.keys()))\n",
        "\n",
        "  return df\n",
        "\n",
        "term_doc_matrix = create_term_document_matrix(documents)\n",
        "print(\"\\n\", term_doc_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sSQ_ki8MT8N",
        "outputId": "7478ff8d-45d0-4bfe-d1af-0259c8a009aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['battle', 'comedy', 'england', 'fool', 'forest', 'good', 'identity', 'king', 'love', 'magic', 'mistaken', 'politics', 'rome', 'war', 'wit']\n",
            "[[1, 0, 3, 4], [0, 1, 0, 0], [0, 0, 0, 1], [1, 1, 1, 0], [1, 0, 0, 0], [1, 1, 1, 1], [0, 1, 0, 0], [0, 0, 0, 1], [1, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 1], [1, 1, 0, 1]]\n",
            "\n",
            "           As You Like It  Twelfth Night  Julius Caesar  Henry V\n",
            "battle                 1              0              3        4\n",
            "comedy                 0              1              0        0\n",
            "england                0              0              0        1\n",
            "fool                   1              1              1        0\n",
            "forest                 1              0              0        0\n",
            "good                   1              1              1        1\n",
            "identity               0              1              0        0\n",
            "king                   0              0              0        1\n",
            "love                   1              1              0        0\n",
            "magic                  1              0              0        0\n",
            "mistaken               0              1              0        0\n",
            "politics               0              0              1        0\n",
            "rome                   0              0              1        0\n",
            "war                    0              0              1        1\n",
            "wit                    1              1              0        1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF\n"
      ],
      "metadata": {
        "id": "-XcBASiWyATG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf_idf(term_doc_matrix):\n",
        "  \"\"\"Compute Term Frequency × Inverse Document Frequency\"\"\"\n",
        "\n",
        "  matrix = term_doc_matrix.values # Convert to numpy.ndarray to apply np.where easily\n",
        "\n",
        "  n_docs = matrix.shape[1]\n",
        "\n",
        "  print(np.where(matrix>0, 1,0))\n",
        "\n",
        "  # Term freq\n",
        "  tf = np.where(matrix > 0, 1 + np.log10(matrix + 1e-10), 0) # Raw counts can be misleading (100 occurrences isn't 100x more important than 1), for taht reason we compress it with the log10. Also, added small epsilon to avoid log(0) and warning\n",
        "  print(\"\\nTerm Freq:\\n\", tf, \"\\n\")\n",
        "\n",
        "  # Doc freq (words appearing on how many docs)\n",
        "  df = np.sum(matrix>0, axis=1)\n",
        "  print(\"\\nDoc Freq:\\n\", df, \"\\n\")\n",
        "\n",
        "  # Inverse doc freq\n",
        "  idf = np.log10(n_docs/df)\n",
        "  print(\"\\nInverse Doc Freq without log(10):\\n\", n_docs/df, \"\\n\")\n",
        "\n",
        "  print(\"\\nInverse Doc Freq (with log(10)):\\n\", idf, \"\\n\")\n",
        "\n",
        "\n",
        "  # TF-IDF\n",
        "  idf = idf[:, np.newaxis] # Flatten idf\n",
        "  print(\"\\nBroadcasted Inverse Doc Freq to dimensionality (n,1):\\n\", idf, \"\\n\")\n",
        "\n",
        "  tf_idf = tf*idf\n",
        "  print(\"\\nTF-IDF:\\n\", tf_idf, \"\\n\")\n",
        "\n",
        "  return pd.DataFrame(tf_idf, index=term_doc_matrix.index, columns=term_doc_matrix.columns)\n",
        "\n",
        "tf_idf_matrix = compute_tf_idf(term_doc_matrix)\n",
        "tf_idf_matrix"
      ],
      "metadata": {
        "id": "nUNgBiI3NjWc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b7c6b77-ccb3-4a3c-ae19-119699105607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 1]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [1 1 1 0]\n",
            " [1 0 0 0]\n",
            " [1 1 1 1]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [0 0 1 0]\n",
            " [0 0 1 1]\n",
            " [1 1 0 1]]\n",
            "\n",
            "Term Freq:\n",
            " [[1.         0.         1.47712125 1.60205999]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         1.        ]\n",
            " [1.         1.         1.         0.        ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [1.         1.         1.         1.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         1.        ]\n",
            " [1.         1.         0.         0.        ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.         0.         1.         1.        ]\n",
            " [1.         1.         0.         1.        ]] \n",
            "\n",
            "\n",
            "Doc Freq:\n",
            " [3 1 1 3 1 4 1 1 2 1 1 1 1 2 3] \n",
            "\n",
            "\n",
            "Inverse Doc Freq without log(10):\n",
            " [1.33333333 4.         4.         1.33333333 4.         1.\n",
            " 4.         4.         2.         4.         4.         4.\n",
            " 4.         2.         1.33333333] \n",
            "\n",
            "\n",
            "Inverse Doc Freq (with log(10)):\n",
            " [0.12493874 0.60205999 0.60205999 0.12493874 0.60205999 0.\n",
            " 0.60205999 0.60205999 0.30103    0.60205999 0.60205999 0.60205999\n",
            " 0.60205999 0.30103    0.12493874] \n",
            "\n",
            "\n",
            "Broadcasted Inverse Doc Freq to dimensionality (n,1):\n",
            " [[0.12493874]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.12493874]\n",
            " [0.60205999]\n",
            " [0.        ]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.30103   ]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.30103   ]\n",
            " [0.12493874]] \n",
            "\n",
            "\n",
            "TF-IDF:\n",
            " [[0.12493874]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.12493874]\n",
            " [0.60205999]\n",
            " [0.        ]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.30103   ]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.60205999]\n",
            " [0.30103   ]\n",
            " [0.12493874]] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          As You Like It  Twelfth Night  Julius Caesar   Henry V\n",
              "battle          0.124939       0.000000       0.184550  0.200159\n",
              "comedy          0.000000       0.602060       0.000000  0.000000\n",
              "england         0.000000       0.000000       0.000000  0.602060\n",
              "fool            0.124939       0.124939       0.124939  0.000000\n",
              "forest          0.602060       0.000000       0.000000  0.000000\n",
              "good            0.000000       0.000000       0.000000  0.000000\n",
              "identity        0.000000       0.602060       0.000000  0.000000\n",
              "king            0.000000       0.000000       0.000000  0.602060\n",
              "love            0.301030       0.301030       0.000000  0.000000\n",
              "magic           0.602060       0.000000       0.000000  0.000000\n",
              "mistaken        0.000000       0.602060       0.000000  0.000000\n",
              "politics        0.000000       0.000000       0.602060  0.000000\n",
              "rome            0.000000       0.000000       0.602060  0.000000\n",
              "war             0.000000       0.000000       0.301030  0.301030\n",
              "wit             0.124939       0.124939       0.000000  0.124939"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da435f75-5b7d-4afa-bb5e-4a98f39fa3c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>As You Like It</th>\n",
              "      <th>Twelfth Night</th>\n",
              "      <th>Julius Caesar</th>\n",
              "      <th>Henry V</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>battle</th>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184550</td>\n",
              "      <td>0.200159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comedy</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>england</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fool</th>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forest</th>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>king</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>magic</th>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mistaken</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politics</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rome</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.602060</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>war</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.301030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wit</th>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.124939</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da435f75-5b7d-4afa-bb5e-4a98f39fa3c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da435f75-5b7d-4afa-bb5e-4a98f39fa3c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da435f75-5b7d-4afa-bb5e-4a98f39fa3c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ebc2efd8-9a4b-4426-aed6-778543da00e3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebc2efd8-9a4b-4426-aed6-778543da00e3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ebc2efd8-9a4b-4426-aed6-778543da00e3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c6142242-6c43-423d-92fa-c695f2935f14\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tf_idf_matrix')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c6142242-6c43-423d-92fa-c695f2935f14 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tf_idf_matrix');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tf_idf_matrix",
              "summary": "{\n  \"name\": \"tf_idf_matrix\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"As You Like It\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2115896292924344,\n        \"min\": 0.0,\n        \"max\": 0.6020599913541096,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.3010299956770548,\n          0.12493873661372594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Twelfth Night\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24478509322208067,\n        \"min\": 0.0,\n        \"max\": 0.6020599913541096,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6020599913541096,\n          0.3010299956770548,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Julius Caesar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21476104570811447,\n        \"min\": 0.0,\n        \"max\": 0.6020599913541096,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.3010299956770548,\n          0.12493873661372594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Henry V\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21512860708592413,\n        \"min\": 0.0,\n        \"max\": 0.6020599913541096,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.12493873661372594,\n          0.6020599913541096\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-Word Co-occurrence (Association)\n"
      ],
      "metadata": {
        "id": "aJPL6zTKPEnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_cooccurrence_matrix(documents, window_size=2):\n",
        "  cooccurrence = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "  for doc in documents.values():\n",
        "    words = doc.split()\n",
        "\n",
        "    for i, target_word in enumerate(words):# print(i,target_word)\n",
        "\n",
        "      start = max(0, i - window_size) # Sets the start of the context window so it is not below 0\n",
        "      end = min(len(words), i + window_size + 1) # Sets the end of the context window so it is not above the last word\n",
        "      print(\"Word \", target_word, \"has a window with indexes range \", start, \"-\", end, \"\\n\")\n",
        "\n",
        "      for j in range(start,end):\n",
        "        if i != j:                  # Not counting the word with itself\n",
        "\n",
        "          context_word = words[j]\n",
        "          cooccurrence[target_word][context_word] += 1\n",
        "\n",
        "      print(\"cooccurrence: \", cooccurrence)\n",
        "\n",
        "\n",
        "  # all_words = []\n",
        "  # for doc in documents.values():\n",
        "  #   for word in doc.split(): all_words.append(word)\n",
        "  # sorted(set(all_words))\n",
        "\n",
        "  all_words = sorted(set(word for doc in documents.values() for word in doc.split(\" \")  )) # Same as above but in a set comprehension\n",
        "  print(\"\\nall_words: \", all_words)\n",
        "\n",
        "  matrix = []\n",
        "  for target_word in all_words:\n",
        "    row = []\n",
        "    for ctxt_word in all_words:\n",
        "      row.append(cooccurrence[target_word][ctxt_word])\n",
        "    matrix.append(row)\n",
        "\n",
        "  return pd.DataFrame(matrix, index = all_words, columns = all_words)\n",
        "\n",
        "cooc_matrix = create_word_cooccurrence_matrix(documents, window_size=2)\n",
        "cooc_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1ZFTGoQK1G5w",
        "outputId": "63b1bd08-3a84-483d-9ee3-da013dead3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word  battle has a window with indexes range  0 - 3 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1})})\n",
            "Word  good has a window with indexes range  0 - 4 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1})})\n",
            "Word  fool has a window with indexes range  0 - 5 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1})})\n",
            "Word  wit has a window with indexes range  1 - 6 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1})})\n",
            "Word  love has a window with indexes range  2 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1})})\n",
            "Word  forest has a window with indexes range  3 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1})})\n",
            "Word  magic has a window with indexes range  4 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 1, 'wit': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1})})\n",
            "Word  good has a window with indexes range  0 - 3 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 1, 'wit': 1, 'love': 1}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1})})\n",
            "Word  fool has a window with indexes range  0 - 4 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'love': 1, 'forest': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1})})\n",
            "Word  wit has a window with indexes range  0 - 5 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 1, 'wit': 1, 'forest': 1, 'magic': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1})})\n",
            "Word  love has a window with indexes range  1 - 6 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1})})\n",
            "Word  comedy has a window with indexes range  2 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1})})\n",
            "Word  mistaken has a window with indexes range  3 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1})})\n",
            "Word  identity has a window with indexes range  4 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  battle has a window with indexes range  0 - 3 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'battle': 2}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  battle has a window with indexes range  0 - 4 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 2, 'fool': 1, 'battle': 4}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  battle has a window with indexes range  0 - 5 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 1, 'fool': 2, 'wit': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  good has a window with indexes range  1 - 6 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 1, 'good': 2, 'wit': 2, 'love': 2}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  fool has a window with indexes range  2 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1})})\n",
            "Word  war has a window with indexes range  3 - 8 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1})})\n",
            "Word  rome has a window with indexes range  4 - 8 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1})})\n",
            "Word  politics has a window with indexes range  5 - 8 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 6}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  battle has a window with indexes range  0 - 3 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 8}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  battle has a window with indexes range  0 - 4 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'battle': 11}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  battle has a window with indexes range  0 - 5 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 4, 'fool': 2, 'battle': 14}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  battle has a window with indexes range  1 - 6 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 3, 'fool': 3, 'wit': 2, 'war': 1}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  good has a window with indexes range  2 - 7 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 5, 'fool': 3, 'wit': 3, 'war': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 2, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  wit has a window with indexes range  3 - 8 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 5, 'fool': 3, 'wit': 3, 'war': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1, 'battle': 1, 'war': 1, 'king': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 1, 'fool': 1, 'rome': 1, 'politics': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  war has a window with indexes range  4 - 9 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 5, 'fool': 3, 'wit': 3, 'war': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1, 'battle': 1, 'war': 1, 'king': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 2, 'fool': 1, 'rome': 1, 'politics': 1, 'wit': 1, 'king': 1, 'england': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1})})\n",
            "Word  king has a window with indexes range  5 - 9 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 5, 'fool': 3, 'wit': 3, 'war': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1, 'battle': 1, 'war': 1, 'king': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 2, 'fool': 1, 'rome': 1, 'politics': 1, 'wit': 1, 'king': 1, 'england': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1}), 'king': defaultdict(<class 'int'>, {'wit': 1, 'war': 1, 'england': 1})})\n",
            "Word  england has a window with indexes range  6 - 9 \n",
            "\n",
            "cooccurrence:  defaultdict(<function create_word_cooccurrence_matrix.<locals>.<lambda> at 0x7f2ce829fba0>, {'battle': defaultdict(<class 'int'>, {'good': 5, 'fool': 2, 'battle': 16, 'wit': 1}), 'good': defaultdict(<class 'int'>, {'battle': 5, 'fool': 3, 'wit': 3, 'war': 2}), 'fool': defaultdict(<class 'int'>, {'battle': 2, 'good': 3, 'wit': 2, 'love': 2, 'war': 1, 'rome': 1}), 'wit': defaultdict(<class 'int'>, {'good': 3, 'fool': 2, 'love': 2, 'forest': 1, 'comedy': 1, 'battle': 1, 'war': 1, 'king': 1}), 'love': defaultdict(<class 'int'>, {'fool': 2, 'wit': 2, 'forest': 1, 'magic': 1, 'comedy': 1, 'mistaken': 1}), 'forest': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'magic': 1}), 'magic': defaultdict(<class 'int'>, {'love': 1, 'forest': 1}), 'comedy': defaultdict(<class 'int'>, {'wit': 1, 'love': 1, 'mistaken': 1, 'identity': 1}), 'mistaken': defaultdict(<class 'int'>, {'love': 1, 'comedy': 1, 'identity': 1}), 'identity': defaultdict(<class 'int'>, {'comedy': 1, 'mistaken': 1}), 'war': defaultdict(<class 'int'>, {'good': 2, 'fool': 1, 'rome': 1, 'politics': 1, 'wit': 1, 'king': 1, 'england': 1}), 'rome': defaultdict(<class 'int'>, {'fool': 1, 'war': 1, 'politics': 1}), 'politics': defaultdict(<class 'int'>, {'war': 1, 'rome': 1}), 'king': defaultdict(<class 'int'>, {'wit': 1, 'war': 1, 'england': 1}), 'england': defaultdict(<class 'int'>, {'war': 1, 'king': 1})})\n",
            "\n",
            "all_words:  ['battle', 'comedy', 'england', 'fool', 'forest', 'good', 'identity', 'king', 'love', 'magic', 'mistaken', 'politics', 'rome', 'war', 'wit']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          battle  comedy  england  fool  forest  good  identity  king  love  \\\n",
              "battle        16       0        0     2       0     5         0     0     0   \n",
              "comedy         0       0        0     0       0     0         1     0     1   \n",
              "england        0       0        0     0       0     0         0     1     0   \n",
              "fool           2       0        0     0       0     3         0     0     2   \n",
              "forest         0       0        0     0       0     0         0     0     1   \n",
              "good           5       0        0     3       0     0         0     0     0   \n",
              "identity       0       1        0     0       0     0         0     0     0   \n",
              "king           0       0        1     0       0     0         0     0     0   \n",
              "love           0       1        0     2       1     0         0     0     0   \n",
              "magic          0       0        0     0       1     0         0     0     1   \n",
              "mistaken       0       1        0     0       0     0         1     0     1   \n",
              "politics       0       0        0     0       0     0         0     0     0   \n",
              "rome           0       0        0     1       0     0         0     0     0   \n",
              "war            0       0        1     1       0     2         0     1     0   \n",
              "wit            1       1        0     2       1     3         0     1     2   \n",
              "\n",
              "          magic  mistaken  politics  rome  war  wit  \n",
              "battle        0         0         0     0    0    1  \n",
              "comedy        0         1         0     0    0    1  \n",
              "england       0         0         0     0    1    0  \n",
              "fool          0         0         0     1    1    2  \n",
              "forest        1         0         0     0    0    1  \n",
              "good          0         0         0     0    2    3  \n",
              "identity      0         1         0     0    0    0  \n",
              "king          0         0         0     0    1    1  \n",
              "love          1         1         0     0    0    2  \n",
              "magic         0         0         0     0    0    0  \n",
              "mistaken      0         0         0     0    0    0  \n",
              "politics      0         0         0     1    1    0  \n",
              "rome          0         0         1     0    1    0  \n",
              "war           0         0         1     1    0    1  \n",
              "wit           0         0         0     0    1    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad86da99-44f1-4ab1-8fa9-d6d10f373f1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battle</th>\n",
              "      <th>comedy</th>\n",
              "      <th>england</th>\n",
              "      <th>fool</th>\n",
              "      <th>forest</th>\n",
              "      <th>good</th>\n",
              "      <th>identity</th>\n",
              "      <th>king</th>\n",
              "      <th>love</th>\n",
              "      <th>magic</th>\n",
              "      <th>mistaken</th>\n",
              "      <th>politics</th>\n",
              "      <th>rome</th>\n",
              "      <th>war</th>\n",
              "      <th>wit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>battle</th>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comedy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>england</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fool</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forest</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>king</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>magic</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mistaken</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politics</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rome</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>war</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wit</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad86da99-44f1-4ab1-8fa9-d6d10f373f1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad86da99-44f1-4ab1-8fa9-d6d10f373f1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad86da99-44f1-4ab1-8fa9-d6d10f373f1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a10bb96-e137-49ce-ae50-6ff076e958a6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a10bb96-e137-49ce-ae50-6ff076e958a6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a10bb96-e137-49ce-ae50-6ff076e958a6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_41a34981-409d-43d5-9cd5-7d29a2f41cda\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('cooc_matrix')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_41a34981-409d-43d5-9cd5-7d29a2f41cda button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('cooc_matrix');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "cooc_matrix",
              "summary": "{\n  \"name\": \"cooc_matrix\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"battle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 16,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comedy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"england\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fool\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"forest\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"good\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"king\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"love\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mistaken\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"politics\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"war\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPMI (Positive Pointwise Mutual Information)"
      ],
      "metadata": {
        "id": "bbZ98YWxY63Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ppmi(cooc_matrix, alpha = 0.75):\n",
        "  \"\"\"\n",
        "  Compute PPMI matrix from the Co-occurrence matrix.\n",
        "  alpha = 0.75: Levy et al. (2015) found that a setting of α = 0.75 improved performance of embeddings on a wide range of tasks\n",
        "  0.75 increases the probability assigned to rare contexts, and hence lowers their PMI (Pα(c) > P(c) when c is rare).\n",
        "  \"\"\"\n",
        "\n",
        "  matrix = cooc_matrix.values.astype(float)\n",
        "\n",
        "  total = np.sum(matrix)\n",
        "\n",
        "\n",
        "  # Joint probabilities P(w,c)\n",
        "  # Element-wise division by scalar\n",
        "  p_wc = matrix / total\n",
        "  print(\"\\n p_wc: \\n\", p_wc, \"\\n\")\n",
        "\n",
        "  # Getting marginal probabilities\n",
        "  p_w = np.sum(matrix, axis=1) / total\n",
        "  p_c = np.sum(matrix, axis=0) / total # Although it is the same bcs it is a symmetric co-occurrence matrix\n",
        "\n",
        "\n",
        "  # Smoothing and re-normalizing\n",
        "  p_c_alpha = np.power(p_c, alpha)\n",
        "  p_c_alpha = p_c_alpha / np.sum(p_c_alpha)\n",
        "\n",
        "  for i,j,k in zip(cooc_matrix.index, p_w, p_c_alpha):print(i,j,round(k,4)) ## p_w = p_c because it is a symmetric matrix, with the power to alpha that changes\n",
        "\n",
        "\n",
        "  # Calculate PMI\n",
        "  epsilon = 1e-10 #Added small epsilon to avoid division by zero and log(0)\n",
        "\n",
        "  pmi = np.log2((p_wc + epsilon)/\n",
        "              (p_w[:,np.newaxis] * p_c_alpha[np.newaxis,:]+epsilon)) # Broadcasting p_w to dimension (1,n)\n",
        "  print(\"\\n PMI: \\n\", pmi, \"\\n\")\n",
        "\n",
        "  # Convert to PPMI (turn negatives to zero)\n",
        "  ppmi = np.maximum(0,pmi)\n",
        "  return pd.DataFrame(ppmi, index=cooc_matrix.index, columns=cooc_matrix.columns)\n",
        "\n",
        "\n",
        "\n",
        "ppmi_matrix = compute_ppmi(cooc_matrix, alpha = 0.75)\n",
        "ppmi_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q60AEmMNgVMa",
        "outputId": "a14eb185-3217-4824-9e9a-1633d0dca33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " p_wc: \n",
            " [[0.16 0.   0.   0.02 0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.01]\n",
            " [0.   0.   0.   0.   0.   0.   0.01 0.   0.01 0.   0.01 0.   0.   0.\n",
            "  0.01]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.01\n",
            "  0.  ]\n",
            " [0.02 0.   0.   0.   0.   0.03 0.   0.   0.02 0.   0.   0.   0.01 0.01\n",
            "  0.02]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.   0.\n",
            "  0.01]\n",
            " [0.05 0.   0.   0.03 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02\n",
            "  0.03]\n",
            " [0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
            "  0.  ]\n",
            " [0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n",
            "  0.01]\n",
            " [0.   0.01 0.   0.02 0.01 0.   0.   0.   0.   0.01 0.01 0.   0.   0.\n",
            "  0.02]\n",
            " [0.   0.   0.   0.   0.01 0.   0.   0.   0.01 0.   0.   0.   0.   0.\n",
            "  0.  ]\n",
            " [0.   0.01 0.   0.   0.   0.   0.01 0.   0.01 0.   0.   0.   0.   0.\n",
            "  0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01\n",
            "  0.  ]\n",
            " [0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.01\n",
            "  0.  ]\n",
            " [0.   0.   0.01 0.01 0.   0.02 0.   0.01 0.   0.   0.   0.01 0.01 0.\n",
            "  0.01]\n",
            " [0.01 0.01 0.   0.02 0.01 0.03 0.   0.01 0.02 0.   0.   0.   0.   0.01\n",
            "  0.  ]] \n",
            "\n",
            "battle 0.24 0.1858\n",
            "comedy 0.04 0.0485\n",
            "england 0.02 0.0288\n",
            "fool 0.11 0.1035\n",
            "forest 0.03 0.0391\n",
            "good 0.13 0.1173\n",
            "identity 0.02 0.0288\n",
            "king 0.03 0.0391\n",
            "love 0.08 0.0815\n",
            "magic 0.02 0.0288\n",
            "mistaken 0.03 0.0391\n",
            "politics 0.02 0.0288\n",
            "rome 0.03 0.0391\n",
            "war 0.08 0.0815\n",
            "wit 0.12 0.1105\n",
            "\n",
            " PMI: \n",
            " [[  1.84333132 -26.79337157 -26.04337158  -0.31252052 -26.48209345\n",
            "    0.8286515  -26.04337158 -26.48209345 -27.54337157 -26.04337158\n",
            "  -26.48209345 -26.04337158 -26.48209345 -27.54337157  -1.40666867]\n",
            " [-26.14713096 -24.20840914 -23.45840919 -25.30298281 -23.89713103\n",
            "  -25.48373888   3.11701559 -23.89713103   1.61701567 -23.45840919\n",
            "    2.67829374 -23.45840919 -23.89713103 -24.95840911   1.1782938 ]\n",
            " [-25.14713098 -23.20840921 -22.45840931 -24.30298285 -22.89713112\n",
            "  -24.48373891 -22.45840931   3.67829365 -23.95840915 -22.45840931\n",
            "  -22.89713112 -22.45840931 -22.89713112   2.61701562 -24.397131  ]\n",
            " [ -0.0311378  -25.66784071 -24.91784073 -26.76241441 -25.35656259\n",
            "    1.21721678 -24.91784073 -25.35656259   1.15758407 -24.91784073\n",
            "  -25.35656259 -24.91784073   1.21886218   0.15758408   0.7188622 ]\n",
            " [-25.73209346 -23.79337166 -23.04337173 -24.88794532 -23.48209356\n",
            "  -25.06870139 -23.04337173 -23.48209356   2.03205315   3.53205304\n",
            "  -23.48209356 -23.04337173 -23.48209356 -24.54337162   1.59333129]\n",
            " [  1.0497822  -25.9088488  -25.15884882   1.15696476 -25.59757068\n",
            "  -27.18417858 -25.15884882 -25.59757068 -26.65884879 -25.15884882\n",
            "  -25.59757068 -25.15884882 -25.59757068   0.91657597   1.0628166 ]\n",
            " [-25.14713098   3.36701556 -22.45840931 -24.30298285 -22.89713112\n",
            "  -24.48373891 -22.45840931 -22.89713112 -23.95840915 -22.45840931\n",
            "    3.67829365 -22.45840931 -22.89713112 -23.95840915 -24.397131  ]\n",
            " [-25.73209346 -23.79337166   3.53205304 -24.88794532 -23.48209356\n",
            "  -25.06870139 -23.04337173 -23.48209356 -24.54337162 -23.04337173\n",
            "  -23.48209356 -23.04337173 -23.48209356   2.03205315   1.59333129]\n",
            " [-27.14713095   1.36701567 -24.45840912   1.27244197   1.67829379\n",
            "  -26.48373887 -24.45840912 -24.89713098 -25.95840908   2.11701565\n",
            "    1.67829379 -24.45840912 -24.89713098 -25.95840908   1.17829381]\n",
            " [-25.14713098 -23.20840921 -22.45840931 -24.30298285   3.67829365\n",
            "  -24.48373891 -22.45840931 -22.89713112   2.61701562 -22.45840931\n",
            "  -22.89713112 -22.45840931 -22.89713112 -23.95840915 -24.397131  ]\n",
            " [-25.73209346   2.78205311 -23.04337173 -24.88794532 -23.48209356\n",
            "  -25.06870139   3.53205304 -23.48209356   2.03205315 -23.04337173\n",
            "  -23.48209356 -23.04337173 -23.48209356 -24.54337162 -24.98209348]\n",
            " [-25.14713098 -23.20840921 -22.45840931 -24.30298285 -22.89713112\n",
            "  -24.48373891 -22.45840931 -22.89713112 -23.95840915 -22.45840931\n",
            "  -22.89713112 -22.45840931   3.67829365   2.61701562 -24.397131  ]\n",
            " [-25.73209346 -23.79337166 -23.04337173   1.68747945 -23.48209356\n",
            "  -25.06870139 -23.04337173 -23.48209356 -24.54337162 -23.04337173\n",
            "  -23.48209356   3.53205304 -23.48209356   2.03205315 -24.98209348]\n",
            " [-27.14713095 -25.2084091    2.11701565   0.27244198 -24.89713098\n",
            "    1.0916859  -24.45840912   1.67829379 -25.95840908 -24.45840912\n",
            "  -24.89713098   2.11701565   1.67829379 -25.95840908   0.17829382]\n",
            " [ -1.15666867   0.78205319 -25.0433716    0.68747948   1.0933313\n",
            "    1.0916859  -25.0433716    1.0933313    1.03205319 -25.0433716\n",
            "  -25.48209347 -25.0433716  -25.48209347   0.0320532  -26.98209345]] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            battle    comedy   england      fool    forest      good  \\\n",
              "battle    1.843331  0.000000  0.000000  0.000000  0.000000  0.828652   \n",
              "comedy    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "england   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "fool      0.000000  0.000000  0.000000  0.000000  0.000000  1.217217   \n",
              "forest    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "good      1.049782  0.000000  0.000000  1.156965  0.000000  0.000000   \n",
              "identity  0.000000  3.367016  0.000000  0.000000  0.000000  0.000000   \n",
              "king      0.000000  0.000000  3.532053  0.000000  0.000000  0.000000   \n",
              "love      0.000000  1.367016  0.000000  1.272442  1.678294  0.000000   \n",
              "magic     0.000000  0.000000  0.000000  0.000000  3.678294  0.000000   \n",
              "mistaken  0.000000  2.782053  0.000000  0.000000  0.000000  0.000000   \n",
              "politics  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "rome      0.000000  0.000000  0.000000  1.687479  0.000000  0.000000   \n",
              "war       0.000000  0.000000  2.117016  0.272442  0.000000  1.091686   \n",
              "wit       0.000000  0.782053  0.000000  0.687479  1.093331  1.091686   \n",
              "\n",
              "          identity      king      love     magic  mistaken  politics  \\\n",
              "battle    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "comedy    3.117016  0.000000  1.617016  0.000000  2.678294  0.000000   \n",
              "england   0.000000  3.678294  0.000000  0.000000  0.000000  0.000000   \n",
              "fool      0.000000  0.000000  1.157584  0.000000  0.000000  0.000000   \n",
              "forest    0.000000  0.000000  2.032053  3.532053  0.000000  0.000000   \n",
              "good      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "identity  0.000000  0.000000  0.000000  0.000000  3.678294  0.000000   \n",
              "king      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "love      0.000000  0.000000  0.000000  2.117016  1.678294  0.000000   \n",
              "magic     0.000000  0.000000  2.617016  0.000000  0.000000  0.000000   \n",
              "mistaken  3.532053  0.000000  2.032053  0.000000  0.000000  0.000000   \n",
              "politics  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "rome      0.000000  0.000000  0.000000  0.000000  0.000000  3.532053   \n",
              "war       0.000000  1.678294  0.000000  0.000000  0.000000  2.117016   \n",
              "wit       0.000000  1.093331  1.032053  0.000000  0.000000  0.000000   \n",
              "\n",
              "              rome       war       wit  \n",
              "battle    0.000000  0.000000  0.000000  \n",
              "comedy    0.000000  0.000000  1.178294  \n",
              "england   0.000000  2.617016  0.000000  \n",
              "fool      1.218862  0.157584  0.718862  \n",
              "forest    0.000000  0.000000  1.593331  \n",
              "good      0.000000  0.916576  1.062817  \n",
              "identity  0.000000  0.000000  0.000000  \n",
              "king      0.000000  2.032053  1.593331  \n",
              "love      0.000000  0.000000  1.178294  \n",
              "magic     0.000000  0.000000  0.000000  \n",
              "mistaken  0.000000  0.000000  0.000000  \n",
              "politics  3.678294  2.617016  0.000000  \n",
              "rome      0.000000  2.032053  0.000000  \n",
              "war       1.678294  0.000000  0.178294  \n",
              "wit       0.000000  0.032053  0.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-639d6ffd-5cb7-4aa5-a4e9-d0f6c31b29ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battle</th>\n",
              "      <th>comedy</th>\n",
              "      <th>england</th>\n",
              "      <th>fool</th>\n",
              "      <th>forest</th>\n",
              "      <th>good</th>\n",
              "      <th>identity</th>\n",
              "      <th>king</th>\n",
              "      <th>love</th>\n",
              "      <th>magic</th>\n",
              "      <th>mistaken</th>\n",
              "      <th>politics</th>\n",
              "      <th>rome</th>\n",
              "      <th>war</th>\n",
              "      <th>wit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>battle</th>\n",
              "      <td>1.843331</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.828652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comedy</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.117016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.617016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.178294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>england</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.617016</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fool</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.217217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.157584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218862</td>\n",
              "      <td>0.157584</td>\n",
              "      <td>0.718862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forest</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.032053</td>\n",
              "      <td>3.532053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.593331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>1.049782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.156965</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916576</td>\n",
              "      <td>1.062817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.367016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>king</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.032053</td>\n",
              "      <td>1.593331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.367016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.272442</td>\n",
              "      <td>1.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.117016</td>\n",
              "      <td>1.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.178294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>magic</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.617016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mistaken</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.782053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.032053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politics</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.678294</td>\n",
              "      <td>2.617016</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rome</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.687479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.032053</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>war</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.117016</td>\n",
              "      <td>0.272442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.091686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.117016</td>\n",
              "      <td>1.678294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.178294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wit</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.782053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687479</td>\n",
              "      <td>1.093331</td>\n",
              "      <td>1.091686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.093331</td>\n",
              "      <td>1.032053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032053</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-639d6ffd-5cb7-4aa5-a4e9-d0f6c31b29ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-639d6ffd-5cb7-4aa5-a4e9-d0f6c31b29ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-639d6ffd-5cb7-4aa5-a4e9-d0f6c31b29ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-40bc902c-5f87-4ff7-b938-a6b31599f489\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40bc902c-5f87-4ff7-b938-a6b31599f489')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-40bc902c-5f87-4ff7-b938-a6b31599f489 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9eafd4ac-e216-4f4c-a351-b30e3d0e5b3c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ppmi_matrix')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9eafd4ac-e216-4f4c-a351-b30e3d0e5b3c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ppmi_matrix');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ppmi_matrix",
              "summary": "{\n  \"name\": \"ppmi_matrix\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"battle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.530626715350918,\n        \"min\": 0.0,\n        \"max\": 1.8433313187026639,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.8433313187026639,\n          0.0,\n          1.0497821954159947\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comedy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1008550763381018,\n        \"min\": 0.0,\n        \"max\": 3.367015562876085,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.367015562876085,\n          0.7820531861961373,\n          1.367015674513172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"england\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0292052698590561,\n        \"min\": 0.0,\n        \"max\": 3.5320530441150884,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          3.5320530441150884,\n          2.1170156491420493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fool\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5753167162969306,\n        \"min\": 0.0,\n        \"max\": 1.68747945055814,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          1.1569647599862143,\n          0.6874794781958653\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"forest\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.027078292043907,\n        \"min\": 0.0,\n        \"max\": 3.678293651491308,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.6782937900113264,\n          1.0933313046812845,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"good\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.48985455406334016,\n        \"min\": 0.0,\n        \"max\": 1.2172167844048303,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          1.091685903252703,\n          1.2172167844048303\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.172416429033008,\n        \"min\": 0.0,\n        \"max\": 3.5320530441150884,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          3.1170155865585656,\n          3.5320530441150884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"king\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0270782920439068,\n        \"min\": 0.0,\n        \"max\": 3.678293651491308,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.678293651491308,\n          1.0933313046812845,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"love\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9559673623844284,\n        \"min\": 0.0,\n        \"max\": 2.617015623219121,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          1.6170156674723277,\n          1.0320531890398357\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0292052698590564,\n        \"min\": 0.0,\n        \"max\": 3.5320530441150884,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          3.5320530441150884,\n          2.1170156491420493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mistaken\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1715623644448594,\n        \"min\": 0.0,\n        \"max\": 3.678293651491308,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.6782937438379855,\n          1.6782937900113264,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"politics\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0292052698590561,\n        \"min\": 0.0,\n        \"max\": 3.5320530441150884,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          3.5320530441150884,\n          2.1170156491420493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.033361484390017,\n        \"min\": 0.0,\n        \"max\": 3.678293651491308,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.218862183966759,\n          1.6782937900113264,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"war\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0557763629186385,\n        \"min\": 0.0,\n        \"max\": 2.617015623219121,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          2.617015623219121,\n          0.0320531962533106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6424878524733432,\n        \"min\": 0.0,\n        \"max\": 1.593331291930868,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.0,\n          1.1782938035351853,\n          1.1782938126464522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity"
      ],
      "metadata": {
        "id": "99JUaoRMxntf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "  \"\"\"\n",
        "  Compute cosine similarity between two vectors\n",
        "  Dot product divide by the product of the Norms of both to normalize them\n",
        "  cosine(v1, v2) = (v1 . v2) / (|v1| x |v1|)\n",
        "  \"\"\"\n",
        "\n",
        "  dot_prod = np.dot(vec1, vec2)\n",
        "\n",
        "  magnitude1 = np.sqrt(np.sum(vec1**2))\n",
        "  magnitude2 = np.sqrt(np.sum(vec2**2))\n",
        "\n",
        "  # Prevent div by zero\n",
        "  if magnitude1 == 0 or magnitude2 == 0: return 0\n",
        "\n",
        "  return np.dot(vec1,vec2)/ (magnitude1 * magnitude2)\n",
        "\n"
      ],
      "metadata": {
        "id": "wqFNwLTf1Iyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usecase for Cosine Similarity"
      ],
      "metadata": {
        "id": "kzrLTaVEzB6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_words(word,matrix,top_n=3):\n",
        "  \"\"\"Finds the most similar words to a target word.\"\"\"\n",
        "\n",
        "  if word not in matrix.index:\n",
        "    return([])\n",
        "\n",
        "  target_vector = matrix.loc[word].values # Convert to pandas series\n",
        "  similarities = []\n",
        "\n",
        "  for other_word in matrix.index:\n",
        "    if other_word != word:\n",
        "      other_vector = matrix.loc[other_word].values\n",
        "      sim = cosine_similarity(target_vector,other_vector)\n",
        "      similarities.append((other_word, sim))\n",
        "\n",
        "  similarities.sort(key=lambda x:x[1], reverse=True)  # Sort by similarity in descending order\n",
        "\n",
        "  return similarities[:top_n]\n",
        "\n",
        "find_similar_words('battle', tf_idf_matrix, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqmyzWuk1Ivy",
        "outputId": "3d86f5ac-d575-4b0d-f9ae-e5a50104bda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('war', 0.9081207804878239),\n",
              " ('england', 0.6681933257466228),\n",
              " ('king', 0.6681933257466228)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple Word2Vec Implementation (Skip-gram concept)"
      ],
      "metadata": {
        "id": "-epuJMId1S8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "TUZWuPZ01bYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "  \"\"\"Simplified Skipgram\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim=10):\n",
        "    # Random initialization multiplied by small value\n",
        "    self.W = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
        "    self.C = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def forward(self, target_idx, context_idx):\n",
        "    \"Conpute P(context_word|target_word)\"\n",
        "\n",
        "    # Get embeddings by indexing into matrices\n",
        "    target_embedding = self.W[target_idx]\n",
        "    context_embedding = self.C[context_idx]\n",
        "\n",
        "    # Dot product and sigmoid\n",
        "    dot_product = np.dot(target_embedding, context_embedding)\n",
        "    probability = sigmoid(dot_product)\n",
        "\n",
        "    return probability\n",
        "\n",
        "  def train_pair(self, target_idx, context_idx, label, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Train on a single target-context pair.\n",
        "    label: 1 for positive (real context), 0 for negative (noise)\n",
        "    \"\"\"\n",
        "\n",
        "    # Forward pass\n",
        "    prob = self.forward(target_idx, context_idx)\n",
        "\n",
        "    gradient  = (prob-label) # derivative of loss w.r.t. activation\n",
        "\n",
        "    # Store original values before updating\n",
        "    w_original = self.W[target_idx].copy()\n",
        "    c_original = self.C[context_idx].copy()\n",
        "\n",
        "    # Update embeddings with gradient descent\n",
        "    # ∂L/∂W[target] = gradient * C[context]\n",
        "    self.W[target_idx] -= learning_rate * gradient * c_original\n",
        "    self.C[context_idx] -= learning_rate * gradient * w_original\n",
        "\n",
        "    print(\"\")\n",
        "    print(self.W[target_idx], self.C[target_idx] )\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "id": "JZfs_4LHm98n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usecase of SkipGram"
      ],
      "metadata": {
        "id": "gjFOdTpLCID-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['battle', 'good', 'fool', 'wit', 'love']\n",
        "\n",
        "vocab_to_idx = {word: idx for idx,word in enumerate(vocab)}\n",
        "print(vocab_to_idx)\n",
        "\n",
        "model = SkipGram(len(vocab), embedding_dim=5)\n",
        "\n",
        "print(\"\\nTraining Skip-gram:\")\n",
        "print(\"Initial embedding for 'battle':\", model.W[vocab_to_idx['battle']].round(3))\n",
        "\n",
        "\n",
        "for _ in range(20):\n",
        "  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['good'], 1) # Positive example: 'battle' appears with 'good'\n",
        "  model.train_pair(vocab_to_idx['battle'], vocab_to_idx['love'], 0) # Negative example: 'battle' doesn't appear with 'love'\n",
        "\n",
        "print(\"Updated embedding for 'battle':\", model.W[vocab_to_idx['battle']].round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3zgFZk4CGzM",
        "outputId": "9f27b510-6ab6-43bb-b046-498c8682c84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'battle': 0, 'good': 1, 'fool': 2, 'wit': 3, 'love': 4}\n",
            "\n",
            "Training Skip-gram:\n",
            "Initial embedding for 'battle': [ 0.051  0.014 -0.137 -0.119 -0.133]\n",
            "\n",
            "[ 0.05103227  0.0145106  -0.13710094 -0.11761859 -0.13316623] [ 0.02259421  0.0129427   0.07595019 -0.01104619  0.02594421]\n",
            "\n",
            "\n",
            "[ 0.05081601  0.01444493 -0.13774327 -0.11723552 -0.13359638] [ 0.02233563  0.0128692   0.07665108 -0.01044965  0.02662401]\n",
            "\n",
            "\n",
            "[ 0.05110116  0.01470047 -0.13742465 -0.11593757 -0.13341427] [ 0.02258791  0.01294177  0.07597263 -0.01102202  0.02596535]\n",
            "\n",
            "\n",
            "[ 0.0508849   0.0146348  -0.13806696 -0.1155545  -0.13384441] [ 0.02232899  0.0128673   0.07667516 -0.01043404  0.0266464 ]\n",
            "\n",
            "\n",
            "[ 0.05117009  0.01489037 -0.1377483  -0.11425638 -0.13366228] [ 0.02258165  0.01294083  0.07599502 -0.01099819  0.02598643]\n",
            "\n",
            "\n",
            "[ 0.05095384  0.0148247  -0.1383906  -0.11387333 -0.13409242] [ 0.02232239  0.0128654   0.07669918 -0.01041878  0.02666872]\n",
            "\n",
            "\n",
            "[ 0.05123907  0.01508031 -0.1380719  -0.11257504 -0.13391026] [ 0.02257541  0.01293987  0.07601735 -0.0109747   0.02600744]\n",
            "\n",
            "\n",
            "[ 0.05102282  0.01501464 -0.13871419 -0.11219199 -0.13434038] [ 0.0223158   0.01286347  0.07672315 -0.01040385  0.02669098]\n",
            "\n",
            "\n",
            "[ 0.05130809  0.01527028 -0.13839545 -0.11089353 -0.13415821] [ 0.02256921  0.01293889  0.07603963 -0.01095154  0.0260284 ]\n",
            "\n",
            "\n",
            "[ 0.05109185  0.01520462 -0.13903772 -0.11051049 -0.13458832] [ 0.02230925  0.01286153  0.07674706 -0.01038926  0.02671318]\n",
            "\n",
            "\n",
            "[ 0.05137715  0.01546029 -0.13871894 -0.10921186 -0.13440612] [ 0.02256303  0.01293789  0.07606186 -0.01092871  0.02604928]\n",
            "\n",
            "\n",
            "[ 0.05116091  0.01539463 -0.1393612  -0.10882883 -0.13483623] [ 0.02230273  0.01285957  0.07677091 -0.010375    0.02673532]\n",
            "\n",
            "\n",
            "[ 0.05144625  0.01565033 -0.13904237 -0.10753003 -0.134654  ] [ 0.02255688  0.01293688  0.07608402 -0.01090622  0.02607011]\n",
            "\n",
            "\n",
            "[ 0.05123002  0.01558467 -0.13968462 -0.10714701 -0.1350841 ] [ 0.02229623  0.01285759  0.07679471 -0.01036107  0.02675739]\n",
            "\n",
            "\n",
            "[ 0.0515154   0.01584041 -0.13936575 -0.10584804 -0.13490185] [ 0.02255076  0.01293585  0.07610613 -0.01088405  0.02609087]\n",
            "\n",
            "\n",
            "[ 0.05129917  0.01577475 -0.14000798 -0.10546503 -0.13533193] [ 0.02228977  0.0128556   0.07681845 -0.01034747  0.02677939]\n",
            "\n",
            "\n",
            "[ 0.05158458  0.01603052 -0.13968907 -0.10416589 -0.13514966] [ 0.02254467  0.01293481  0.07612818 -0.0108622   0.02611156]\n",
            "\n",
            "\n",
            "[ 0.05136836  0.01596486 -0.14033129 -0.10378289 -0.13557974] [ 0.02228333  0.01285359  0.07684213 -0.0103342   0.02680133]\n",
            "\n",
            "\n",
            "[ 0.05165381  0.01622067 -0.14001234 -0.10248358 -0.13539744] [ 0.0225386   0.01293375  0.07615018 -0.01084068  0.02613219]\n",
            "\n",
            "\n",
            "[ 0.05143759  0.01615501 -0.14065454 -0.10210058 -0.13582751] [ 0.02227692  0.01285156  0.07686575 -0.01032125  0.0268232 ]\n",
            "\n",
            "\n",
            "[ 0.05172308  0.01641085 -0.14033555 -0.10080111 -0.13564519] [ 0.02253257  0.01293268  0.07617211 -0.01081948  0.02615275]\n",
            "\n",
            "\n",
            "[ 0.05150686  0.0163452  -0.14097774 -0.10041811 -0.13607525] [ 0.02227054  0.01284952  0.07688932 -0.01030862  0.02684501]\n",
            "\n",
            "\n",
            "[ 0.05179239  0.01660107 -0.14065871 -0.09911847 -0.13589291] [ 0.02252656  0.01293159  0.07619399 -0.0107986   0.02617324]\n",
            "\n",
            "\n",
            "[ 0.05157618  0.01653541 -0.14130089 -0.09873549 -0.13632295] [ 0.02226418  0.01284747  0.07691282 -0.01029631  0.02686675]\n",
            "\n",
            "\n",
            "[ 0.05186174  0.01679132 -0.14098181 -0.09743567 -0.13614059] [ 0.02252059  0.01293049  0.07621581 -0.01077803  0.02619367]\n",
            "\n",
            "\n",
            "[ 0.05164554  0.01672567 -0.14162397 -0.0970527  -0.13657063] [ 0.02225786  0.0128454   0.07693627 -0.01028431  0.02688842]\n",
            "\n",
            "\n",
            "[ 0.05193114  0.01698161 -0.14130486 -0.09575271 -0.13638824] [ 0.02251464  0.01292937  0.07623756 -0.01075777  0.02621403]\n",
            "\n",
            "\n",
            "[ 0.05171494  0.01691595 -0.14194701 -0.09536975 -0.13681827] [ 0.02225157  0.01284332  0.07695965 -0.01027262  0.02691003]\n",
            "\n",
            "\n",
            "[ 0.05200057  0.01717193 -0.14162785 -0.09406959 -0.13663585] [ 0.02250873  0.01292824  0.07625926 -0.01073783  0.02623432]\n",
            "\n",
            "\n",
            "[ 0.05178438  0.01710628 -0.14226998 -0.09368664 -0.13706587] [ 0.0222453   0.01284122  0.07698298 -0.01026125  0.02693157]\n",
            "\n",
            "\n",
            "[ 0.05207005  0.01736228 -0.14195078 -0.09238631 -0.13688344] [ 0.02250284  0.01292709  0.0762809  -0.01071819  0.02625455]\n",
            "\n",
            "\n",
            "[ 0.05185386  0.01729663 -0.1425929  -0.09200336 -0.13731345] [ 0.02223907  0.01283911  0.07700624 -0.01025018  0.02695304]\n",
            "\n",
            "\n",
            "[ 0.05213957  0.01755267 -0.14227366 -0.09070287 -0.13713099] [ 0.02249698  0.01292593  0.07630247 -0.01069885  0.02627471]\n",
            "\n",
            "\n",
            "[ 0.05192339  0.01748703 -0.14291577 -0.09031993 -0.13756099] [ 0.02223286  0.01283698  0.07702944 -0.01023942  0.02697444]\n",
            "\n",
            "\n",
            "[ 0.05220914  0.0177431  -0.14259649 -0.08901927 -0.1373785 ] [ 0.02249115  0.01292476  0.07632398 -0.01067982  0.02629479]\n",
            "\n",
            "\n",
            "[ 0.05199295  0.01767745 -0.14323858 -0.08863633 -0.1378085 ] [ 0.02222669  0.01283484  0.07705258 -0.01022896  0.02699577]\n",
            "\n",
            "\n",
            "[ 0.05227874  0.01793356 -0.14291926 -0.0873355  -0.13762599] [ 0.02248535  0.01292357  0.07634543 -0.01066109  0.02631481]\n",
            "\n",
            "\n",
            "[ 0.05206256  0.01786791 -0.14356134 -0.08695258 -0.13805597] [ 0.02222054  0.01283269  0.07707565 -0.01021881  0.02701703]\n",
            "\n",
            "\n",
            "[ 0.05234839  0.01812406 -0.14324197 -0.08565157 -0.13787344] [ 0.02247959  0.01292238  0.07636682 -0.01064266  0.02633476]\n",
            "\n",
            "\n",
            "[ 0.05213221  0.01805841 -0.14388404 -0.08526866 -0.13830341] [ 0.02221442  0.01283053  0.07709866 -0.01020895  0.02703823]\n",
            "\n",
            "Updated embedding for 'battle': [ 0.052  0.018 -0.144 -0.085 -0.138]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Visualization of Embeddings with t-SNE"
      ],
      "metadata": {
        "id": "dEkdrTP7Ffzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_embeddings(embeddings, labels):\n",
        "  \"\"\"Visualize high-dimensional embedding in two dimensions with t-Distributed Stochastic Neighbor Embedding\"\"\"\n",
        "\n",
        "  # Reducing dimensionality to 2 with t-SNE\n",
        "  if embeddings.shape[1] > 2 and embeddings.shape[0] > 5: # Checking that there are enough samples\n",
        "    # Adjust perplexity based on number of samples\n",
        "    ppl = min(30,embeddings.shape[0]-1) # 30 is the default, but must be < n_samples\n",
        "    tsne = TSNE(n_components = 2, random_state=42, perplexity=ppl)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings)\n",
        "  else:\n",
        "    embeddings_2d = embeddings[:,:2] # Takes just 2 first dimensions\n",
        "\n",
        "  print(embeddings_2d)\n",
        "\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
        "\n",
        "  # Adding labels\n",
        "  for i,label in enumerate(labels):\n",
        "    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "  plt.title(\"Word Embeddings\")\n",
        "  plt.xlabel(\"Dim 1\")\n",
        "  plt.ylabel(\"Dim 2\")\n",
        "  plt.grid(True, alpha=.3)\n",
        "  plt.show()\n",
        "\n",
        "if ppmi_matrix.shape[0]>0:\n",
        "  print(\"Visualizing word embeddings from PPMI matrix\")\n",
        "  embeddings = ppmi_matrix.values\n",
        "  labels = ppmi_matrix.index.tolist()\n",
        "  visualize_embeddings(embeddings, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "qHIQKsVBEkKp",
        "outputId": "f2030e65-57ab-4336-8d32-7a55b64922a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing word embeddings from PPMI matrix\n",
            "[[ 27.061226    19.571266  ]\n",
            " [ 41.878567   -13.310328  ]\n",
            " [ 42.025864    10.631084  ]\n",
            " [  5.84458      8.318714  ]\n",
            " [ 50.117714    -0.27983123]\n",
            " [ 29.434593     6.7900376 ]\n",
            " [ 29.489666   -19.734055  ]\n",
            " [ 13.671186    21.93513   ]\n",
            " [ 37.44587     -1.3878596 ]\n",
            " [ 17.967104    -1.2565985 ]\n",
            " [ 27.606993    -7.166381  ]\n",
            " [  6.0496125   -5.7165866 ]\n",
            " [ 18.413963    10.150228  ]\n",
            " [ 39.11019     22.808086  ]\n",
            " [ 16.349966   -15.06282   ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec (with Gensim lib instead, common approach)\n"
      ],
      "metadata": {
        "id": "aCfAxyIHIrlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using brown corpus\n",
        "sentences = brown.sents()#[:10000]\n",
        "print(sentences[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1ZI4ahzEVp0",
        "outputId": "b01d4808-ab7f-4da2-d3c6-c8bc0d5ec464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Word2Vec\n",
        "model = Word2Vec(\n",
        "    sentences = sentences,\n",
        "    vector_size = 100, # Embedding dimensionality\n",
        "    window = 5,\n",
        "    min_count = 5, # Ignoring words with frequency < 5\n",
        "    workers = 4, # Parallel threads\n",
        "    sg = 1, # Skip-gram (1) or Continuous Bag-of-Words (0)\n",
        "    negative = 5, # Negative samples per each positive\n",
        "    epochs = 10 # iters over the whole corpus\n",
        ")"
      ],
      "metadata": {
        "id": "QZD3K5ArJZvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMost similar words to 'man':\")\n",
        "similar_words = model.wv.most_similar('man', topn=5)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word}: {similarity:.3f}\")\n",
        "### Interesting results\n",
        "\n",
        "print(\"\\nMost similar words to 'Christ':\")\n",
        "similar_words = model.wv.most_similar('Christ', topn=5)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word}: {similarity:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVhh_mqYL5AJ",
        "outputId": "d47c9500-ea2e-4c63-a1d4-a5ec6230ac42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar words to 'man':\n",
            "  woman: 0.632\n",
            "  boy: 0.622\n",
            "  citizen: 0.599\n",
            "  monk: 0.599\n",
            "  guy: 0.598\n",
            "\n",
            "Most similar words to 'Christ':\n",
            "  Jesus: 0.825\n",
            "  eternal: 0.804\n",
            "  God: 0.800\n",
            "  Son: 0.791\n",
            "  Saviour: 0.762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word analogies\n",
        "print(\"\\nWord analogy: father - man + woman = ?\")\n",
        "# positive: words to add, negative: words to subtract\n",
        "result = model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)\n",
        "print(f\"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})\")\n",
        "# Probably needs more data in the corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dheRAr_UL_47",
        "outputId": "54979b42-7ca8-40b9-dc31-4dac94c9ea9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word analogy: father - man + woman = ?\n",
            "  Result: mother (similarity: 0.720)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word analogies\n",
        "print(\"\\nWord analogy: queen - woman + man = ?\")\n",
        "# positive: words to add, negative: words to subtract\n",
        "result = model.wv.most_similar(positive=['queen', 'man'], negative=['woman'], topn=1)\n",
        "print(f\"  Result: {result[0][0]} (similarity: {result[0][1]:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PImXryjRCy6e",
        "outputId": "22f1f2a5-a70b-4aa0-ced1-e749566b848c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word analogy: queen - woman + man = ?\n",
            "  Result: presiding (similarity: 0.655)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization in 2D with PCA"
      ],
      "metadata": {
        "id": "Mur9RQvTDtjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_word_vectors(model, words):\n",
        "  \"\"\"Plot word vectors in 2-D with Princpal Component Analysis\"\"\"\n",
        "\n",
        "  word_vectors = np.array([model.wv[word] for word in words if word in model.wv])\n",
        "  word_labels = [word for word in words if word in model.wv]\n",
        "\n",
        "  print(\"word_vectors shpae:\", word_vectors.shape)\n",
        "  print(\"word_vectors[:5,:5]: \\n\", str(word_vectors[:5,:5]))\n",
        "  print(\"word_labels: \\n\", word_labels)\n",
        "\n",
        "\n",
        "  # Reducing dimensionality to just 2-D with PCA\n",
        "  pca = PCA(n_components = 2)\n",
        "  vectors_2d = pca.fit_transform(word_vectors)\n",
        "\n",
        "  print(\"vectors_2d with PCA:\\n\", vectors_2d)\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.scatter(vectors_2d[:,0], vectors_2d[:,1])\n",
        "  plt.title(\"Word2Vec Embeddings with PCA projection into 2-D\")\n",
        "  plt.grid(True, alpha =.3)\n",
        "\n",
        "  for i, word in enumerate(word_labels):\n",
        "    plt.annotate(word, xy=(vectors_2d[i,0], vectors_2d[i,1]), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "words_to_plot = ['brave', 'fear', 'devil', 'hero' ,'monster','doctor','faith', 'eternal', 'health','king', 'queen', 'man', 'woman', 'prince', 'princess', 'boy', 'girl', 'father', 'mother', 'god', 'noise_asdfasdf', 'power', 'money',  'glory', 'God', 'Son', 'Jesus', 'Christ']\n",
        "plot_word_vectors(model, words_to_plot)\n"
      ],
      "metadata": {
        "id": "iQFHrf55N7dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b1d6c7d-f114-4014-a585-c91ac4b374da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_vectors shpae: (26, 100)\n",
            "word_vectors[:5,:5]: \n",
            " [[ 0.04189809 -0.19704002  0.15089552  0.11237249  0.14436695]\n",
            " [ 0.2228708   0.11076368  0.49441546  0.29954737  0.03207644]\n",
            " [-0.00621594  0.04499584  0.07347209 -0.13069215  0.08310986]\n",
            " [-0.36987075  0.1555789   0.13787106  0.09389531  0.22582097]\n",
            " [ 0.28475466  0.05514208  0.23694187 -0.06801151  0.0141738 ]]\n",
            "word_labels: \n",
            " ['brave', 'fear', 'devil', 'hero', 'monster', 'doctor', 'faith', 'eternal', 'health', 'king', 'queen', 'man', 'woman', 'princess', 'boy', 'girl', 'father', 'mother', 'god', 'power', 'money', 'glory', 'God', 'Son', 'Jesus', 'Christ']\n",
            "vectors_2d with PCA:\n",
            " [[ 3.4723446e-01 -5.5621654e-01]\n",
            " [-6.2840217e-01 -3.8991493e-01]\n",
            " [ 6.0459793e-02 -3.2063973e-01]\n",
            " [-1.7839107e-01  3.6523989e-01]\n",
            " [ 2.9235309e-01 -8.4042764e-01]\n",
            " [ 1.2630624e+00 -2.2122906e-01]\n",
            " [-1.0984217e+00 -3.4366462e-02]\n",
            " [-1.0092980e+00  1.3243965e-03]\n",
            " [ 4.0589109e-02 -1.6540844e+00]\n",
            " [-8.1661254e-02 -5.2211982e-01]\n",
            " [ 7.6459473e-01 -1.1590302e-01]\n",
            " [ 6.3067377e-01  1.1787002e+00]\n",
            " [ 1.4662167e+00  8.7875485e-01]\n",
            " [ 4.0039897e-01 -6.4457619e-01]\n",
            " [ 1.6293654e+00  8.4028870e-01]\n",
            " [ 1.7174560e+00  9.8564452e-01]\n",
            " [ 4.4007960e-01  4.5186272e-01]\n",
            " [ 1.0219395e+00  3.6778811e-01]\n",
            " [-3.8019270e-01 -5.2601552e-01]\n",
            " [-1.2223898e+00 -1.1882215e+00]\n",
            " [ 6.0798109e-01 -1.5270101e+00]\n",
            " [-4.8201114e-01 -3.7921855e-01]\n",
            " [-1.7958272e+00  1.3089569e+00]\n",
            " [-8.4724462e-01  5.9621114e-01]\n",
            " [-1.1601213e+00  1.0517004e+00]\n",
            " [-1.7984434e+00  8.9347124e-01]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Scikit-learn"
      ],
      "metadata": {
        "id": "5LIZM7EFHs2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"A quick brown dog outpaces a quick fox\",\n",
        "    \"The lazy cat sleeps all day long\",\n",
        "    \"Machine learning is fascinating and powerful\",\n",
        "    \"Deep learning revolutionized natural language processing\",\n",
        "    \"Natural language processing helps computers understand human language\"\n",
        "]"
      ],
      "metadata": {
        "id": "MQ2GGQUcH41X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features = 100, # Max vocab size\n",
        "    ngram_range = (1,2), # Use unigrams and bigrams (1-word and 2-word phrases)\n",
        "    stop_words = 'english', # Removing English stopwords\n",
        "    lowercase = True, # Converts to lowercase\n",
        "    use_idf = True,   # IDF weighting\n",
        "    smooth_idf = True, # Add-1 (Laplace) Smoothing\n",
        "    sublinear_tf = True # Uses log(tf) instead of just tf\n",
        ")\n",
        "\n",
        "# vectorizer\n",
        "\n",
        "# Fit and trasnform docs\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"tfidf_matrix[1]\\n\")\n",
        "for i in tfidf_matrix[1]: print(i)\n",
        "\n",
        "print(\"feature_names:\\n \",feature_names)\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "  tfidf_matrix.toarray(),\n",
        "  columns=feature_names,\n",
        "  index=[f\"Doc{i}\" for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(\"\\ntfidf_df.iloc[1][:20]\\n\")\n",
        "print(tfidf_df.iloc[1][:20])\n",
        "\n",
        "print(\"TF-IDF Matrix shape:\", tfidf_matrix.shape)\n",
        "print(\"\\nTop terms by TF-IDF score in first document:\")\n",
        "doc_tfidf = tfidf_df.iloc[0]  # First doc\n",
        "top_terms = doc_tfidf.nlargest(5)  # Get 5 largest values\n",
        "for term, score in top_terms.items():\n",
        "  print(f\"  {term}: {score:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF9FGh8SG4Sz",
        "outputId": "749fc3af-01d6-44d4-834e-b85657c06a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_matrix[1]\n",
            "\n",
            "  (0, 41)\t0.4477010965312605\n",
            "  (0, 0)\t0.264419479695321\n",
            "  (0, 15)\t0.264419479695321\n",
            "  (0, 11)\t0.264419479695321\n",
            "  (0, 42)\t0.264419479695321\n",
            "  (0, 36)\t0.3224571547338746\n",
            "  (0, 1)\t0.3224571547338746\n",
            "  (0, 12)\t0.3224571547338746\n",
            "  (0, 37)\t0.3224571547338746\n",
            "  (0, 43)\t0.3224571547338746\n",
            "feature_names:\n",
            "  ['brown' 'brown dog' 'brown fox' 'cat' 'cat sleeps' 'computers'\n",
            " 'computers understand' 'day' 'day long' 'deep' 'deep learning' 'dog'\n",
            " 'dog outpaces' 'fascinating' 'fascinating powerful' 'fox' 'fox jumps'\n",
            " 'helps' 'helps computers' 'human' 'human language' 'jumps' 'jumps lazy'\n",
            " 'language' 'language processing' 'lazy' 'lazy cat' 'lazy dog' 'learning'\n",
            " 'learning fascinating' 'learning revolutionized' 'long' 'machine'\n",
            " 'machine learning' 'natural' 'natural language' 'outpaces'\n",
            " 'outpaces quick' 'powerful' 'processing' 'processing helps' 'quick'\n",
            " 'quick brown' 'quick fox' 'revolutionized' 'revolutionized natural'\n",
            " 'sleeps' 'sleeps day' 'understand' 'understand human']\n",
            "\n",
            "tfidf_df.iloc[1][:20]\n",
            "\n",
            "brown                   0.264419\n",
            "brown dog               0.322457\n",
            "brown fox               0.000000\n",
            "cat                     0.000000\n",
            "cat sleeps              0.000000\n",
            "computers               0.000000\n",
            "computers understand    0.000000\n",
            "day                     0.000000\n",
            "day long                0.000000\n",
            "deep                    0.000000\n",
            "deep learning           0.000000\n",
            "dog                     0.264419\n",
            "dog outpaces            0.322457\n",
            "fascinating             0.000000\n",
            "fascinating powerful    0.000000\n",
            "fox                     0.264419\n",
            "fox jumps               0.000000\n",
            "helps                   0.000000\n",
            "helps computers         0.000000\n",
            "human                   0.000000\n",
            "Name: Doc1, dtype: float64\n",
            "TF-IDF Matrix shape: (6, 50)\n",
            "\n",
            "Top terms by TF-IDF score in first document:\n",
            "  brown fox: 0.333\n",
            "  fox jumps: 0.333\n",
            "  jumps: 0.333\n",
            "  jumps lazy: 0.333\n",
            "  lazy dog: 0.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document similarity on the tfidf_matrix\n"
      ],
      "metadata": {
        "id": "KMnjUb-bKwxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity # Cosine similarity from the lib\n",
        "\n",
        "print(\"\\nDocument similarity matrix:\")\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "similarity_df = pd.DataFrame(\n",
        "  similarity_matrix,\n",
        "  index=[f\"Doc{i}\" for i in range(len(documents))],\n",
        "  columns=[f\"Doc{i}\" for i in range(len(documents))]\n",
        ")\n",
        "print(similarity_df.round(3))\n",
        "\n",
        "def find_similar_documents(doc_id, similarity_matrix, documents, top_n=2):\n",
        "  \"\"\"Find documents most similar to a given document.\"\"\"\n",
        "  similarities = similarity_matrix[doc_id]\n",
        "  print(\"\\nsimilarities:\\n\", similarities, \"\\n\")\n",
        "\n",
        "  similar_indices = np.argsort(similarities)[::-1][1:top_n+1] # argsort() returns indices that would sort array\n",
        "  print(\"\\nnp.argsort(similarities):\", np.argsort(similarities), \"\\n\")\n",
        "\n",
        "  print(f\"\\nDocuments similar to: '{documents[doc_id]}'\")\n",
        "  for idx in similar_indices:\n",
        "    print(f\"  Similarity {similarities[idx]:.3f}: '{documents[idx]}'\")\n",
        "\n",
        "\n",
        "find_similar_documents(4, similarity_matrix, documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxuUy2-CJc00",
        "outputId": "08494e8b-55e1-4a96-f283-df383fa9d0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document similarity matrix:\n",
            "       Doc0   Doc1   Doc2   Doc3   Doc4   Doc5\n",
            "Doc0  1.000  0.411  0.076  0.000  0.000  0.000\n",
            "Doc1  0.411  1.000  0.000  0.000  0.000  0.000\n",
            "Doc2  0.076  0.000  1.000  0.000  0.000  0.000\n",
            "Doc3  0.000  0.000  0.000  1.000  0.087  0.000\n",
            "Doc4  0.000  0.000  0.000  0.087  1.000  0.345\n",
            "Doc5  0.000  0.000  0.000  0.000  0.345  1.000\n",
            "\n",
            "similarities:\n",
            " [0.         0.         0.         0.08660601 1.         0.34514035] \n",
            "\n",
            "\n",
            "np.argsort(similarities): [0 1 2 3 5 4] \n",
            "\n",
            "\n",
            "Documents similar to: 'Deep learning revolutionized natural language processing'\n",
            "  Similarity 0.345: 'Natural language processing helps computers understand human language'\n",
            "  Similarity 0.087: 'Machine learning is fascinating and powerful'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings with SpaCy (a bit more modern)\n",
        "\n",
        "### Written in Cython, spaCy is optimized for performance, enabling fast processing of large volumes of text data."
      ],
      "metadata": {
        "id": "NK-XiI5gLisn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## !python -m spacy download en_core_web_md -q\n",
        "\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "print(dir(nlp))\n",
        "\n",
        "# Process text and get word vectors\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "\n",
        "print(\"spaCy Word Vectors:\")\n",
        "print(f\"Vector dimension: {doc[0].vector.shape[0]}\")  # First token's vector dimension\n",
        "\n",
        "\n",
        "# Word similarity using spaCy\n",
        "# Process individual words\n",
        "word1 = nlp(\"king\")\n",
        "word2 = nlp(\"queen\")\n",
        "word3 = nlp(\"car\")\n",
        "\n",
        "print(f\"\\nWord similarities:\")\n",
        "# .similarity() method computes cosine similarity\n",
        "print(f\"  king - queen: {word1.similarity(word2):.3f}\")\n",
        "print(f\"  king - car: {word1.similarity(word3):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCAot2phLt3D",
        "outputId": "54ae76a8-73eb-48b9-f6c2-07327be6568c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Defaults', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_components', '_config', '_disabled', '_ensure_doc', '_ensure_doc_with_context', '_factory_meta', '_get_pipe_index', '_has_gpu_model', '_link_components', '_meta', '_multiprocessing_pipe', '_optimizer', '_path', '_pipe_configs', '_pipe_meta', '_resolve_component_status', 'add_pipe', 'analyze_pipes', 'batch_size', 'begin_training', 'component', 'component_names', 'components', 'config', 'create_optimizer', 'create_pipe', 'create_pipe_from_source', 'default_config', 'default_error_handler', 'disable_pipe', 'disable_pipes', 'disabled', 'enable_pipe', 'evaluate', 'factories', 'factory', 'factory_names', 'from_bytes', 'from_config', 'from_disk', 'get_factory_meta', 'get_factory_name', 'get_pipe', 'get_pipe_config', 'get_pipe_meta', 'has_factory', 'has_pipe', 'initialize', 'lang', 'make_doc', 'max_length', 'memory_zone', 'meta', 'path', 'pipe', 'pipe_factories', 'pipe_labels', 'pipe_names', 'pipeline', 'rehearse', 'remove_pipe', 'rename_pipe', 'replace_listeners', 'replace_pipe', 'resume_training', 'select_pipes', 'set_error_handler', 'set_factory_meta', 'to_bytes', 'to_disk', 'tokenizer', 'update', 'use_params', 'vocab']\n",
            "The quick brown fox jumps over the lazy dog\n",
            "spaCy Word Vectors:\n",
            "Vector dimension: 300\n",
            "\n",
            "Word similarities:\n",
            "  king - queen: 0.383\n",
            "  king - car: 0.142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual Embeddings with Transformers (more modern)"
      ],
      "metadata": {
        "id": "zwYMl1SnOIvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading BERT\") # Requires using the HF_TOKEN, I have it loaded on colab\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # AutoTokenizer automatically loads the correct tokenizer for that model\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name) # AutoModel loads the model architecture and weights\n",
        "\n",
        "# for i in dir(model):print(i)\n",
        "\n",
        "# Key architecture details\n",
        "print(f\"Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"Num layers: {model.config.num_hidden_layers}\")\n",
        "print(f\"Num attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"Vocab size: {model.config.vocab_size}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "print(\"\\n========== LISTING THE LAYERS ========\\n\")\n",
        "# List all layers\n",
        "for name, module in model.named_modules():\n",
        "    print(f\"{name}: {module.__class__.__name__}\")\n",
        "print(\"\\n==================\\n\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n=== MODEL SUMMARY ===\\n\")\n",
        "# Create actual input tensors for BERT\n",
        "dummy_tokens = torch.randint(0, 1000, (1, 128))  # Random token IDs\n",
        "summary(model, input_data={'input_ids': dummy_tokens},\n",
        "        col_names=['input_size', 'output_size', 'num_params'],\n",
        "        depth=3)\n",
        "print(\"\\n==================\\n\")\n",
        "\n",
        "# Making downloadable viz of the arch of the model\n",
        "print(\"\\n=== CREATING COMPUTATION GRAPH ===\\n\")\n",
        "dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
        "outputs = model(**dummy_input)\n",
        "graph = make_dot(outputs.last_hidden_state.mean(), params=dict(model.named_parameters()))\n",
        "graph.render(\"bert_graph\", format=\"png\")\n",
        "print(\"Graph saved as bert_graph.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6-kjxFKoNfeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e526f43c-691e-46c8-9191-a0074927b31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT\n",
            "Hidden size: 768\n",
            "Num layers: 12\n",
            "Num attention heads: 12\n",
            "Vocab size: 30522\n",
            "Total parameters: 109,482,240\n",
            "\n",
            "========== LISTING THE LAYERS ========\n",
            "\n",
            ": BertModel\n",
            "embeddings: BertEmbeddings\n",
            "embeddings.word_embeddings: Embedding\n",
            "embeddings.position_embeddings: Embedding\n",
            "embeddings.token_type_embeddings: Embedding\n",
            "embeddings.LayerNorm: LayerNorm\n",
            "embeddings.dropout: Dropout\n",
            "encoder: BertEncoder\n",
            "encoder.layer: ModuleList\n",
            "encoder.layer.0: BertLayer\n",
            "encoder.layer.0.attention: BertAttention\n",
            "encoder.layer.0.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.0.attention.self.query: Linear\n",
            "encoder.layer.0.attention.self.key: Linear\n",
            "encoder.layer.0.attention.self.value: Linear\n",
            "encoder.layer.0.attention.self.dropout: Dropout\n",
            "encoder.layer.0.attention.output: BertSelfOutput\n",
            "encoder.layer.0.attention.output.dense: Linear\n",
            "encoder.layer.0.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.0.attention.output.dropout: Dropout\n",
            "encoder.layer.0.intermediate: BertIntermediate\n",
            "encoder.layer.0.intermediate.dense: Linear\n",
            "encoder.layer.0.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.0.output: BertOutput\n",
            "encoder.layer.0.output.dense: Linear\n",
            "encoder.layer.0.output.LayerNorm: LayerNorm\n",
            "encoder.layer.0.output.dropout: Dropout\n",
            "encoder.layer.1: BertLayer\n",
            "encoder.layer.1.attention: BertAttention\n",
            "encoder.layer.1.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.1.attention.self.query: Linear\n",
            "encoder.layer.1.attention.self.key: Linear\n",
            "encoder.layer.1.attention.self.value: Linear\n",
            "encoder.layer.1.attention.self.dropout: Dropout\n",
            "encoder.layer.1.attention.output: BertSelfOutput\n",
            "encoder.layer.1.attention.output.dense: Linear\n",
            "encoder.layer.1.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.1.attention.output.dropout: Dropout\n",
            "encoder.layer.1.intermediate: BertIntermediate\n",
            "encoder.layer.1.intermediate.dense: Linear\n",
            "encoder.layer.1.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.1.output: BertOutput\n",
            "encoder.layer.1.output.dense: Linear\n",
            "encoder.layer.1.output.LayerNorm: LayerNorm\n",
            "encoder.layer.1.output.dropout: Dropout\n",
            "encoder.layer.2: BertLayer\n",
            "encoder.layer.2.attention: BertAttention\n",
            "encoder.layer.2.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.2.attention.self.query: Linear\n",
            "encoder.layer.2.attention.self.key: Linear\n",
            "encoder.layer.2.attention.self.value: Linear\n",
            "encoder.layer.2.attention.self.dropout: Dropout\n",
            "encoder.layer.2.attention.output: BertSelfOutput\n",
            "encoder.layer.2.attention.output.dense: Linear\n",
            "encoder.layer.2.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.2.attention.output.dropout: Dropout\n",
            "encoder.layer.2.intermediate: BertIntermediate\n",
            "encoder.layer.2.intermediate.dense: Linear\n",
            "encoder.layer.2.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.2.output: BertOutput\n",
            "encoder.layer.2.output.dense: Linear\n",
            "encoder.layer.2.output.LayerNorm: LayerNorm\n",
            "encoder.layer.2.output.dropout: Dropout\n",
            "encoder.layer.3: BertLayer\n",
            "encoder.layer.3.attention: BertAttention\n",
            "encoder.layer.3.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.3.attention.self.query: Linear\n",
            "encoder.layer.3.attention.self.key: Linear\n",
            "encoder.layer.3.attention.self.value: Linear\n",
            "encoder.layer.3.attention.self.dropout: Dropout\n",
            "encoder.layer.3.attention.output: BertSelfOutput\n",
            "encoder.layer.3.attention.output.dense: Linear\n",
            "encoder.layer.3.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.3.attention.output.dropout: Dropout\n",
            "encoder.layer.3.intermediate: BertIntermediate\n",
            "encoder.layer.3.intermediate.dense: Linear\n",
            "encoder.layer.3.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.3.output: BertOutput\n",
            "encoder.layer.3.output.dense: Linear\n",
            "encoder.layer.3.output.LayerNorm: LayerNorm\n",
            "encoder.layer.3.output.dropout: Dropout\n",
            "encoder.layer.4: BertLayer\n",
            "encoder.layer.4.attention: BertAttention\n",
            "encoder.layer.4.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.4.attention.self.query: Linear\n",
            "encoder.layer.4.attention.self.key: Linear\n",
            "encoder.layer.4.attention.self.value: Linear\n",
            "encoder.layer.4.attention.self.dropout: Dropout\n",
            "encoder.layer.4.attention.output: BertSelfOutput\n",
            "encoder.layer.4.attention.output.dense: Linear\n",
            "encoder.layer.4.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.4.attention.output.dropout: Dropout\n",
            "encoder.layer.4.intermediate: BertIntermediate\n",
            "encoder.layer.4.intermediate.dense: Linear\n",
            "encoder.layer.4.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.4.output: BertOutput\n",
            "encoder.layer.4.output.dense: Linear\n",
            "encoder.layer.4.output.LayerNorm: LayerNorm\n",
            "encoder.layer.4.output.dropout: Dropout\n",
            "encoder.layer.5: BertLayer\n",
            "encoder.layer.5.attention: BertAttention\n",
            "encoder.layer.5.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.5.attention.self.query: Linear\n",
            "encoder.layer.5.attention.self.key: Linear\n",
            "encoder.layer.5.attention.self.value: Linear\n",
            "encoder.layer.5.attention.self.dropout: Dropout\n",
            "encoder.layer.5.attention.output: BertSelfOutput\n",
            "encoder.layer.5.attention.output.dense: Linear\n",
            "encoder.layer.5.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.5.attention.output.dropout: Dropout\n",
            "encoder.layer.5.intermediate: BertIntermediate\n",
            "encoder.layer.5.intermediate.dense: Linear\n",
            "encoder.layer.5.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.5.output: BertOutput\n",
            "encoder.layer.5.output.dense: Linear\n",
            "encoder.layer.5.output.LayerNorm: LayerNorm\n",
            "encoder.layer.5.output.dropout: Dropout\n",
            "encoder.layer.6: BertLayer\n",
            "encoder.layer.6.attention: BertAttention\n",
            "encoder.layer.6.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.6.attention.self.query: Linear\n",
            "encoder.layer.6.attention.self.key: Linear\n",
            "encoder.layer.6.attention.self.value: Linear\n",
            "encoder.layer.6.attention.self.dropout: Dropout\n",
            "encoder.layer.6.attention.output: BertSelfOutput\n",
            "encoder.layer.6.attention.output.dense: Linear\n",
            "encoder.layer.6.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.6.attention.output.dropout: Dropout\n",
            "encoder.layer.6.intermediate: BertIntermediate\n",
            "encoder.layer.6.intermediate.dense: Linear\n",
            "encoder.layer.6.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.6.output: BertOutput\n",
            "encoder.layer.6.output.dense: Linear\n",
            "encoder.layer.6.output.LayerNorm: LayerNorm\n",
            "encoder.layer.6.output.dropout: Dropout\n",
            "encoder.layer.7: BertLayer\n",
            "encoder.layer.7.attention: BertAttention\n",
            "encoder.layer.7.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.7.attention.self.query: Linear\n",
            "encoder.layer.7.attention.self.key: Linear\n",
            "encoder.layer.7.attention.self.value: Linear\n",
            "encoder.layer.7.attention.self.dropout: Dropout\n",
            "encoder.layer.7.attention.output: BertSelfOutput\n",
            "encoder.layer.7.attention.output.dense: Linear\n",
            "encoder.layer.7.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.7.attention.output.dropout: Dropout\n",
            "encoder.layer.7.intermediate: BertIntermediate\n",
            "encoder.layer.7.intermediate.dense: Linear\n",
            "encoder.layer.7.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.7.output: BertOutput\n",
            "encoder.layer.7.output.dense: Linear\n",
            "encoder.layer.7.output.LayerNorm: LayerNorm\n",
            "encoder.layer.7.output.dropout: Dropout\n",
            "encoder.layer.8: BertLayer\n",
            "encoder.layer.8.attention: BertAttention\n",
            "encoder.layer.8.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.8.attention.self.query: Linear\n",
            "encoder.layer.8.attention.self.key: Linear\n",
            "encoder.layer.8.attention.self.value: Linear\n",
            "encoder.layer.8.attention.self.dropout: Dropout\n",
            "encoder.layer.8.attention.output: BertSelfOutput\n",
            "encoder.layer.8.attention.output.dense: Linear\n",
            "encoder.layer.8.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.8.attention.output.dropout: Dropout\n",
            "encoder.layer.8.intermediate: BertIntermediate\n",
            "encoder.layer.8.intermediate.dense: Linear\n",
            "encoder.layer.8.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.8.output: BertOutput\n",
            "encoder.layer.8.output.dense: Linear\n",
            "encoder.layer.8.output.LayerNorm: LayerNorm\n",
            "encoder.layer.8.output.dropout: Dropout\n",
            "encoder.layer.9: BertLayer\n",
            "encoder.layer.9.attention: BertAttention\n",
            "encoder.layer.9.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.9.attention.self.query: Linear\n",
            "encoder.layer.9.attention.self.key: Linear\n",
            "encoder.layer.9.attention.self.value: Linear\n",
            "encoder.layer.9.attention.self.dropout: Dropout\n",
            "encoder.layer.9.attention.output: BertSelfOutput\n",
            "encoder.layer.9.attention.output.dense: Linear\n",
            "encoder.layer.9.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.9.attention.output.dropout: Dropout\n",
            "encoder.layer.9.intermediate: BertIntermediate\n",
            "encoder.layer.9.intermediate.dense: Linear\n",
            "encoder.layer.9.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.9.output: BertOutput\n",
            "encoder.layer.9.output.dense: Linear\n",
            "encoder.layer.9.output.LayerNorm: LayerNorm\n",
            "encoder.layer.9.output.dropout: Dropout\n",
            "encoder.layer.10: BertLayer\n",
            "encoder.layer.10.attention: BertAttention\n",
            "encoder.layer.10.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.10.attention.self.query: Linear\n",
            "encoder.layer.10.attention.self.key: Linear\n",
            "encoder.layer.10.attention.self.value: Linear\n",
            "encoder.layer.10.attention.self.dropout: Dropout\n",
            "encoder.layer.10.attention.output: BertSelfOutput\n",
            "encoder.layer.10.attention.output.dense: Linear\n",
            "encoder.layer.10.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.10.attention.output.dropout: Dropout\n",
            "encoder.layer.10.intermediate: BertIntermediate\n",
            "encoder.layer.10.intermediate.dense: Linear\n",
            "encoder.layer.10.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.10.output: BertOutput\n",
            "encoder.layer.10.output.dense: Linear\n",
            "encoder.layer.10.output.LayerNorm: LayerNorm\n",
            "encoder.layer.10.output.dropout: Dropout\n",
            "encoder.layer.11: BertLayer\n",
            "encoder.layer.11.attention: BertAttention\n",
            "encoder.layer.11.attention.self: BertSdpaSelfAttention\n",
            "encoder.layer.11.attention.self.query: Linear\n",
            "encoder.layer.11.attention.self.key: Linear\n",
            "encoder.layer.11.attention.self.value: Linear\n",
            "encoder.layer.11.attention.self.dropout: Dropout\n",
            "encoder.layer.11.attention.output: BertSelfOutput\n",
            "encoder.layer.11.attention.output.dense: Linear\n",
            "encoder.layer.11.attention.output.LayerNorm: LayerNorm\n",
            "encoder.layer.11.attention.output.dropout: Dropout\n",
            "encoder.layer.11.intermediate: BertIntermediate\n",
            "encoder.layer.11.intermediate.dense: Linear\n",
            "encoder.layer.11.intermediate.intermediate_act_fn: GELUActivation\n",
            "encoder.layer.11.output: BertOutput\n",
            "encoder.layer.11.output.dense: Linear\n",
            "encoder.layer.11.output.LayerNorm: LayerNorm\n",
            "encoder.layer.11.output.dropout: Dropout\n",
            "pooler: BertPooler\n",
            "pooler.dense: Linear\n",
            "pooler.activation: Tanh\n",
            "\n",
            "==================\n",
            "\n",
            "\n",
            "\n",
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSdpaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n",
            "\n",
            "=== MODEL SUMMARY ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1793: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  result = forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================\n",
            "\n",
            "\n",
            "=== CREATING COMPUTATION GRAPH ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph saved as bert_graph.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(text):\n",
        "  \"\"\" Get BERT embeddings for a text.\"\"\"\n",
        "\n",
        "  # tokenize and prepare inputs\n",
        "  inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\", # python tensors\n",
        "    padding = True, # pads to same length\n",
        "    truncation = True, # Truncates to max length\n",
        "    max_length = 512\n",
        "  )\n",
        "\n",
        "\n",
        "  # Get token IDs and convert back to tokens\n",
        "  token_ids = inputs['input_ids'][0]  # Get first sequence\n",
        "  tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "  for i in range(min(20, len(tokens))):\n",
        "    print(f\"{i}: {token_ids[i]:5d} -> '{tokens[i]}'\")\n",
        "\n",
        "\n",
        "  with torch.no_grad(): # inference mode to get embeddings, no need on computing gradients\n",
        "    outputs = model(**inputs) # unpacking dicts as kwards\n",
        "\n",
        "\n",
        "  # Extracting embeddings\n",
        "  sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy() # last_hidden_state shape: (batch_size, sequence_length, hidden_size). The [:, 0, :] selects CLS token (sentence representation)\n",
        "  print(\"\\nsentence_embedding[:,:10]\\n\")\n",
        "  print(sentence_embedding[:,:10])\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # Mean Pooling: averaging all the token embeddigns\n",
        "  mean_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "  print(\"\\nmean_embedding[:,:10]\\n\")\n",
        "  print(mean_embedding[:,:10])\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return sentence_embedding[0], mean_embedding[0]\n",
        "\n",
        "\n",
        "# Test sentences showcasing different linguistic phenomena\n",
        "sentences = [\n",
        "  # Polysemy: \"bank\" (financial vs. river)\n",
        "  \"The bank is by the river\",\n",
        "  \"I need to go to the bank to deposit money\",\n",
        "  \"The river bank is muddy\",\n",
        "  \"She works at the central bank\",\n",
        "  \"The boat docked at the river bank\",\n",
        "  \"My bank account is overdrawn\",\n",
        "\n",
        "  # Polysemy: \"bat\" (animal vs. sports equipment)\n",
        "  \"The bat flew out of the cave at dusk\",\n",
        "  \"She swung the bat and hit a home run\",\n",
        "  \"Bats sleep hanging upside down\",\n",
        "  \"He bought a new cricket bat\",\n",
        "  \"The vampire bat feeds on blood\",\n",
        "\n",
        "  # Context changes meaning: \"light\"\n",
        "  \"The feather is very light\",\n",
        "  \"Please turn on the light\",\n",
        "  \"She wore a light blue dress\",\n",
        "  \"Light travels faster than sound\",\n",
        "  \"This suitcase feels light\",\n",
        "  \"The room needs more light\",\n",
        "\n",
        "  # Synonyms in different contexts\n",
        "  \"The doctor prescribed medicine for my cold\",\n",
        "  \"The physician recommended medication for my illness\",\n",
        "  \"The surgeon performed the operation\",\n",
        "  \"The medic treated the wounded soldier\",\n",
        "\n",
        "  # Idioms vs. literal meaning\n",
        "  \"It's raining cats and dogs outside\",\n",
        "  \"The cats and dogs are playing together\",\n",
        "  \"Break a leg at your performance tonight\",\n",
        "  \"He literally broke his leg skiing\",\n",
        "\n",
        "  # Technical vs. common usage\n",
        "  \"I need to debug this Python code\",\n",
        "  \"The bug crawled across the keyboard\",\n",
        "  \"The software has a critical bug\",\n",
        "  \"I found a bug in my salad\"\n",
        "]\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  cls_emb, mean_emb = get_bert_embeddings(sentence)\n",
        "  embeddings.append(mean_emb)\n",
        "  print(f\"\\nSentence: '{sentence}', with shape: {mean_emb.shape}. First 5 values: {mean_emb[:5].round(3)}.\\n\")\n",
        "\n",
        "# Compute similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"\\nContextual similarity matrix:\\n\")\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(i+1, len(sentences)):\n",
        "    print(f\" '{sentences[i]}' vs '{sentences[j]}':\"\n",
        "          f\"{similarity_matrix[i,j]:.4f}\" )"
      ],
      "metadata": {
        "id": "u7AxrMieMKzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ada561-33d6-45e3-cdef-ed9f9d211c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  2924 -> 'bank'\n",
            "3:  2003 -> 'is'\n",
            "4:  2011 -> 'by'\n",
            "5:  1996 -> 'the'\n",
            "6:  2314 -> 'river'\n",
            "7:   102 -> '[SEP]'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.26922476  0.1791691   0.00322632 -0.21722725  0.19217332 -0.14972234\n",
            "   0.25192466  0.40964547  0.00103763 -0.37182572]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.2837183  -0.02020967 -0.13097188 -0.15327573  0.3872578   0.03079577\n",
            "   0.03031972  0.7190123   0.16200946 -0.4066363 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The bank is by the river', with shape: (768,). First 5 values: [-0.284 -0.02  -0.131 -0.153  0.387].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1045 -> 'i'\n",
            "2:  2342 -> 'need'\n",
            "3:  2000 -> 'to'\n",
            "4:  2175 -> 'go'\n",
            "5:  2000 -> 'to'\n",
            "6:  1996 -> 'the'\n",
            "7:  2924 -> 'bank'\n",
            "8:  2000 -> 'to'\n",
            "9: 12816 -> 'deposit'\n",
            "10:  2769 -> 'money'\n",
            "11:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.36173242  0.46019268  0.10844228 -0.25013417 -0.05892862 -0.16726668\n",
            "   0.24827677  0.2632707   0.19325846 -0.1615366 ]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.42910492 -0.01528945  0.10143626  0.19277531  0.39588583 -0.4461132\n",
            "   0.17052828  0.27796343  0.12179431 -0.12477583]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'I need to go to the bank to deposit money', with shape: (768,). First 5 values: [ 0.429 -0.015  0.101  0.193  0.396].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  2314 -> 'river'\n",
            "3:  2924 -> 'bank'\n",
            "4:  2003 -> 'is'\n",
            "5: 15405 -> 'muddy'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.24150193  0.21342771 -0.27860332 -0.13920166  0.08225192 -0.11034325\n",
            "   0.14281274  0.8688093  -0.14044322 -0.48718882]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.17057492 -0.11038925 -0.6159825   0.00155332  0.3284865   0.24197587\n",
            "   0.12562832  1.0659797   0.14065872 -0.5426466 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The river bank is muddy', with shape: (768,). First 5 values: [-0.171 -0.11  -0.616  0.002  0.328].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2016 -> 'she'\n",
            "2:  2573 -> 'works'\n",
            "3:  2012 -> 'at'\n",
            "4:  1996 -> 'the'\n",
            "5:  2430 -> 'central'\n",
            "6:  2924 -> 'bank'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-1.1613514e-01  3.2131030e-04 -1.6432828e-01 -2.4343006e-01\n",
            "  -2.3530731e-02 -2.4845954e-02 -4.7808192e-03  4.2727894e-01\n",
            "  -3.5656002e-01  1.5663938e-01]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.12586021 -0.37744483 -0.30198306 -0.01022309  0.37830096  0.21837425\n",
            "  -0.21528147  0.4188242  -0.4916468   0.1011202 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'She works at the central bank', with shape: (768,). First 5 values: [ 0.126 -0.377 -0.302 -0.01   0.378].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  4049 -> 'boat'\n",
            "3: 25727 -> 'docked'\n",
            "4:  2012 -> 'at'\n",
            "5:  1996 -> 'the'\n",
            "6:  2314 -> 'river'\n",
            "7:  2924 -> 'bank'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.10392255  0.07587826  0.31796902  0.04181034 -0.11005386 -0.0575226\n",
            "   0.21811508  0.26922148  0.01873151 -0.17065142]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.307268   -0.30114388 -0.04000554  0.25922564 -0.09317502  0.03342059\n",
            "   0.11639561  0.43365106  0.05623937 -0.5182421 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The boat docked at the river bank', with shape: (768,). First 5 values: [ 0.307 -0.301 -0.04   0.259 -0.093].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2026 -> 'my'\n",
            "2:  2924 -> 'bank'\n",
            "3:  4070 -> 'account'\n",
            "4:  2003 -> 'is'\n",
            "5:  2058 -> 'over'\n",
            "6:  7265 -> '##dra'\n",
            "7:  7962 -> '##wn'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.10535668  0.33989373 -0.0705804  -0.44639185 -0.23719375 -0.12687688\n",
            "   0.28750718  0.487554    0.06943932 -0.02398001]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.18258524 -0.11383338 -0.07234195 -0.37054297  0.0229497  -0.16282421\n",
            "  -0.34307817  0.71093565 -0.355375    0.04813909]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'My bank account is overdrawn', with shape: (768,). First 5 values: [ 0.183 -0.114 -0.072 -0.371  0.023].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  7151 -> 'bat'\n",
            "3:  5520 -> 'flew'\n",
            "4:  2041 -> 'out'\n",
            "5:  1997 -> 'of'\n",
            "6:  1996 -> 'the'\n",
            "7:  5430 -> 'cave'\n",
            "8:  2012 -> 'at'\n",
            "9: 18406 -> 'dusk'\n",
            "10:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.04729353  0.13737544  0.0425021  -0.08157925 -0.23130675  0.03099467\n",
            "   0.32274348  0.4867051   0.00418795 -0.26215297]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.00921661 -0.12342327 -0.12539656 -0.10739987 -0.06680185  0.05585397\n",
            "   0.47655532  0.7039799  -0.01904102 -0.3093623 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The bat flew out of the cave at dusk', with shape: (768,). First 5 values: [ 0.009 -0.123 -0.125 -0.107 -0.067].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2016 -> 'she'\n",
            "2:  7671 -> 'swung'\n",
            "3:  1996 -> 'the'\n",
            "4:  7151 -> 'bat'\n",
            "5:  1998 -> 'and'\n",
            "6:  2718 -> 'hit'\n",
            "7:  1037 -> 'a'\n",
            "8:  2188 -> 'home'\n",
            "9:  2448 -> 'run'\n",
            "10:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.14872953 -0.25535324 -0.12763673  0.31561443 -0.06901435 -0.276428\n",
            "   0.09209574  0.5020606   0.02725618  0.05089525]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.03995825 -0.58406776 -0.1310321   0.11167876 -0.16688675 -0.2464431\n",
            "   0.09312736  0.3550638   0.27647012 -0.19140817]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'She swung the bat and hit a home run', with shape: (768,). First 5 values: [ 0.04  -0.584 -0.131  0.112 -0.167].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1: 12236 -> 'bats'\n",
            "2:  3637 -> 'sleep'\n",
            "3:  5689 -> 'hanging'\n",
            "4: 14961 -> 'upside'\n",
            "5:  2091 -> 'down'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.06058588  0.3042907  -0.05751703  0.01930736 -0.05653737 -0.2782754\n",
            "   0.5863369   0.9426239  -0.6289433  -0.21073359]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.4740308   0.44145742  0.11524872  0.08424516  0.00869456 -0.39799467\n",
            "   0.09795129  0.84894216 -0.52290714 -0.24545835]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'Bats sleep hanging upside down', with shape: (768,). First 5 values: [0.474 0.441 0.115 0.084 0.009].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2002 -> 'he'\n",
            "2:  4149 -> 'bought'\n",
            "3:  1037 -> 'a'\n",
            "4:  2047 -> 'new'\n",
            "5:  4533 -> 'cricket'\n",
            "6:  7151 -> 'bat'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.20273243  0.23617096 -0.08452126 -0.04637901 -0.0827391   0.01526973\n",
            "   0.2824146   0.30880755 -0.24700524 -0.0572329 ]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.34015673 -0.05387594 -0.04152701 -0.06272785  0.01805193 -0.06771591\n",
            "   0.15277818  0.30631807 -0.04051914 -0.04300248]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'He bought a new cricket bat', with shape: (768,). First 5 values: [ 0.34  -0.054 -0.042 -0.063  0.018].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  4393 -> 'vampire'\n",
            "3:  7151 -> 'bat'\n",
            "4: 14172 -> 'feeds'\n",
            "5:  2006 -> 'on'\n",
            "6:  2668 -> 'blood'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.18791096  0.3183902  -0.19498539  0.00880336 -0.29517096  0.2525734\n",
            "   0.44739428  0.53000855 -0.14141712 -0.26076683]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.4539984   0.26446924 -0.11901659 -0.03716208  0.01925904  0.05599174\n",
            "   0.02992663  0.45842782  0.13925365 -0.49563825]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The vampire bat feeds on blood', with shape: (768,). First 5 values: [ 0.454  0.264 -0.119 -0.037  0.019].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2: 15550 -> 'feather'\n",
            "3:  2003 -> 'is'\n",
            "4:  2200 -> 'very'\n",
            "5:  2422 -> 'light'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.43680504  0.13649152 -0.269733   -0.23732616 -0.10865348 -0.04779699\n",
            "   0.08251211  0.5498759   0.03996972 -0.53866005]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.2003762  -0.22056058 -0.37997344  0.02940835  0.161113    0.2130728\n",
            "  -0.00332165  0.45589703  0.02278319 -0.62712497]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The feather is very light', with shape: (768,). First 5 values: [-0.2   -0.221 -0.38   0.029  0.161].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  3531 -> 'please'\n",
            "2:  2735 -> 'turn'\n",
            "3:  2006 -> 'on'\n",
            "4:  1996 -> 'the'\n",
            "5:  2422 -> 'light'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.05569554  0.3370253   0.02294884 -0.03564028  0.14111997 -0.34107408\n",
            "   0.6091106   0.4887203  -0.06243861 -0.49682984]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.024522    0.18861847  0.06766231  0.04691375  0.6088871  -0.20622654\n",
            "   0.3508051   0.4964381  -0.258369   -0.49229193]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'Please turn on the light', with shape: (768,). First 5 values: [0.025 0.189 0.068 0.047 0.609].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2016 -> 'she'\n",
            "2:  5078 -> 'wore'\n",
            "3:  1037 -> 'a'\n",
            "4:  2422 -> 'light'\n",
            "5:  2630 -> 'blue'\n",
            "6:  4377 -> 'dress'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.10579953 -0.09731294 -0.27864882  0.00735208 -0.21898043 -0.0246367\n",
            "   0.05539566  0.33748478 -0.10168727  0.04515309]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.25333554 -0.47567073 -0.41877708  0.0060566   0.19912407  0.02961503\n",
            "  -0.13285795  0.5049728  -0.23669174 -0.3368682 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'She wore a light blue dress', with shape: (768,). First 5 values: [ 0.253 -0.476 -0.419  0.006  0.199].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2422 -> 'light'\n",
            "2:  7930 -> 'travels'\n",
            "3:  5514 -> 'faster'\n",
            "4:  2084 -> 'than'\n",
            "5:  2614 -> 'sound'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.1448877   0.35191137  0.02173407  0.16409966 -0.27316442 -0.07875478\n",
            "   0.38083807  0.60303116 -0.02809679 -0.7940695 ]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.13971493 -0.05843241  0.19625568  0.16699459 -0.01351825  0.03934836\n",
            "   0.3161468   0.44157764 -0.24341194 -0.65350884]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'Light travels faster than sound', with shape: (768,). First 5 values: [ 0.14  -0.058  0.196  0.167 -0.014].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2023 -> 'this'\n",
            "2: 15940 -> 'suitcase'\n",
            "3:  5683 -> 'feels'\n",
            "4:  2422 -> 'light'\n",
            "5:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.13182575  0.11080028  0.11348552 -0.15446094 -0.12187488 -0.23669153\n",
            "   0.13206573  0.2624384  -0.04836292 -0.25358266]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.05043607 -0.07055765  0.20742871  0.04120008 -0.09575456 -0.35341692\n",
            "   0.09970206  0.01175304 -0.18896626 -0.22021663]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'This suitcase feels light', with shape: (768,). First 5 values: [-0.05  -0.071  0.207  0.041 -0.096].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  2282 -> 'room'\n",
            "3:  3791 -> 'needs'\n",
            "4:  2062 -> 'more'\n",
            "5:  2422 -> 'light'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.1155864   0.3830928   0.27813098 -0.20088676 -0.12885334 -0.11217055\n",
            "   0.27610242  0.15661834 -0.08944837 -0.29173413]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.13865066  0.04837024  0.15303521  0.31985182  0.16440856 -0.27544534\n",
            "   0.06071272  0.1813196  -0.3238075  -0.5895611 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The room needs more light', with shape: (768,). First 5 values: [0.139 0.048 0.153 0.32  0.164].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  3460 -> 'doctor'\n",
            "3: 16250 -> 'prescribed'\n",
            "4:  4200 -> 'medicine'\n",
            "5:  2005 -> 'for'\n",
            "6:  2026 -> 'my'\n",
            "7:  3147 -> 'cold'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.11252215  0.29544035 -0.08022214 -0.00151484  0.02593806  0.07283914\n",
            "   0.13834259  0.12479164  0.03323534 -0.14516187]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.00716678  0.18118444  0.00387121 -0.00124979  0.02194752  0.3087348\n",
            "   0.3913492   0.73019123  0.15277947 -0.18493247]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The doctor prescribed medicine for my cold', with shape: (768,). First 5 values: [-0.007  0.181  0.004 -0.001  0.022].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  7522 -> 'physician'\n",
            "3:  6749 -> 'recommended'\n",
            "4: 14667 -> 'medication'\n",
            "5:  2005 -> 'for'\n",
            "6:  2026 -> 'my'\n",
            "7:  7355 -> 'illness'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.02577688  0.28859994 -0.06767031  0.00818749 -0.00356493 -0.0611333\n",
            "   0.16270639  0.1583612   0.05809442 -0.21884719]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.19877164  0.0561132   0.0655259  -0.02190936  0.3035607   0.04909654\n",
            "   0.13817321  0.33784443  0.13085034 -0.15884775]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The physician recommended medication for my illness', with shape: (768,). First 5 values: [-0.199  0.056  0.066 -0.022  0.304].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  9431 -> 'surgeon'\n",
            "3:  2864 -> 'performed'\n",
            "4:  1996 -> 'the'\n",
            "5:  3169 -> 'operation'\n",
            "6:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.19702415 -0.01965646  0.09984224 -0.05981246  0.43319604 -0.00741846\n",
            "   0.29226083  0.02824886 -0.11364806 -0.17442317]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.27726656 -0.6127943   0.11964258 -0.12906392  0.5829734   0.18867418\n",
            "   0.3693629   0.7104863   0.44263816 -0.30268365]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The surgeon performed the operation', with shape: (768,). First 5 values: [-0.277 -0.613  0.12  -0.129  0.583].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2: 19960 -> 'med'\n",
            "3:  2594 -> '##ic'\n",
            "4:  5845 -> 'treated'\n",
            "5:  1996 -> 'the'\n",
            "6:  5303 -> 'wounded'\n",
            "7:  5268 -> 'soldier'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.14745617  0.1381103   0.06382993  0.03344006  0.13312785  0.14394681\n",
            "   0.30202097 -0.05244796 -0.08656445  0.01069108]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.19664492 -0.35736856 -0.11792666  0.14756282  0.06720612  0.18420404\n",
            "   0.0472951   0.09991433 -0.01004977 -0.12049581]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The medic treated the wounded soldier', with shape: (768,). First 5 values: [-0.197 -0.357 -0.118  0.148  0.067].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2009 -> 'it'\n",
            "2:  1005 -> '''\n",
            "3:  1055 -> 's'\n",
            "4: 24057 -> 'raining'\n",
            "5:  8870 -> 'cats'\n",
            "6:  1998 -> 'and'\n",
            "7:  6077 -> 'dogs'\n",
            "8:  2648 -> 'outside'\n",
            "9:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.10440689  0.55850565 -0.02569607 -0.5842446  -0.4975357  -0.63985956\n",
            "   0.78033507  0.45693275  0.05375417 -0.38914952]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.7085448   0.33877075  0.21203294 -0.21457453 -0.04044974 -0.46604818\n",
            "   0.14996085  1.0314119  -0.12736194 -0.30701524]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'It's raining cats and dogs outside', with shape: (768,). First 5 values: [ 0.709  0.339  0.212 -0.215 -0.04 ].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  8870 -> 'cats'\n",
            "3:  1998 -> 'and'\n",
            "4:  6077 -> 'dogs'\n",
            "5:  2024 -> 'are'\n",
            "6:  2652 -> 'playing'\n",
            "7:  2362 -> 'together'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.11293449  0.3814981   0.00589054 -0.36526737  0.18755017 -0.6912092\n",
            "   0.2965085   0.6798092  -0.49121693 -0.5087694 ]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.32944006  0.095614    0.01898412  0.25374076  0.44932398 -0.52334946\n",
            "  -0.26533082  0.47664997 -0.23184922 -0.18006359]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The cats and dogs are playing together', with shape: (768,). First 5 values: [0.329 0.096 0.019 0.254 0.449].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  3338 -> 'break'\n",
            "2:  1037 -> 'a'\n",
            "3:  4190 -> 'leg'\n",
            "4:  2012 -> 'at'\n",
            "5:  2115 -> 'your'\n",
            "6:  2836 -> 'performance'\n",
            "7:  3892 -> 'tonight'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 8.5803844e-02  6.9960319e-02 -2.1077661e-01  4.6958390e-04\n",
            "  -1.6492465e-01 -3.0379060e-01  5.3699034e-01  6.3013989e-01\n",
            "   9.5848888e-02 -3.0213213e-02]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.11979908 -0.45009217  0.05990544 -0.15266283  0.04475056  0.07877221\n",
            "   0.4996494   0.9344545  -0.02171434 -0.16396475]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'Break a leg at your performance tonight', with shape: (768,). First 5 values: [ 0.12  -0.45   0.06  -0.153  0.045].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  2002 -> 'he'\n",
            "2:  6719 -> 'literally'\n",
            "3:  3631 -> 'broke'\n",
            "4:  2010 -> 'his'\n",
            "5:  4190 -> 'leg'\n",
            "6: 12701 -> 'skiing'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.182245    0.0829651  -0.4143325   0.0043682  -0.10354114 -0.34811464\n",
            "   0.53288877  0.47008985 -0.01050104 -0.35657296]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.2312803  -0.30744597 -0.15509047  0.23885638  0.20515598 -0.10231128\n",
            "  -0.00113211  0.9314699   0.07885045 -0.42645988]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'He literally broke his leg skiing', with shape: (768,). First 5 values: [ 0.231 -0.307 -0.155  0.239  0.205].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1045 -> 'i'\n",
            "2:  2342 -> 'need'\n",
            "3:  2000 -> 'to'\n",
            "4:  2139 -> 'de'\n",
            "5:  8569 -> '##bu'\n",
            "6:  2290 -> '##g'\n",
            "7:  2023 -> 'this'\n",
            "8: 18750 -> 'python'\n",
            "9:  3642 -> 'code'\n",
            "10:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.14970638  0.20256628  0.02220924 -0.17752059 -0.17070884 -0.50269306\n",
            "   0.36143818  0.48698276 -0.07109662 -0.09350535]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[ 0.0147578   0.06282758 -0.00537962 -0.23214485  0.02435713 -0.6452488\n",
            "   0.34658554  0.5227727  -0.2125196   0.0071629 ]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'I need to debug this Python code', with shape: (768,). First 5 values: [ 0.015  0.063 -0.005 -0.232  0.024].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2: 11829 -> 'bug'\n",
            "3: 12425 -> 'crawled'\n",
            "4:  2408 -> 'across'\n",
            "5:  1996 -> 'the'\n",
            "6:  9019 -> 'keyboard'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.04947478  0.06166752  0.16743793  0.02899605 -0.07486202 -0.04731038\n",
            "   0.03199034  0.3310099  -0.20823523 -0.05965836]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.30365372 -0.14145821 -0.0413885   0.15371263  0.56893885 -0.10068427\n",
            "  -0.07375951  0.73642826 -0.34903625 -0.32080823]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The bug crawled across the keyboard', with shape: (768,). First 5 values: [-0.304 -0.141 -0.041  0.154  0.569].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1996 -> 'the'\n",
            "2:  4007 -> 'software'\n",
            "3:  2038 -> 'has'\n",
            "4:  1037 -> 'a'\n",
            "5:  4187 -> 'critical'\n",
            "6: 11829 -> 'bug'\n",
            "7:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[-0.17158888  0.27078667  0.00060681 -0.10432427 -0.06713795 -0.3316469\n",
            "   0.28780225  0.41152692  0.01859778 -0.2008932 ]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.14550717  0.18514314 -0.0559397   0.15366913  0.20123257 -0.17525098\n",
            "  -0.03142925  0.49098238  0.02103321 -0.24915813]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'The software has a critical bug', with shape: (768,). First 5 values: [-0.146  0.185 -0.056  0.154  0.201].\n",
            "\n",
            "0:   101 -> '[CLS]'\n",
            "1:  1045 -> 'i'\n",
            "2:  2179 -> 'found'\n",
            "3:  1037 -> 'a'\n",
            "4: 11829 -> 'bug'\n",
            "5:  1999 -> 'in'\n",
            "6:  2026 -> 'my'\n",
            "7: 16521 -> 'salad'\n",
            "8:   102 -> '[SEP]'\n",
            "\n",
            "sentence_embedding[:,:10]\n",
            "\n",
            "[[ 0.10809018  0.11201282 -0.02885535  0.00327694 -0.01060454 -0.11018934\n",
            "   0.06202956  0.35840183  0.01968686 -0.00983659]]\n",
            "\n",
            "\n",
            "\n",
            "mean_embedding[:,:10]\n",
            "\n",
            "[[-0.07209735 -0.24955279 -0.2571082   0.10462356  0.2902406  -0.09204932\n",
            "  -0.173006    0.9834953  -0.10181611 -0.16247886]]\n",
            "\n",
            "\n",
            "\n",
            "Sentence: 'I found a bug in my salad', with shape: (768,). First 5 values: [-0.072 -0.25  -0.257  0.105  0.29 ].\n",
            "\n",
            "\n",
            "Contextual similarity matrix:\n",
            "\n",
            " 'The bank is by the river' vs 'I need to go to the bank to deposit money':0.5428\n",
            " 'The bank is by the river' vs 'The river bank is muddy':0.8059\n",
            " 'The bank is by the river' vs 'She works at the central bank':0.6260\n",
            " 'The bank is by the river' vs 'The boat docked at the river bank':0.7099\n",
            " 'The bank is by the river' vs 'My bank account is overdrawn':0.4862\n",
            " 'The bank is by the river' vs 'The bat flew out of the cave at dusk':0.6175\n",
            " 'The bank is by the river' vs 'She swung the bat and hit a home run':0.4185\n",
            " 'The bank is by the river' vs 'Bats sleep hanging upside down':0.5282\n",
            " 'The bank is by the river' vs 'He bought a new cricket bat':0.5421\n",
            " 'The bank is by the river' vs 'The vampire bat feeds on blood':0.5985\n",
            " 'The bank is by the river' vs 'The feather is very light':0.5626\n",
            " 'The bank is by the river' vs 'Please turn on the light':0.4868\n",
            " 'The bank is by the river' vs 'She wore a light blue dress':0.5678\n",
            " 'The bank is by the river' vs 'Light travels faster than sound':0.4789\n",
            " 'The bank is by the river' vs 'This suitcase feels light':0.4912\n",
            " 'The bank is by the river' vs 'The room needs more light':0.5378\n",
            " 'The bank is by the river' vs 'The doctor prescribed medicine for my cold':0.5540\n",
            " 'The bank is by the river' vs 'The physician recommended medication for my illness':0.4905\n",
            " 'The bank is by the river' vs 'The surgeon performed the operation':0.4972\n",
            " 'The bank is by the river' vs 'The medic treated the wounded soldier':0.4996\n",
            " 'The bank is by the river' vs 'It's raining cats and dogs outside':0.5369\n",
            " 'The bank is by the river' vs 'The cats and dogs are playing together':0.5540\n",
            " 'The bank is by the river' vs 'Break a leg at your performance tonight':0.5255\n",
            " 'The bank is by the river' vs 'He literally broke his leg skiing':0.5136\n",
            " 'The bank is by the river' vs 'I need to debug this Python code':0.4646\n",
            " 'The bank is by the river' vs 'The bug crawled across the keyboard':0.5129\n",
            " 'The bank is by the river' vs 'The software has a critical bug':0.5656\n",
            " 'The bank is by the river' vs 'I found a bug in my salad':0.5330\n",
            " 'I need to go to the bank to deposit money' vs 'The river bank is muddy':0.5453\n",
            " 'I need to go to the bank to deposit money' vs 'She works at the central bank':0.5997\n",
            " 'I need to go to the bank to deposit money' vs 'The boat docked at the river bank':0.5850\n",
            " 'I need to go to the bank to deposit money' vs 'My bank account is overdrawn':0.6795\n",
            " 'I need to go to the bank to deposit money' vs 'The bat flew out of the cave at dusk':0.5568\n",
            " 'I need to go to the bank to deposit money' vs 'She swung the bat and hit a home run':0.4631\n",
            " 'I need to go to the bank to deposit money' vs 'Bats sleep hanging upside down':0.5108\n",
            " 'I need to go to the bank to deposit money' vs 'He bought a new cricket bat':0.5859\n",
            " 'I need to go to the bank to deposit money' vs 'The vampire bat feeds on blood':0.5313\n",
            " 'I need to go to the bank to deposit money' vs 'The feather is very light':0.4584\n",
            " 'I need to go to the bank to deposit money' vs 'Please turn on the light':0.5660\n",
            " 'I need to go to the bank to deposit money' vs 'She wore a light blue dress':0.5701\n",
            " 'I need to go to the bank to deposit money' vs 'Light travels faster than sound':0.4823\n",
            " 'I need to go to the bank to deposit money' vs 'This suitcase feels light':0.5447\n",
            " 'I need to go to the bank to deposit money' vs 'The room needs more light':0.6086\n",
            " 'I need to go to the bank to deposit money' vs 'The doctor prescribed medicine for my cold':0.6430\n",
            " 'I need to go to the bank to deposit money' vs 'The physician recommended medication for my illness':0.6184\n",
            " 'I need to go to the bank to deposit money' vs 'The surgeon performed the operation':0.4727\n",
            " 'I need to go to the bank to deposit money' vs 'The medic treated the wounded soldier':0.5565\n",
            " 'I need to go to the bank to deposit money' vs 'It's raining cats and dogs outside':0.5782\n",
            " 'I need to go to the bank to deposit money' vs 'The cats and dogs are playing together':0.5317\n",
            " 'I need to go to the bank to deposit money' vs 'Break a leg at your performance tonight':0.5344\n",
            " 'I need to go to the bank to deposit money' vs 'He literally broke his leg skiing':0.5219\n",
            " 'I need to go to the bank to deposit money' vs 'I need to debug this Python code':0.6235\n",
            " 'I need to go to the bank to deposit money' vs 'The bug crawled across the keyboard':0.4954\n",
            " 'I need to go to the bank to deposit money' vs 'The software has a critical bug':0.5134\n",
            " 'I need to go to the bank to deposit money' vs 'I found a bug in my salad':0.6332\n",
            " 'The river bank is muddy' vs 'She works at the central bank':0.6240\n",
            " 'The river bank is muddy' vs 'The boat docked at the river bank':0.7086\n",
            " 'The river bank is muddy' vs 'My bank account is overdrawn':0.5692\n",
            " 'The river bank is muddy' vs 'The bat flew out of the cave at dusk':0.6200\n",
            " 'The river bank is muddy' vs 'She swung the bat and hit a home run':0.3969\n",
            " 'The river bank is muddy' vs 'Bats sleep hanging upside down':0.5663\n",
            " 'The river bank is muddy' vs 'He bought a new cricket bat':0.5471\n",
            " 'The river bank is muddy' vs 'The vampire bat feeds on blood':0.5906\n",
            " 'The river bank is muddy' vs 'The feather is very light':0.6072\n",
            " 'The river bank is muddy' vs 'Please turn on the light':0.5024\n",
            " 'The river bank is muddy' vs 'She wore a light blue dress':0.5651\n",
            " 'The river bank is muddy' vs 'Light travels faster than sound':0.5019\n",
            " 'The river bank is muddy' vs 'This suitcase feels light':0.5409\n",
            " 'The river bank is muddy' vs 'The room needs more light':0.5717\n",
            " 'The river bank is muddy' vs 'The doctor prescribed medicine for my cold':0.5655\n",
            " 'The river bank is muddy' vs 'The physician recommended medication for my illness':0.4981\n",
            " 'The river bank is muddy' vs 'The surgeon performed the operation':0.4792\n",
            " 'The river bank is muddy' vs 'The medic treated the wounded soldier':0.5200\n",
            " 'The river bank is muddy' vs 'It's raining cats and dogs outside':0.5824\n",
            " 'The river bank is muddy' vs 'The cats and dogs are playing together':0.5290\n",
            " 'The river bank is muddy' vs 'Break a leg at your performance tonight':0.5565\n",
            " 'The river bank is muddy' vs 'He literally broke his leg skiing':0.5436\n",
            " 'The river bank is muddy' vs 'I need to debug this Python code':0.4641\n",
            " 'The river bank is muddy' vs 'The bug crawled across the keyboard':0.5250\n",
            " 'The river bank is muddy' vs 'The software has a critical bug':0.6082\n",
            " 'The river bank is muddy' vs 'I found a bug in my salad':0.5602\n",
            " 'She works at the central bank' vs 'The boat docked at the river bank':0.6090\n",
            " 'She works at the central bank' vs 'My bank account is overdrawn':0.5477\n",
            " 'She works at the central bank' vs 'The bat flew out of the cave at dusk':0.5265\n",
            " 'She works at the central bank' vs 'She swung the bat and hit a home run':0.4635\n",
            " 'She works at the central bank' vs 'Bats sleep hanging upside down':0.4887\n",
            " 'She works at the central bank' vs 'He bought a new cricket bat':0.5140\n",
            " 'She works at the central bank' vs 'The vampire bat feeds on blood':0.5120\n",
            " 'She works at the central bank' vs 'The feather is very light':0.4631\n",
            " 'She works at the central bank' vs 'Please turn on the light':0.4533\n",
            " 'She works at the central bank' vs 'She wore a light blue dress':0.5804\n",
            " 'She works at the central bank' vs 'Light travels faster than sound':0.4462\n",
            " 'She works at the central bank' vs 'This suitcase feels light':0.4808\n",
            " 'She works at the central bank' vs 'The room needs more light':0.5378\n",
            " 'She works at the central bank' vs 'The doctor prescribed medicine for my cold':0.5415\n",
            " 'She works at the central bank' vs 'The physician recommended medication for my illness':0.4731\n",
            " 'She works at the central bank' vs 'The surgeon performed the operation':0.4582\n",
            " 'She works at the central bank' vs 'The medic treated the wounded soldier':0.5393\n",
            " 'She works at the central bank' vs 'It's raining cats and dogs outside':0.5435\n",
            " 'She works at the central bank' vs 'The cats and dogs are playing together':0.5412\n",
            " 'She works at the central bank' vs 'Break a leg at your performance tonight':0.4617\n",
            " 'She works at the central bank' vs 'He literally broke his leg skiing':0.4645\n",
            " 'She works at the central bank' vs 'I need to debug this Python code':0.4705\n",
            " 'She works at the central bank' vs 'The bug crawled across the keyboard':0.5014\n",
            " 'She works at the central bank' vs 'The software has a critical bug':0.5438\n",
            " 'She works at the central bank' vs 'I found a bug in my salad':0.5191\n",
            " 'The boat docked at the river bank' vs 'My bank account is overdrawn':0.5064\n",
            " 'The boat docked at the river bank' vs 'The bat flew out of the cave at dusk':0.6917\n",
            " 'The boat docked at the river bank' vs 'She swung the bat and hit a home run':0.4755\n",
            " 'The boat docked at the river bank' vs 'Bats sleep hanging upside down':0.5673\n",
            " 'The boat docked at the river bank' vs 'He bought a new cricket bat':0.6023\n",
            " 'The boat docked at the river bank' vs 'The vampire bat feeds on blood':0.5932\n",
            " 'The boat docked at the river bank' vs 'The feather is very light':0.5395\n",
            " 'The boat docked at the river bank' vs 'Please turn on the light':0.5306\n",
            " 'The boat docked at the river bank' vs 'She wore a light blue dress':0.6139\n",
            " 'The boat docked at the river bank' vs 'Light travels faster than sound':0.4930\n",
            " 'The boat docked at the river bank' vs 'This suitcase feels light':0.5459\n",
            " 'The boat docked at the river bank' vs 'The room needs more light':0.6250\n",
            " 'The boat docked at the river bank' vs 'The doctor prescribed medicine for my cold':0.5952\n",
            " 'The boat docked at the river bank' vs 'The physician recommended medication for my illness':0.5544\n",
            " 'The boat docked at the river bank' vs 'The surgeon performed the operation':0.5187\n",
            " 'The boat docked at the river bank' vs 'The medic treated the wounded soldier':0.6001\n",
            " 'The boat docked at the river bank' vs 'It's raining cats and dogs outside':0.6106\n",
            " 'The boat docked at the river bank' vs 'The cats and dogs are playing together':0.6331\n",
            " 'The boat docked at the river bank' vs 'Break a leg at your performance tonight':0.5476\n",
            " 'The boat docked at the river bank' vs 'He literally broke his leg skiing':0.5363\n",
            " 'The boat docked at the river bank' vs 'I need to debug this Python code':0.5118\n",
            " 'The boat docked at the river bank' vs 'The bug crawled across the keyboard':0.6042\n",
            " 'The boat docked at the river bank' vs 'The software has a critical bug':0.5532\n",
            " 'The boat docked at the river bank' vs 'I found a bug in my salad':0.6042\n",
            " 'My bank account is overdrawn' vs 'The bat flew out of the cave at dusk':0.5029\n",
            " 'My bank account is overdrawn' vs 'She swung the bat and hit a home run':0.4232\n",
            " 'My bank account is overdrawn' vs 'Bats sleep hanging upside down':0.5341\n",
            " 'My bank account is overdrawn' vs 'He bought a new cricket bat':0.5482\n",
            " 'My bank account is overdrawn' vs 'The vampire bat feeds on blood':0.5425\n",
            " 'My bank account is overdrawn' vs 'The feather is very light':0.4665\n",
            " 'My bank account is overdrawn' vs 'Please turn on the light':0.4675\n",
            " 'My bank account is overdrawn' vs 'She wore a light blue dress':0.5698\n",
            " 'My bank account is overdrawn' vs 'Light travels faster than sound':0.4919\n",
            " 'My bank account is overdrawn' vs 'This suitcase feels light':0.5719\n",
            " 'My bank account is overdrawn' vs 'The room needs more light':0.5524\n",
            " 'My bank account is overdrawn' vs 'The doctor prescribed medicine for my cold':0.5589\n",
            " 'My bank account is overdrawn' vs 'The physician recommended medication for my illness':0.5235\n",
            " 'My bank account is overdrawn' vs 'The surgeon performed the operation':0.4564\n",
            " 'My bank account is overdrawn' vs 'The medic treated the wounded soldier':0.4997\n",
            " 'My bank account is overdrawn' vs 'It's raining cats and dogs outside':0.5620\n",
            " 'My bank account is overdrawn' vs 'The cats and dogs are playing together':0.4978\n",
            " 'My bank account is overdrawn' vs 'Break a leg at your performance tonight':0.5748\n",
            " 'My bank account is overdrawn' vs 'He literally broke his leg skiing':0.5468\n",
            " 'My bank account is overdrawn' vs 'I need to debug this Python code':0.5936\n",
            " 'My bank account is overdrawn' vs 'The bug crawled across the keyboard':0.5250\n",
            " 'My bank account is overdrawn' vs 'The software has a critical bug':0.5247\n",
            " 'My bank account is overdrawn' vs 'I found a bug in my salad':0.5992\n",
            " 'The bat flew out of the cave at dusk' vs 'She swung the bat and hit a home run':0.5334\n",
            " 'The bat flew out of the cave at dusk' vs 'Bats sleep hanging upside down':0.6424\n",
            " 'The bat flew out of the cave at dusk' vs 'He bought a new cricket bat':0.5737\n",
            " 'The bat flew out of the cave at dusk' vs 'The vampire bat feeds on blood':0.7092\n",
            " 'The bat flew out of the cave at dusk' vs 'The feather is very light':0.5792\n",
            " 'The bat flew out of the cave at dusk' vs 'Please turn on the light':0.5456\n",
            " 'The bat flew out of the cave at dusk' vs 'She wore a light blue dress':0.5521\n",
            " 'The bat flew out of the cave at dusk' vs 'Light travels faster than sound':0.5762\n",
            " 'The bat flew out of the cave at dusk' vs 'This suitcase feels light':0.5124\n",
            " 'The bat flew out of the cave at dusk' vs 'The room needs more light':0.6357\n",
            " 'The bat flew out of the cave at dusk' vs 'The doctor prescribed medicine for my cold':0.6134\n",
            " 'The bat flew out of the cave at dusk' vs 'The physician recommended medication for my illness':0.5236\n",
            " 'The bat flew out of the cave at dusk' vs 'The surgeon performed the operation':0.4930\n",
            " 'The bat flew out of the cave at dusk' vs 'The medic treated the wounded soldier':0.5314\n",
            " 'The bat flew out of the cave at dusk' vs 'It's raining cats and dogs outside':0.6420\n",
            " 'The bat flew out of the cave at dusk' vs 'The cats and dogs are playing together':0.5826\n",
            " 'The bat flew out of the cave at dusk' vs 'Break a leg at your performance tonight':0.5885\n",
            " 'The bat flew out of the cave at dusk' vs 'He literally broke his leg skiing':0.5337\n",
            " 'The bat flew out of the cave at dusk' vs 'I need to debug this Python code':0.5303\n",
            " 'The bat flew out of the cave at dusk' vs 'The bug crawled across the keyboard':0.6149\n",
            " 'The bat flew out of the cave at dusk' vs 'The software has a critical bug':0.5027\n",
            " 'The bat flew out of the cave at dusk' vs 'I found a bug in my salad':0.6525\n",
            " 'She swung the bat and hit a home run' vs 'Bats sleep hanging upside down':0.4268\n",
            " 'She swung the bat and hit a home run' vs 'He bought a new cricket bat':0.5293\n",
            " 'She swung the bat and hit a home run' vs 'The vampire bat feeds on blood':0.4710\n",
            " 'She swung the bat and hit a home run' vs 'The feather is very light':0.4113\n",
            " 'She swung the bat and hit a home run' vs 'Please turn on the light':0.4280\n",
            " 'She swung the bat and hit a home run' vs 'She wore a light blue dress':0.4586\n",
            " 'She swung the bat and hit a home run' vs 'Light travels faster than sound':0.4127\n",
            " 'She swung the bat and hit a home run' vs 'This suitcase feels light':0.3751\n",
            " 'She swung the bat and hit a home run' vs 'The room needs more light':0.4268\n",
            " 'She swung the bat and hit a home run' vs 'The doctor prescribed medicine for my cold':0.4394\n",
            " 'She swung the bat and hit a home run' vs 'The physician recommended medication for my illness':0.3889\n",
            " 'She swung the bat and hit a home run' vs 'The surgeon performed the operation':0.4273\n",
            " 'She swung the bat and hit a home run' vs 'The medic treated the wounded soldier':0.4987\n",
            " 'She swung the bat and hit a home run' vs 'It's raining cats and dogs outside':0.4512\n",
            " 'She swung the bat and hit a home run' vs 'The cats and dogs are playing together':0.5165\n",
            " 'She swung the bat and hit a home run' vs 'Break a leg at your performance tonight':0.4469\n",
            " 'She swung the bat and hit a home run' vs 'He literally broke his leg skiing':0.4436\n",
            " 'She swung the bat and hit a home run' vs 'I need to debug this Python code':0.4736\n",
            " 'She swung the bat and hit a home run' vs 'The bug crawled across the keyboard':0.4701\n",
            " 'She swung the bat and hit a home run' vs 'The software has a critical bug':0.3987\n",
            " 'She swung the bat and hit a home run' vs 'I found a bug in my salad':0.4953\n",
            " 'Bats sleep hanging upside down' vs 'He bought a new cricket bat':0.5703\n",
            " 'Bats sleep hanging upside down' vs 'The vampire bat feeds on blood':0.6733\n",
            " 'Bats sleep hanging upside down' vs 'The feather is very light':0.5573\n",
            " 'Bats sleep hanging upside down' vs 'Please turn on the light':0.5824\n",
            " 'Bats sleep hanging upside down' vs 'She wore a light blue dress':0.5248\n",
            " 'Bats sleep hanging upside down' vs 'Light travels faster than sound':0.5515\n",
            " 'Bats sleep hanging upside down' vs 'This suitcase feels light':0.5276\n",
            " 'Bats sleep hanging upside down' vs 'The room needs more light':0.5723\n",
            " 'Bats sleep hanging upside down' vs 'The doctor prescribed medicine for my cold':0.5214\n",
            " 'Bats sleep hanging upside down' vs 'The physician recommended medication for my illness':0.4991\n",
            " 'Bats sleep hanging upside down' vs 'The surgeon performed the operation':0.4541\n",
            " 'Bats sleep hanging upside down' vs 'The medic treated the wounded soldier':0.5046\n",
            " 'Bats sleep hanging upside down' vs 'It's raining cats and dogs outside':0.6797\n",
            " 'Bats sleep hanging upside down' vs 'The cats and dogs are playing together':0.6822\n",
            " 'Bats sleep hanging upside down' vs 'Break a leg at your performance tonight':0.5839\n",
            " 'Bats sleep hanging upside down' vs 'He literally broke his leg skiing':0.4941\n",
            " 'Bats sleep hanging upside down' vs 'I need to debug this Python code':0.4869\n",
            " 'Bats sleep hanging upside down' vs 'The bug crawled across the keyboard':0.5740\n",
            " 'Bats sleep hanging upside down' vs 'The software has a critical bug':0.4709\n",
            " 'Bats sleep hanging upside down' vs 'I found a bug in my salad':0.5903\n",
            " 'He bought a new cricket bat' vs 'The vampire bat feeds on blood':0.5861\n",
            " 'He bought a new cricket bat' vs 'The feather is very light':0.5396\n",
            " 'He bought a new cricket bat' vs 'Please turn on the light':0.5231\n",
            " 'He bought a new cricket bat' vs 'She wore a light blue dress':0.5955\n",
            " 'He bought a new cricket bat' vs 'Light travels faster than sound':0.5122\n",
            " 'He bought a new cricket bat' vs 'This suitcase feels light':0.6055\n",
            " 'He bought a new cricket bat' vs 'The room needs more light':0.6548\n",
            " 'He bought a new cricket bat' vs 'The doctor prescribed medicine for my cold':0.6096\n",
            " 'He bought a new cricket bat' vs 'The physician recommended medication for my illness':0.5555\n",
            " 'He bought a new cricket bat' vs 'The surgeon performed the operation':0.4885\n",
            " 'He bought a new cricket bat' vs 'The medic treated the wounded soldier':0.5597\n",
            " 'He bought a new cricket bat' vs 'It's raining cats and dogs outside':0.5847\n",
            " 'He bought a new cricket bat' vs 'The cats and dogs are playing together':0.6239\n",
            " 'He bought a new cricket bat' vs 'Break a leg at your performance tonight':0.5666\n",
            " 'He bought a new cricket bat' vs 'He literally broke his leg skiing':0.6144\n",
            " 'He bought a new cricket bat' vs 'I need to debug this Python code':0.5465\n",
            " 'He bought a new cricket bat' vs 'The bug crawled across the keyboard':0.5440\n",
            " 'He bought a new cricket bat' vs 'The software has a critical bug':0.5464\n",
            " 'He bought a new cricket bat' vs 'I found a bug in my salad':0.6537\n",
            " 'The vampire bat feeds on blood' vs 'The feather is very light':0.5820\n",
            " 'The vampire bat feeds on blood' vs 'Please turn on the light':0.5270\n",
            " 'The vampire bat feeds on blood' vs 'She wore a light blue dress':0.5342\n",
            " 'The vampire bat feeds on blood' vs 'Light travels faster than sound':0.6318\n",
            " 'The vampire bat feeds on blood' vs 'This suitcase feels light':0.5403\n",
            " 'The vampire bat feeds on blood' vs 'The room needs more light':0.6082\n",
            " 'The vampire bat feeds on blood' vs 'The doctor prescribed medicine for my cold':0.5913\n",
            " 'The vampire bat feeds on blood' vs 'The physician recommended medication for my illness':0.5524\n",
            " 'The vampire bat feeds on blood' vs 'The surgeon performed the operation':0.4850\n",
            " 'The vampire bat feeds on blood' vs 'The medic treated the wounded soldier':0.5294\n",
            " 'The vampire bat feeds on blood' vs 'It's raining cats and dogs outside':0.5975\n",
            " 'The vampire bat feeds on blood' vs 'The cats and dogs are playing together':0.6088\n",
            " 'The vampire bat feeds on blood' vs 'Break a leg at your performance tonight':0.5780\n",
            " 'The vampire bat feeds on blood' vs 'He literally broke his leg skiing':0.5273\n",
            " 'The vampire bat feeds on blood' vs 'I need to debug this Python code':0.4908\n",
            " 'The vampire bat feeds on blood' vs 'The bug crawled across the keyboard':0.5435\n",
            " 'The vampire bat feeds on blood' vs 'The software has a critical bug':0.5128\n",
            " 'The vampire bat feeds on blood' vs 'I found a bug in my salad':0.6064\n",
            " 'The feather is very light' vs 'Please turn on the light':0.4795\n",
            " 'The feather is very light' vs 'She wore a light blue dress':0.5730\n",
            " 'The feather is very light' vs 'Light travels faster than sound':0.4744\n",
            " 'The feather is very light' vs 'This suitcase feels light':0.5200\n",
            " 'The feather is very light' vs 'The room needs more light':0.5223\n",
            " 'The feather is very light' vs 'The doctor prescribed medicine for my cold':0.4868\n",
            " 'The feather is very light' vs 'The physician recommended medication for my illness':0.4718\n",
            " 'The feather is very light' vs 'The surgeon performed the operation':0.4867\n",
            " 'The feather is very light' vs 'The medic treated the wounded soldier':0.4635\n",
            " 'The feather is very light' vs 'It's raining cats and dogs outside':0.5432\n",
            " 'The feather is very light' vs 'The cats and dogs are playing together':0.4876\n",
            " 'The feather is very light' vs 'Break a leg at your performance tonight':0.4951\n",
            " 'The feather is very light' vs 'He literally broke his leg skiing':0.4547\n",
            " 'The feather is very light' vs 'I need to debug this Python code':0.4151\n",
            " 'The feather is very light' vs 'The bug crawled across the keyboard':0.4466\n",
            " 'The feather is very light' vs 'The software has a critical bug':0.4810\n",
            " 'The feather is very light' vs 'I found a bug in my salad':0.4884\n",
            " 'Please turn on the light' vs 'She wore a light blue dress':0.5291\n",
            " 'Please turn on the light' vs 'Light travels faster than sound':0.5021\n",
            " 'Please turn on the light' vs 'This suitcase feels light':0.4853\n",
            " 'Please turn on the light' vs 'The room needs more light':0.6318\n",
            " 'Please turn on the light' vs 'The doctor prescribed medicine for my cold':0.5033\n",
            " 'Please turn on the light' vs 'The physician recommended medication for my illness':0.5309\n",
            " 'Please turn on the light' vs 'The surgeon performed the operation':0.5365\n",
            " 'Please turn on the light' vs 'The medic treated the wounded soldier':0.5179\n",
            " 'Please turn on the light' vs 'It's raining cats and dogs outside':0.5733\n",
            " 'Please turn on the light' vs 'The cats and dogs are playing together':0.5508\n",
            " 'Please turn on the light' vs 'Break a leg at your performance tonight':0.5459\n",
            " 'Please turn on the light' vs 'He literally broke his leg skiing':0.4395\n",
            " 'Please turn on the light' vs 'I need to debug this Python code':0.5263\n",
            " 'Please turn on the light' vs 'The bug crawled across the keyboard':0.5196\n",
            " 'Please turn on the light' vs 'The software has a critical bug':0.4593\n",
            " 'Please turn on the light' vs 'I found a bug in my salad':0.5292\n",
            " 'She wore a light blue dress' vs 'Light travels faster than sound':0.5249\n",
            " 'She wore a light blue dress' vs 'This suitcase feels light':0.6233\n",
            " 'She wore a light blue dress' vs 'The room needs more light':0.5795\n",
            " 'She wore a light blue dress' vs 'The doctor prescribed medicine for my cold':0.5699\n",
            " 'She wore a light blue dress' vs 'The physician recommended medication for my illness':0.5382\n",
            " 'She wore a light blue dress' vs 'The surgeon performed the operation':0.4987\n",
            " 'She wore a light blue dress' vs 'The medic treated the wounded soldier':0.5522\n",
            " 'She wore a light blue dress' vs 'It's raining cats and dogs outside':0.5571\n",
            " 'She wore a light blue dress' vs 'The cats and dogs are playing together':0.5578\n",
            " 'She wore a light blue dress' vs 'Break a leg at your performance tonight':0.5626\n",
            " 'She wore a light blue dress' vs 'He literally broke his leg skiing':0.5439\n",
            " 'She wore a light blue dress' vs 'I need to debug this Python code':0.5129\n",
            " 'She wore a light blue dress' vs 'The bug crawled across the keyboard':0.5118\n",
            " 'She wore a light blue dress' vs 'The software has a critical bug':0.4919\n",
            " 'She wore a light blue dress' vs 'I found a bug in my salad':0.5944\n",
            " 'Light travels faster than sound' vs 'This suitcase feels light':0.6082\n",
            " 'Light travels faster than sound' vs 'The room needs more light':0.6324\n",
            " 'Light travels faster than sound' vs 'The doctor prescribed medicine for my cold':0.5108\n",
            " 'Light travels faster than sound' vs 'The physician recommended medication for my illness':0.4706\n",
            " 'Light travels faster than sound' vs 'The surgeon performed the operation':0.4262\n",
            " 'Light travels faster than sound' vs 'The medic treated the wounded soldier':0.4590\n",
            " 'Light travels faster than sound' vs 'It's raining cats and dogs outside':0.4901\n",
            " 'Light travels faster than sound' vs 'The cats and dogs are playing together':0.5023\n",
            " 'Light travels faster than sound' vs 'Break a leg at your performance tonight':0.4963\n",
            " 'Light travels faster than sound' vs 'He literally broke his leg skiing':0.4811\n",
            " 'Light travels faster than sound' vs 'I need to debug this Python code':0.4462\n",
            " 'Light travels faster than sound' vs 'The bug crawled across the keyboard':0.5313\n",
            " 'Light travels faster than sound' vs 'The software has a critical bug':0.4240\n",
            " 'Light travels faster than sound' vs 'I found a bug in my salad':0.4954\n",
            " 'This suitcase feels light' vs 'The room needs more light':0.7142\n",
            " 'This suitcase feels light' vs 'The doctor prescribed medicine for my cold':0.5008\n",
            " 'This suitcase feels light' vs 'The physician recommended medication for my illness':0.5087\n",
            " 'This suitcase feels light' vs 'The surgeon performed the operation':0.4372\n",
            " 'This suitcase feels light' vs 'The medic treated the wounded soldier':0.4281\n",
            " 'This suitcase feels light' vs 'It's raining cats and dogs outside':0.4909\n",
            " 'This suitcase feels light' vs 'The cats and dogs are playing together':0.4740\n",
            " 'This suitcase feels light' vs 'Break a leg at your performance tonight':0.4712\n",
            " 'This suitcase feels light' vs 'He literally broke his leg skiing':0.4634\n",
            " 'This suitcase feels light' vs 'I need to debug this Python code':0.4753\n",
            " 'This suitcase feels light' vs 'The bug crawled across the keyboard':0.5184\n",
            " 'This suitcase feels light' vs 'The software has a critical bug':0.4892\n",
            " 'This suitcase feels light' vs 'I found a bug in my salad':0.5640\n",
            " 'The room needs more light' vs 'The doctor prescribed medicine for my cold':0.6171\n",
            " 'The room needs more light' vs 'The physician recommended medication for my illness':0.5724\n",
            " 'The room needs more light' vs 'The surgeon performed the operation':0.4708\n",
            " 'The room needs more light' vs 'The medic treated the wounded soldier':0.5478\n",
            " 'The room needs more light' vs 'It's raining cats and dogs outside':0.6121\n",
            " 'The room needs more light' vs 'The cats and dogs are playing together':0.5971\n",
            " 'The room needs more light' vs 'Break a leg at your performance tonight':0.5166\n",
            " 'The room needs more light' vs 'He literally broke his leg skiing':0.5057\n",
            " 'The room needs more light' vs 'I need to debug this Python code':0.5074\n",
            " 'The room needs more light' vs 'The bug crawled across the keyboard':0.5544\n",
            " 'The room needs more light' vs 'The software has a critical bug':0.5294\n",
            " 'The room needs more light' vs 'I found a bug in my salad':0.6159\n",
            " 'The doctor prescribed medicine for my cold' vs 'The physician recommended medication for my illness':0.8753\n",
            " 'The doctor prescribed medicine for my cold' vs 'The surgeon performed the operation':0.6311\n",
            " 'The doctor prescribed medicine for my cold' vs 'The medic treated the wounded soldier':0.6558\n",
            " 'The doctor prescribed medicine for my cold' vs 'It's raining cats and dogs outside':0.5354\n",
            " 'The doctor prescribed medicine for my cold' vs 'The cats and dogs are playing together':0.5066\n",
            " 'The doctor prescribed medicine for my cold' vs 'Break a leg at your performance tonight':0.5966\n",
            " 'The doctor prescribed medicine for my cold' vs 'He literally broke his leg skiing':0.6155\n",
            " 'The doctor prescribed medicine for my cold' vs 'I need to debug this Python code':0.5446\n",
            " 'The doctor prescribed medicine for my cold' vs 'The bug crawled across the keyboard':0.5198\n",
            " 'The doctor prescribed medicine for my cold' vs 'The software has a critical bug':0.5752\n",
            " 'The doctor prescribed medicine for my cold' vs 'I found a bug in my salad':0.7006\n",
            " 'The physician recommended medication for my illness' vs 'The surgeon performed the operation':0.6566\n",
            " 'The physician recommended medication for my illness' vs 'The medic treated the wounded soldier':0.6423\n",
            " 'The physician recommended medication for my illness' vs 'It's raining cats and dogs outside':0.4575\n",
            " 'The physician recommended medication for my illness' vs 'The cats and dogs are playing together':0.4500\n",
            " 'The physician recommended medication for my illness' vs 'Break a leg at your performance tonight':0.5658\n",
            " 'The physician recommended medication for my illness' vs 'He literally broke his leg skiing':0.5135\n",
            " 'The physician recommended medication for my illness' vs 'I need to debug this Python code':0.5266\n",
            " 'The physician recommended medication for my illness' vs 'The bug crawled across the keyboard':0.5025\n",
            " 'The physician recommended medication for my illness' vs 'The software has a critical bug':0.5425\n",
            " 'The physician recommended medication for my illness' vs 'I found a bug in my salad':0.6635\n",
            " 'The surgeon performed the operation' vs 'The medic treated the wounded soldier':0.7024\n",
            " 'The surgeon performed the operation' vs 'It's raining cats and dogs outside':0.4775\n",
            " 'The surgeon performed the operation' vs 'The cats and dogs are playing together':0.4514\n",
            " 'The surgeon performed the operation' vs 'Break a leg at your performance tonight':0.4944\n",
            " 'The surgeon performed the operation' vs 'He literally broke his leg skiing':0.5143\n",
            " 'The surgeon performed the operation' vs 'I need to debug this Python code':0.4155\n",
            " 'The surgeon performed the operation' vs 'The bug crawled across the keyboard':0.4847\n",
            " 'The surgeon performed the operation' vs 'The software has a critical bug':0.4666\n",
            " 'The surgeon performed the operation' vs 'I found a bug in my salad':0.4600\n",
            " 'The medic treated the wounded soldier' vs 'It's raining cats and dogs outside':0.5526\n",
            " 'The medic treated the wounded soldier' vs 'The cats and dogs are playing together':0.5649\n",
            " 'The medic treated the wounded soldier' vs 'Break a leg at your performance tonight':0.5356\n",
            " 'The medic treated the wounded soldier' vs 'He literally broke his leg skiing':0.5641\n",
            " 'The medic treated the wounded soldier' vs 'I need to debug this Python code':0.4575\n",
            " 'The medic treated the wounded soldier' vs 'The bug crawled across the keyboard':0.5029\n",
            " 'The medic treated the wounded soldier' vs 'The software has a critical bug':0.4828\n",
            " 'The medic treated the wounded soldier' vs 'I found a bug in my salad':0.5090\n",
            " 'It's raining cats and dogs outside' vs 'The cats and dogs are playing together':0.7460\n",
            " 'It's raining cats and dogs outside' vs 'Break a leg at your performance tonight':0.5579\n",
            " 'It's raining cats and dogs outside' vs 'He literally broke his leg skiing':0.5107\n",
            " 'It's raining cats and dogs outside' vs 'I need to debug this Python code':0.4728\n",
            " 'It's raining cats and dogs outside' vs 'The bug crawled across the keyboard':0.4911\n",
            " 'It's raining cats and dogs outside' vs 'The software has a critical bug':0.4746\n",
            " 'It's raining cats and dogs outside' vs 'I found a bug in my salad':0.5696\n",
            " 'The cats and dogs are playing together' vs 'Break a leg at your performance tonight':0.4855\n",
            " 'The cats and dogs are playing together' vs 'He literally broke his leg skiing':0.4835\n",
            " 'The cats and dogs are playing together' vs 'I need to debug this Python code':0.4723\n",
            " 'The cats and dogs are playing together' vs 'The bug crawled across the keyboard':0.5423\n",
            " 'The cats and dogs are playing together' vs 'The software has a critical bug':0.4475\n",
            " 'The cats and dogs are playing together' vs 'I found a bug in my salad':0.5561\n",
            " 'Break a leg at your performance tonight' vs 'He literally broke his leg skiing':0.7376\n",
            " 'Break a leg at your performance tonight' vs 'I need to debug this Python code':0.5532\n",
            " 'Break a leg at your performance tonight' vs 'The bug crawled across the keyboard':0.5310\n",
            " 'Break a leg at your performance tonight' vs 'The software has a critical bug':0.5832\n",
            " 'Break a leg at your performance tonight' vs 'I found a bug in my salad':0.6590\n",
            " 'He literally broke his leg skiing' vs 'I need to debug this Python code':0.4862\n",
            " 'He literally broke his leg skiing' vs 'The bug crawled across the keyboard':0.4922\n",
            " 'He literally broke his leg skiing' vs 'The software has a critical bug':0.5571\n",
            " 'He literally broke his leg skiing' vs 'I found a bug in my salad':0.6069\n",
            " 'I need to debug this Python code' vs 'The bug crawled across the keyboard':0.6343\n",
            " 'I need to debug this Python code' vs 'The software has a critical bug':0.6303\n",
            " 'I need to debug this Python code' vs 'I found a bug in my salad':0.6643\n",
            " 'The bug crawled across the keyboard' vs 'The software has a critical bug':0.6244\n",
            " 'The bug crawled across the keyboard' vs 'I found a bug in my salad':0.7004\n",
            " 'The software has a critical bug' vs 'I found a bug in my salad':0.6291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ChromaDB for Embedding Storage and Retrieval\n"
      ],
      "metadata": {
        "id": "SMxBpgobHP4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents to index\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Deep learning uses neural networks with multiple layers\",\n",
        "    \"Natural language processing helps computers understand text\",\n",
        "    \"Computer vision enables machines to interpret visual information\",\n",
        "    \"Reinforcement learning trains agents through rewards and penalties\",\n",
        "    \"Transfer learning reuses knowledge from pre-trained models\",\n",
        "    \"Unsupervised learning finds patterns without labeled data\",\n",
        "    \"Supervised learning requires labeled training examples\"\n",
        "]"
      ],
      "metadata": {
        "id": "cGz0jwmSMoC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")  # Initialize persisting client\n",
        "\n",
        "# Create embedding function\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "      model_name = \"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create or get collections\n",
        "try:\n",
        "  collection = client.get_collection(\n",
        "      name=\"document_embeddings\",\n",
        "      embedding_function = sentence_transformer_ef\n",
        "      )\n",
        "  print(\"Loading existing collection\")\n",
        "except Exception as e:\n",
        "  collection = client.create_collection(\n",
        "      name=\"document_embeddings\",\n",
        "      embedding_function = sentence_transformer_ef,\n",
        "      metadata = {\"description\": \"Document embedding for semantic search\"}\n",
        "  )\n",
        "  print(f\"Creating new collection: {e}\")\n",
        "\n",
        "# Check existence of docs before addign\n",
        "existing_count = collection.count()\n",
        "if existing_count == 0:\n",
        "  # Adding if empty\n",
        "  collection.add(         # In collection.add() uses plurals for the params\n",
        "    documents = documents,\n",
        "    metadatas = [{\"source\": f\"doc_{i}\", \"category\":\"AI/ML\"}\n",
        "                for i in range(len(documents))],\n",
        "    ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
        "  )\n",
        "  print(f\"Added {len(documents)} documents\")\n",
        "else:\n",
        "  print(f\"Collection already has {existing_count} documents\")\n",
        "\n",
        "\n",
        "# Perform semantic search\n",
        "print(\"\\nSemantic Search:\\n\")\n",
        "query = \"How do neural networks learn from examples?\"\n",
        "results = collection.query(    # query() finds most similar documents\n",
        "    query_texts=[query],\n",
        "    n_results = 3\n",
        ")\n",
        "\n",
        "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
        "  print(f\"{i+1}. Distance: {distance:.4f}. {doc}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Advanced search with metadata filtering\n",
        "print(\"\\n\\nFiltered search (category='AI/ML'):\\n\")\n",
        "filtered_results = collection.query(\n",
        "    query_texts = [\"What is learning without labels?\"],\n",
        "    n_results = 2,\n",
        "    where = {\"category\": \"AI/ML\"} # Filtering by metadata\n",
        ")\n",
        "\n",
        "for doc, dist in zip(filtered_results['documents'][0], filtered_results['distances'][0]) : print(f\"{dist:.4f} - {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnc4sU6oW6ba",
        "outputId": "87b11c28-f792-4613-b30c-325ae3657d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing collection\n",
            "Collection already has 8 documents\n",
            "\n",
            "Semantic Search:\n",
            "\n",
            "1. Distance: 0.9700. Deep learning uses neural networks with multiple layers\n",
            "2. Distance: 1.0247. Supervised learning requires labeled training examples\n",
            "3. Distance: 1.1870. Transfer learning reuses knowledge from pre-trained models\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Filtered search (category='AI/ML'):\n",
            "\n",
            "0.7620 - Supervised learning requires labeled training examples\n",
            "0.8435 - Unsupervised learning finds patterns without labeled data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Different Embedding Methods"
      ],
      "metadata": {
        "id": "SgA4LoD3dsmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "# Load required models\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Requires using the HF_TOKEN, I have it loaded on colab\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # AutoTokenizer automatically loads the correct tokenizer for that model\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name) # AutoModel loads the model architecture and weights\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# RENAME Word2Vec model to avoid conflict\n",
        "from gensim.models import Word2Vec\n",
        "sample_sentences = [[\"machine\", \"learning\", \"is\", \"great\"],\n",
        "                   [\"computers\", \"process\", \"information\", \"fast\"]]\n",
        "w2v_model = Word2Vec(sentences=sample_sentences, vector_size=100, window=5, min_count=1)  # Changed name!\n"
      ],
      "metadata": {
        "id": "LrjokoOwjHSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_embedding_methods(text):\n",
        "  \"\"\"Comparing different embedding methods for the same text\"\"\"\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  # TF-IDF\n",
        "  sample_docs = [text, \"Just another document\", \"And one more as yapa\"]\n",
        "  tfidf_vec = TfidfVectorizer(max_features = 50) # Dimensions\n",
        "  tfidf_matrix = tfidf_vec.fit_transform(sample_docs)\n",
        "  results['TF-IDF'] = {\n",
        "      'dimension': tfidf_matrix.shape[1],\n",
        "      'type': 'sparse',\n",
        "      'values': tfidf_matrix[0].toarray()[0][:5] # Just first 5 vals\n",
        "  }\n",
        "\n",
        "\n",
        "  # Word2Vec\n",
        "  words = text.lower().split()\n",
        "  valid_words = [w for w in words if w in w2v_model.wv] # Only existing words in the vocabulary\n",
        "  if not valid_words:\n",
        "    print(f\"Warning: No valid words found for Word2Vec from: {words}\")\n",
        "  print(\"valid_words\", valid_words)\n",
        "  if valid_words:\n",
        "    # Average word vectors\n",
        "    w2v_embedding = np.mean([w2v_model.wv[w] for w in valid_words], axis = 0)\n",
        "    results['Word2Vec'] = {\n",
        "      'dimension': len(w2v_embedding),\n",
        "      'type': 'dense',\n",
        "      'values': w2v_embedding[:5] # Just first 5 vals\n",
        "  }\n",
        "\n",
        "\n",
        "  # spaCy\n",
        "  spacy_doc = nlp(text)\n",
        "  results['spaCy'] = {\n",
        "    'dimension': len(spacy_doc.vector),\n",
        "    'type': 'dense',\n",
        "    'values': spacy_doc.vector[:5]\n",
        "  }\n",
        "\n",
        "  st_embedding = st_model.encode(text)\n",
        "  results['Sentence-Transformer'] = {\n",
        "    'dimension': len(st_embedding),\n",
        "    'type': 'dense',\n",
        "    'values': st_embedding[:5]\n",
        "  }\n",
        "\n",
        "  cls_emb, mean_emb = get_bert_embeddings(text)\n",
        "\n",
        "  results['BERT'] = {\n",
        "    'dimension': len(mean_emb),\n",
        "    'type':'dense, contextual',\n",
        "    'values': mean_emb[:5]\n",
        "  }\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compare diff methods\n",
        "test_text = \"Machine learning transforms how computers process information\"\n",
        "print(f\"Comparing embeddings for: '{test_text}'\\n\")\n",
        "\n",
        "comparison = compare_embedding_methods(test_text)\n",
        "for method, info in comparison.items():\n",
        "  print(f\"{method}:\")\n",
        "  print(f\"  Dimension: {info['dimension']}\")\n",
        "  print(f\"  Type: {info['type']}\")\n",
        "  print(f\"  First 5 values: {np.array(info['values']).round(3)}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWrIKVO7duV2",
        "outputId": "b649368b-fe86-43f1-f582-176bf3f3aadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing embeddings for: 'Machine learning transforms how computers process information'\n",
            "\n",
            "valid_words ['machine', 'learning', 'computers', 'process', 'information']\n",
            "TF-IDF:\n",
            "  Dimension: 15\n",
            "  Type: sparse\n",
            "  First 5 values: [0.    0.    0.    0.378 0.   ]\n",
            "\n",
            "Word2Vec:\n",
            "  Dimension: 100\n",
            "  Type: dense\n",
            "  First 5 values: [-0.     0.001  0.001  0.002  0.003]\n",
            "\n",
            "spaCy:\n",
            "  Dimension: 96\n",
            "  Type: dense\n",
            "  First 5 values: [-0.057  0.164 -0.27   0.308  0.079]\n",
            "\n",
            "Sentence-Transformer:\n",
            "  Dimension: 384\n",
            "  Type: dense\n",
            "  First 5 values: [-0.035  0.039  0.025 -0.048  0.049]\n",
            "\n",
            "BERT:\n",
            "  Dimension: 768\n",
            "  Type: dense, contextual\n",
            "  First 5 values: [-0.072 -0.25  -0.257  0.105  0.29 ]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usecase: PDF Question Answering with Semantic Search"
      ],
      "metadata": {
        "id": "H4jJYIdVpt0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download chapter from the book\n"
      ],
      "metadata": {
        "id": "YTIMFXPNBeci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://web.stanford.edu/~jurafsky/slp3/6.pdf # Easiest way\n",
        "\n",
        "# More python way\n",
        "import requests\n",
        "url = \"https://web.stanford.edu/~jurafsky/slp3/6.pdf\"\n",
        "\n",
        "resp = requests.get(url)\n",
        "\n",
        "with open(\"6.pdf\", \"wb\") as fh:\n",
        "  fh.write(resp.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6LZ5sBFAfJG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specific libs for this usecase"
      ],
      "metadata": {
        "id": "dPDTVrIaBaxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pdfplumber # Better for complex pdfs\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "mTGR88ac_BmG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Class\n"
      ],
      "metadata": {
        "id": "8oE0h9aDCQZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFSemanticSearch:\n",
        "  \"\"\"Semantic Search Engine for PDF documents.\"\"\"\n",
        "\n",
        "  def __init__(self, embedding_model='all-MiniLM-L6-v2'):\n",
        "    self.model = SentenceTransformer(embedding_model)\n",
        "    self. paragraphs = []\n",
        "    self.embeddings = None\n",
        "    self.metadata = []\n",
        "    self.full_text = \"\" # Keeping the complete text for reference\n",
        "    self.sections = []\n",
        "\n",
        "  def extract_text_from_pdf(self,pdf_path):\n",
        "    \"\"\"PDF text extraction with multiple methods\"\"\" # using just PyPDF2 was not enough\n",
        "    print(f\"Reading PDF: {pdf_path}\")\n",
        "    all_text = []\n",
        "\n",
        "    try:\n",
        "      # Method 1: pdfplumber, better for complex layouts\n",
        "      with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "          text = page.extract_text() or \"\"\n",
        "          if text.strip():\n",
        "            all_text.append({\n",
        "                'text': text,\n",
        "                'page': page_num + 1,\n",
        "                'method': 'pdfplumber'\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"pdfplumber failed :( \\nError: {e} \")\n",
        "\n",
        "    # Method 2: PyPDF2 as fallback if required\n",
        "    if not all_text:\n",
        "      print(\"Falling back to PyPDF2\")\n",
        "      with open(pdf_path, 'rb') as fh:\n",
        "        pdf_reader = PyPDF2.PdfReader(fh)\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "          page = pdf_reader.pages[page_num]\n",
        "          text = page.extract_text()\n",
        "          if text.strip():\n",
        "            all_text.append({\n",
        "                'text': text,\n",
        "                'page': page_num + 1,\n",
        "                'method': 'PyPDF2'\n",
        "            })\n",
        "\n",
        "    # Store full text for reference\n",
        "    self.full_text = \"\\n\\n\".join([page['text'] for page in all_text ])\n",
        "    return(all_text)\n",
        "\n",
        "\n",
        "  def clean_text(self, text):\n",
        "    \"\"\"Clean and normalize the extracted text.\"\"\"\n",
        "\n",
        "    # Common pdf extraction issues\n",
        "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text) # Adding space btwn camelCase\n",
        "    text = re.sub(r'(?<=[a-zA-Z])(?=[0-9])', ' ', text) # Add space between letter and number\n",
        "    text = re.sub(r'(?<=[0-9])(?=[a-zA-Z])', ' ', text)\n",
        "    text = re.sub(r'([a-z])([.!?,;:])([a-zA-Z])', r'\\1\\2 \\3', text)  # Fix words that are concatenated with punctuation\n",
        "    text = re.sub(r'\\s+', ' ' , text) # Normalize whitespaces\n",
        "    text = re.sub(r'(\\w)-\\s*\\n\\s*(\\w)', r'\\1\\2', text) # Fixing hyphenated words\n",
        "    text = re.sub(r'([.!?])\\s*\\n', r'\\1 ', text)  # Fix sentence breaks\n",
        "\n",
        "    # Remove excessive spaces but preserving paragraphs structure\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(cleaned_lines)\n",
        "\n",
        "\n",
        "  def split_into_chunks(self, pdf_text, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks for context preservation.\n",
        "    chunk_size: target size of each chunk in chars.\n",
        "    overlap: number of chars to overlap between chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "\n",
        "    for page_data in pdf_text:\n",
        "      text = self.clean_text(page_data['text'])\n",
        "      page_num = page_data['page']\n",
        "\n",
        "      # First try to split by paragraphs\n",
        "      paragraphs = re.split(r'\\n\\s*\\n|\\n(?=[A-Z])', text)\n",
        "\n",
        "      current_chunk = \"\"\n",
        "      for para in paragraphs:\n",
        "        para = para.strip()\n",
        "\n",
        "        # if the paragraph is too long, split it further\n",
        "        if len(para) > chunk_size:\n",
        "          #split by sentencse\n",
        "          sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "          for sent in sentences:\n",
        "            if len(current_chunk) + len(sent) > chunk_size and current_chunk:\n",
        "              chunks.append({\n",
        "                  'text': current_chunk.strip(),\n",
        "                  'page': page_num,\n",
        "                  'length': len(current_chunk)\n",
        "              })\n",
        "              # Keep overlap\n",
        "              current_chunk = current_chunk[-overlap:] + \" \" + sent\n",
        "            else:\n",
        "              current_chunk += \" \" + sent\n",
        "\n",
        "        # if adding paragraph does not exceed the chunk size\n",
        "        elif len(current_chunk) + len(para) <= chunk_size:\n",
        "          current_chunk += \"\\n\" + para if current_chunk else para\n",
        "\n",
        "\n",
        "        # If exceeds the chunk size, save current chunk and start a new one\n",
        "        else:\n",
        "          if current_chunk:\n",
        "            chunks.append({\n",
        "                'text': current_chunk.strip(),\n",
        "                'page': page_num,\n",
        "                'length': len(current_chunk)\n",
        "            })\n",
        "          current_chunk = para\n",
        "\n",
        "      # Last chunk\n",
        "      if current_chunk.strip():\n",
        "          chunks.append({\n",
        "              'text': current_chunk.strip(),\n",
        "              'page': page_num,\n",
        "              'length': len(current_chunk)\n",
        "          })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "  def extract_sections(self, text):\n",
        "    \"\"\" Extract section headers and structure from text.\"\"\"\n",
        "\n",
        "    # Common section patterns for pdfs\n",
        "    section_patterns = [\n",
        "      r'^(\\d+\\.?\\d*)\\s+([A-Z][A-Za-z\\s]+)$',  # Example: \"6.1 Lexical Semantics\"\n",
        "      r'^([A-Z][A-Za-z\\s]+):$',  # Example \"Introduction:\"\n",
        "      r'^#{1,3}\\s*(.+)$',  #  ###Markdown style headers\n",
        "    ]\n",
        "\n",
        "    sections = []\n",
        "    for pattern in section_patterns:\n",
        "      matches = re.finditer(pattern, text, re.MULTILINE) # re.MULTILINE modifies the behavior of the ^ (caret) and $ (dollar sign) anchors, acting also after the new lines \\n\n",
        "      for match in matches:\n",
        "        sections.append({\n",
        "          'title': match.group(0).strip(),\n",
        "          'start': match.start(),\n",
        "          'level': match.group(1) if match.lastindex > 1 else '0'\n",
        "        })\n",
        "\n",
        "    return(sorted(sections, key=lambda x: x['start']))\n",
        "\n",
        "\n",
        "\n",
        "  def index_pdf(self, pdf_path, chunk_size=500):\n",
        "    \"\"\"Extract and index chunks from the pdf with processing\"\"\"\n",
        "\n",
        "    # Extract text from the pdf\n",
        "    pdf_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if not pdf_text:\n",
        "      raise ValueError(\"No text could be extracted from the PDF.\")\n",
        "\n",
        "\n",
        "    # Extract sections for better context\n",
        "    self.sections = self.extract_sections(self.full_text)\n",
        "    print(f\"Found {len(self.sections)} sections in the document\")\n",
        "\n",
        "    # Split into chunks\n",
        "    chunk_data = self.split_into_chunks(pdf_text, chunk_size=chunk_size)\n",
        "\n",
        "    # Extract text and metadata\n",
        "    self.paragraphs = [c['text'] for c in chunk_data]\n",
        "    self.metadata = []\n",
        "\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "      # Find which section this chunk belongs to\n",
        "      chunk_start = self.full_text.find(chunk['text'][:50])\n",
        "      section = \"Unknown\"\n",
        "      for s in reversed(self.sections):\n",
        "        if s['start'] <= chunk_start:\n",
        "          section = s['title']\n",
        "          break\n",
        "\n",
        "      self.metadata.append({\n",
        "          'page': chunk['page'],\n",
        "          'length': chunk['length'],\n",
        "          'section': section,\n",
        "          'index': i\n",
        "      })\n",
        "\n",
        "    print(f\"\\nCreated {len(self.paragraphs)} chunks\")\n",
        "    print(f\"Average chunk size: {np.mean([m['length'] for m in self.metadata]):.0f} characters\")\n",
        "    print(\"Creating embeddings...\")\n",
        "\n",
        "    # Create the embeddings\n",
        "    self.embeddings = self.model.encode(\n",
        "        self.paragraphs,\n",
        "        show_progress_bar = True,\n",
        "        batch_size = 32,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    print(\"Indexing complete!\")\n",
        "\n",
        "  def search(self, query, top_k=5, threshold=0.25, rerank=True):\n",
        "    \"\"\"Search with optional reranking\"\"\"\n",
        "\n",
        "    if self.embeddings is None:\n",
        "      raise ValueError(\"No documents indexed. Use index_pdf() first.\")\n",
        "\n",
        "\n",
        "    # Encode query\n",
        "    query_embedding = self.model.encode([query], convert_to_numpy=True) # np.argsort(similarities) - Gets indices that would sort the array (ascending). top_k*2 candidates if reranking is enabled (to have more options for reranking), so if top_k=5 and rerank=True, it gets the indices of the 10 most similar chunks, which will later be reranked and trimmed to 5.\n",
        "\n",
        "    # Compute similarities\n",
        "    similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "\n",
        "    # Get top candidates\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k*2 if rerank else top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "      score = similarities[idx]\n",
        "      if score >= threshold:\n",
        "        results.append({\n",
        "            'paragraph': self.paragraphs[idx],\n",
        "            'score': score,\n",
        "            'page': self.metadata[idx]['page'],\n",
        "            'section': self.metadata[idx]['section'],\n",
        "            'index':idx\n",
        "        })\n",
        "\n",
        "    # Rerank based on additional factors\n",
        "    if rerank and results:\n",
        "      results = self._rerank_results(results, query)\n",
        "\n",
        "    return(results[:top_k])\n",
        "\n",
        "  def _rerank_results(self, results, query):\n",
        "    \"\"\"Rerank results based on additional criteria: exact matches in the text/section/title.\"\"\"\n",
        "\n",
        "    query_terms = set(query.lower().split())  # LowerCase and split to match exactly the query with the text\n",
        "\n",
        "    for result in results:\n",
        "      text_lower = result['paragraph'].lower() # LowerCase to match exactly the query with the text\n",
        "\n",
        "      # Boost score for exact term matches\n",
        "      term_matches = sum(1 for term in query_terms if term in text_lower)\n",
        "\n",
        "      # Boost for title/section matches\n",
        "      section_boost = 0.1 if any(term in result['section'].lower()\n",
        "                                for term in query_terms) else 0\n",
        "\n",
        "      # Combine scores\n",
        "      result['final_score'] = result['score'] + (term_matches * 0.05) + section_boost\n",
        "\n",
        "    # Sort by final score\n",
        "    return sorted(results, key=lambda x: x['final_score'], reverse=True)\n",
        "\n",
        "  def answer_question(self, question, top_k=3, show_context=True):\n",
        "    \"\"\" Question answering\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Searching for relevant chunks\n",
        "    results = self.search(question, top_k=top_k)\n",
        "\n",
        "    if not results:\n",
        "      print(\"No relevant content found. Review your question, pal.\")\n",
        "      return None\n",
        "\n",
        "    print(f\"Found {len(results)} relevant passages:\\n\")\n",
        "\n",
        "    # Display results\n",
        "    for i, result in enumerate(results, 1):\n",
        "      print(f\"Result {i} (Page {result['page']}, Score: {result['score']:.3f})\")\n",
        "      print(f\"Section: {result['section']}\")\n",
        "      print(\"-\" * 60)\n",
        "\n",
        "      # Format and display text\n",
        "      text = result['paragraph']\n",
        "\n",
        "      # Highlight query terms\n",
        "      highlighted_text = self._highlight_terms(text, question)\n",
        "\n",
        "      # Wrap text for better readability\n",
        "      wrapped_lines = textwrap.wrap(highlighted_text, width=80)\n",
        "      for line in wrapped_lines:\n",
        "          print(line)\n",
        "\n",
        "      print(\"\\n\")\n",
        "\n",
        "      # Show surrounding context if requested\n",
        "      if show_context and i==1: # For the top result only\n",
        "        self._show_context(result['index'])\n",
        "\n",
        "\n",
        "    return(results)\n",
        "\n",
        "  def _highlight_terms(self, text, query):\n",
        "    \"\"\"Highlight query terms in text.\"\"\"\n",
        "    # Extract important terms from query\n",
        "    stop_words = {'what', 'is', 'the', 'how', 'does', 'are', 'in', 'of', 'to', 'a', 'an'}\n",
        "    query_terms = [term.lower() for term in query.split()\n",
        "                  if term.lower() not in stop_words and len(term) > 2]\n",
        "\n",
        "    # Highlight each term\n",
        "    for term in query_terms:\n",
        "      # Case-insensitive replacement with word boundaries\n",
        "      pattern = re.compile(rf'\\b{re.escape(term)}\\b', re.IGNORECASE)\n",
        "      text = pattern.sub(lambda m: f\"**{m.group().upper()}**\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "  def _show_context(self, index, window=1):\n",
        "    \"\"\"Show surrounding context for a result.\"\"\"\n",
        "    print(\"\\n Extended Context:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Get surrounding chunks\n",
        "    start_idx = max(0, index - window)\n",
        "    end_idx = min(len(self.paragraphs), index + window + 1)\n",
        "\n",
        "    for i in range(start_idx, end_idx):\n",
        "      if i == index:\n",
        "        print(\">>> MAIN PASSAGE <<<\")\n",
        "      else:\n",
        "        print(f\"[Context from page {self.metadata[i]['page']}]\")\n",
        "\n",
        "      wrapped = textwrap.wrap(self.paragraphs[i], width=80)\n",
        "\n",
        "      for line in wrapped[:3]:  # Show first 3 lines of context\n",
        "        print(line)\n",
        "      if len(wrapped) > 3:\n",
        "        print(\"...\")\n",
        "      print()\n",
        "\n",
        "  def get_section_summary(self):\n",
        "    \"\"\"Get a summary of document sections.\"\"\"\n",
        "    if not self.sections:\n",
        "      return \"No sections found.\"\n",
        "\n",
        "    print(\"\\n Document Structure:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    current_level = None\n",
        "    for section in self.sections[:20]:  # Show first 20 sections\n",
        "      level = section.get('level', '0')\n",
        "      indent = \"  \" * (len(level.split('.')) - 1)\n",
        "      print(f\"{indent}{section['title']}\")\n",
        "\n",
        "    if len(self.sections) > 20:\n",
        "      print(f\"... and {len(self.sections) - 20} more sections\")\n",
        "\n",
        "\n",
        "\n",
        "# Create enhanced search engine\n",
        "pdf_search = PDFSemanticSearch()\n",
        "\n",
        "# Index the PDF\n",
        "pdf_path = \"6.pdf\"\n",
        "pdf_search.index_pdf(pdf_path, chunk_size=600)\n",
        "\n",
        "# Show document structure\n",
        "pdf_search.get_section_summary()"
      ],
      "metadata": {
        "id": "ju-MtZgftNrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763,
          "referenced_widgets": [
            "821b2afcebfe429bafe4f83f69686a8f",
            "b5da19b9789f4c97aeb9c812c769398f",
            "4a9a02fb07ef451581eb023360794c4e",
            "a34e3c1c709b4fdbb30e4685da572431",
            "026792d645844e7ba444f3cf7ee0a3d7",
            "e366c223ce58479b95964dfdd620470d",
            "0afdbd025e9844bdb11598e5a78d0bbe",
            "e596e5dfeb994d6194ba19237c0fd974",
            "ccb776b391184dd4b7b22eec6f0b83ae",
            "6810611fd2e345c28504c32a8eaa39dd",
            "7c723ed4be8046c4ab93a67f41bbf1ae"
          ]
        },
        "outputId": "f19041b6-b47f-473f-8178-1c6fd1ca52e1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading PDF: 6.pdf\n",
            "Found 18 sections in the document\n",
            "\n",
            "Created 224 chunks\n",
            "Average chunk size: 489 characters\n",
            "Creating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "821b2afcebfe429bafe4f83f69686a8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing complete!\n",
            "\n",
            " Document Structure:\n",
            "============================================================\n",
            "6 Vector Semantics and\n",
            "Embeddings\n",
            "  6.1 Lexical Semantics\n",
            "  6.2 Vector Semantics\n",
            "Andsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n",
            "  6.3 Words and Vectors\n",
            "  6.4 Cosine for measuring similarity\n",
            "10\n",
            "Notethattheidfweightinghaseliminatedtheimportanceoftheubiquitouswordgoodand\n",
            "Wemodeltheprobabilitythatwordcisarealcontextwordfortargetwordwas:\n",
            "  6.9 Visualizing Embeddings\n",
            "DOG\n",
            "CAT\n",
            "TURTLE\n",
            "PKUIPTPTYEN COW LION NASHVILLE\n",
            "  6.10 Semantic properties of embeddings\n",
            "Inthissectionwebrieflysummarizesomeofthesemanticpropertiesofembeddings\n",
            "# » # » # »\n",
            "# »\n",
            "# » # » # » # »\n",
            "# » # » # » # » # » # »\n",
            "# »\n",
            "# » # » # » # »\n",
            "  6.12 Evaluating Vector Models\n",
            "The most important evaluation metric for vector models is extrinsic evaluation on\n",
            "  6.13 Summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive mode with improved features\n",
        "def enhanced_interactive_qa():\n",
        "  \"\"\"Enhanced interactive Q&A with helpful features.\"\"\"\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "  print(\" Enhanced PDF Q&A System\")\n",
        "  print(\" Commands: 'help', 'sections', 'search [term]', 'quit'\")\n",
        "  print(\"=\"*80)\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nAsk a question (or command): \").strip()\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    elif user_input.lower() == 'help':\n",
        "      print(\"\\nAvailable commands:\")\n",
        "      print(\" - Ask any question about the content\")\n",
        "      print(\" - 'sections': Show document structure\")\n",
        "      print(\" - 'search [term]': Search for specific term\")\n",
        "      print(\" - 'quit': Exit the system\")\n",
        "\n",
        "    elif user_input.lower() == 'sections':\n",
        "      pdf_search.get_section_summary()\n",
        "\n",
        "    elif user_input.lower().startswith('search '):\n",
        "      search_term = user_input[7:]\n",
        "      print(f\"\\nSearching for '{search_term}'...\")\n",
        "      results = pdf_search.search(search_term, top_k=3)\n",
        "      if results:\n",
        "        print(f\"Found {len(results)} results containing '{search_term}'\")\n",
        "        for i, r in enumerate(results, 1):\n",
        "          print(f\"\\n{i}. Page {r['page']}, Section: {r['section']}\")\n",
        "          print(f\" Preview: {r['paragraph'][:100]}...\")\n",
        "\n",
        "    elif user_input:\n",
        "      pdf_search.answer_question(user_input)\n",
        "\n",
        "    else:\n",
        "      print(\"Please enter a question or command.\")\n",
        "\n",
        "# Run enhanced interactive mode\n",
        "enhanced_interactive_qa()\n",
        "\n",
        "# Example questions about Chapter 6 content\n",
        "questions = [\n",
        "    \"What is the distributional hypothesis?\",\n",
        "    \"How does Word2Vec skip-gram work?\",\n",
        "    \"What is the difference between sparse and dense embeddings?\",\n",
        "    \"How do we compute cosine similarity between vectors?\",\n",
        "    \"What is PPMI and how is it calculated?\",\n",
        "    \"What are the problems with bias in embeddings?\",\n",
        "    \"How does TF-IDF weighting work?\"\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gp6f8UPyI1jX",
        "outputId": "0394a9dc-0a14-471b-e95b-f126dae76e0e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " Enhanced PDF Q&A System\n",
            " Commands: 'help', 'sections', 'search [term]', 'quit'\n",
            "================================================================================\n",
            "\n",
            "Ask a question (or command): What are the problems with bias in embeddings?\n",
            "\n",
            "================================================================================\n",
            "Question: What are the problems with bias in embeddings?\n",
            "================================================================================\n",
            "\n",
            "Found 3 relevant passages:\n",
            "\n",
            "Result 1 (Page 27, Score: 0.571)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "nenand Goldberg,2019),andthisremainsanopenproblem. Historical embeddings are\n",
            "also being used to measure biases in the past. Garg et al. (2018) used\n",
            "embeddings from historical texts to measure the association be- tween embeddings\n",
            "for occupations and embeddings for names of various ethnici-\n",
            "tiesorgenders(forexampletherelativecosinesimilarityofwomen’snamesversus men’s to\n",
            "occupation words like ‘librarian’ or ‘carpenter’) across the 20 th century. They\n",
            "found that the cosines correlate **WITH** the empirical historical percentages\n",
            "of women or ethnic groups in those occupations.\n",
            "\n",
            "\n",
            "\n",
            " Extended Context:\n",
            "------------------------------------------------------------\n",
            "[Context from page 27]\n",
            "rm asystemdemeaningorevenignoringsomesocialgroups. Anyembedding-awareal-\n",
            "gorithmthatmadeuseofwordsentimentcouldthusexacerbatebiasagainst African\n",
            "Americans. Recent research focuses on ways to try to remove these kinds of\n",
            "...\n",
            "\n",
            ">>> MAIN PASSAGE <<<\n",
            "nenand Goldberg,2019),andthisremainsanopenproblem. Historical embeddings are\n",
            "also being used to measure biases in the past. Garg et al. (2018) used\n",
            "embeddings from historical texts to measure the association be- tween embeddings\n",
            "...\n",
            "\n",
            "[Context from page 27]\n",
            "es of women or ethnic groups in those occupations. Historical embeddings also\n",
            "repli- catedoldsurveysofethnicstereotypes;\n",
            "thetendencyofexperimentalparticipantsin 1933\n",
            "...\n",
            "\n",
            "Result 2 (Page 27, Score: 0.610)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "rm asystemdemeaningorevenignoringsomesocialgroups. Anyembedding-awareal-\n",
            "gorithmthatmadeuseofwordsentimentcouldthusexacerbatebiasagainst African\n",
            "Americans. Recent research focuses on ways to try to remove these kinds of\n",
            "biases, for examplebydevelopingatransformationoftheembeddingspacethatremovesgen-\n",
            "derstereotypesbutpreservesdefinitionalgender(Bolukbasietal.2016,Zhaoetal. 2017)\n",
            "or changing the training procedure (Zhao et al., 2018). However, although\n",
            "debiasing these sorts of debiasing may reduce **BIAS** in embeddings, they do\n",
            "not eliminate it (Gonenand Goldberg,2019),andthisremainsanopenproblem.\n",
            "\n",
            "\n",
            "Result 3 (Page 28, Score: 0.475)\n",
            "Section: 6.12 Evaluating Vector Models\n",
            "The most important evaluation metric for vector models is extrinsic evaluation on\n",
            "------------------------------------------------------------\n",
            "lembeddingalgorithmssufferfrominherentvariability. Forexamplebecause of\n",
            "randomness in the initialization and the random negative sampling, algorithms\n",
            "like word 2 vec may produce different results even from the same dataset, and\n",
            "in- dividual documents in a collection may strongly impact the resulting\n",
            "embeddings (Tianetal.2016,Hellrichand Hahn 2016,Antoniakand Mimno 2018). Whenem-\n",
            "beddings are used to study word associations in particular corpora, therefore,\n",
            "it is bestpracticetotrainmultipleembeddingswithbootstrapsamplingoverdocuments\n",
            "andaveragetheresults(Antoniakand Mimno,2018).\n",
            "\n",
            "\n",
            "\n",
            "Ask a question (or command): How do we compute cosine similarity between vectors?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Question: How do we compute cosine similarity between vectors?\n",
            "================================================================================\n",
            "\n",
            "Found 3 relevant passages:\n",
            "\n",
            "Result 1 (Page 11, Score: 0.628)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "ultthatseemssensible. Fig.6.8 showsavisualization. 500 cherry digital\n",
            "information 500 1000 1500 2000 2500 3000 ’eip‘ :1 noisnemi D Dimension 2:\n",
            "‘computer’ Figure 6.8 A (rough) graphical demonstration of **COSINE**\n",
            "**SIMILARITY**, showing vectors for threewords(cherry, digital,\n",
            "andinformation)inthetwodimensionalspacedefinedbycounts\n",
            "ofthewordscomputerandpienearby. Thefiguredoesn’tshowthecosine,\n",
            "butithighlightsthe angles;\n",
            "notethattheanglebetweendigitalandinformationissmallerthantheanglebetween\n",
            "cherryandinformation.\n",
            "\n",
            "\n",
            "\n",
            " Extended Context:\n",
            "------------------------------------------------------------\n",
            "[Context from page 11]\n",
            "-negative, thecosineforthesevectorsrangesfrom 0–1.\n",
            "Let’sseehowthecosinecomputeswhichofthewordscherryordigitaliscloser\n",
            "inmeaningtoinformation, justusingrawcountsfromthefollowingshortenedtable: pie\n",
            "...\n",
            "\n",
            ">>> MAIN PASSAGE <<<\n",
            "ultthatseemssensible. Fig.6.8 showsavisualization. 500 cherry digital\n",
            "information 500 1000 1500 2000 2500 3000 ’eip‘ :1 noisnemi D Dimension 2:\n",
            "‘computer’ Figure 6.8 A (rough) graphical demonstration of cosine similarity,\n",
            "...\n",
            "\n",
            "[Context from page 11]\n",
            "issmallerthantheanglebetween cherryandinformation. Whentwovectorsaremoresimilar,\n",
            "thecosineislargerbuttheangle issmaller;\n",
            "thecosinehasitsmaximum(1)whentheanglebetweentwovectorsissmallest\n",
            "...\n",
            "\n",
            "Result 2 (Page 19, Score: 0.615)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "6.8 • WORD 2 VEC 19\n",
            "occurnearthetargetifitsembeddingvectorissimilartothetargetembedding. To\n",
            "**COMPUTE** **SIMILARITY** **BETWEEN** these dense embeddings, we rely on the\n",
            "intuition that two vectors are similar if they have a high dot product (after\n",
            "all, **COSINE** is just a normalizeddotproduct).\n",
            "\n",
            "\n",
            "Result 3 (Page 10, Score: 0.551)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "Therawdotproduct thuswillbehigherforfrequentwords. Butthisisaproblem;\n",
            "we’dlikeasimilarity\n",
            "metricthattellsushowsimilartwowordsareregardlessoftheirfrequency. We modify the\n",
            "dot product to normalize for the vector length by dividing the\n",
            "dotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\n",
            "turnsouttobethesameasthecosineoftheanglebetweenthetwovectors, following\n",
            "fromthedefinitionofthedotproductbetweentwovectorsaandb: a·b = |a||b|cosθ a·b =\n",
            "cosθ (6.9) |a||b| **COSINE**\n",
            "Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas:\n",
            "\n",
            "\n",
            "\n",
            "Ask a question (or command): How does Word2Vec skip-gram work?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Question: How does Word2Vec skip-gram work?\n",
            "================================================================================\n",
            "\n",
            "Found 3 relevant passages:\n",
            "\n",
            "Result 1 (Page 19, Score: 0.681)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "ftheembeddingsofthetargetword witheachcontextword. Tocomputethisprobability,\n",
            "wejustneedembeddingsfor eachtargetwordandcontextwordinthevocabulary. Fig. 6.13\n",
            "shows the intuition of the parameters we’ll need. **SKIP-GRAM** actually\n",
            "storestwoembeddingsforeachword, oneforthewordasatarget, andoneforthe\n",
            "wordconsideredascontext. Thustheparametersweneedtolearnaretwomatrices W and C,\n",
            "each containing an embedding for every one of the |V| words in the vocabulary\n",
            "V.6 Let’snowturntolearningtheseembeddings(whichistherealgoal\n",
            "oftrainingthisclassifierinthefirstplace).\n",
            "\n",
            "\n",
            "\n",
            " Extended Context:\n",
            "------------------------------------------------------------\n",
            "[Context from page 19]\n",
            "word, but there are many context wordsinthewindow. Skip-\n",
            "grammakesthesimplifyingassumptionthatallcontext wordsareindependent,\n",
            "allowingustojustmultiplytheirprobabilities: L (cid:89) P(+|w, c 1:L ) = σ(c i\n",
            "...\n",
            "\n",
            ">>> MAIN PASSAGE <<<\n",
            "ftheembeddingsofthetargetword witheachcontextword. Tocomputethisprobability,\n",
            "wejustneedembeddingsfor eachtargetwordandcontextwordinthevocabulary. Fig. 6.13\n",
            "shows the intuition of the parameters we’ll need. Skip-gram actually\n",
            "...\n",
            "\n",
            "[Context from page 19]\n",
            "realgoal oftrainingthisclassifierinthefirstplace). 6\n",
            "Inprinciplethetargetmatrixandthecontextmatrixcouldusedifferentvocabularies,\n",
            "butwe’llsimplify byassumingonesharedvocabulary V.\n",
            "\n",
            "Result 2 (Page 17, Score: 0.629)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "a neighbor and a word with automobileasaneighbor. **SKIP-GRAM**\n",
            "Inthissectionweintroduceonemethodforcomputingembeddings: **SKIP-GRAM** SGNS\n",
            "withnegativesampling, sometimescalled SGNS.Theskip-gramalgorithmisone word 2 vec\n",
            "of two algorithms in a software package called word 2 vec, and so sometimes the\n",
            "algorithm is loosely referred to as word 2 vec (Mikolov et al. 2013 a, Mikolov\n",
            "et al. 2013 b). Theword 2 vecmethodsarefast, efficienttotrain,\n",
            "andeasilyavailableon-\n",
            "\n",
            "\n",
            "Result 3 (Page 23, Score: 0.655)\n",
            "Section: Unknown\n",
            "------------------------------------------------------------\n",
            "orms for each noun and verb may only occur rarely. Fasttext deals with these\n",
            "problems byusingsubwordmodels, representingeachwordasitselfplusabagofconstituent\n",
            "n-grams, withspecialboundarysymbols<and>addedtoeachword. Forexample, withn=3\n",
            "thewordwherewouldberepresentedbythesequence<where>plusthe charactern-grams: <wh,\n",
            "whe, her, ere, re> Then a skipgram embedding is learned for each constituent\n",
            "n-gram, and the word\n",
            "whereisrepresentedbythesumofalloftheembeddingsofitsconstituentn-grams.\n",
            "Unknownwordscanthenbepresentedonlybythesumoftheconstituentn-grams.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4275910095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Run enhanced interactive mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0menhanced_interactive_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4275910095.py\u001b[0m in \u001b[0;36menhanced_interactive_qa\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAsk a question (or command): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}