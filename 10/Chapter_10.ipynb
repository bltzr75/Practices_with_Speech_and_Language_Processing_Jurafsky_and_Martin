{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes peft torchinfo torchmetrics\n",
        "!pip install -q pytorch-lightning wandb"
      ],
      "metadata": {
        "id": "v7QAVcTCY1SB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi4yoo52jlLB",
        "outputId": "8a43e883-1092-424a-ef11-b62f45a6c69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "# PyTorch ecosystem imports\n",
        "from torch.cuda.amp import GradScaler  # Mixed precision training\n",
        "from torch import amp\n",
        "from transformers import AutoTokenizer, GPT2Tokenizer  # HuggingFace tokenizers\n",
        "import torch._dynamo as dynamo  # For torch.compile\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Colab GPU detection and setup\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "  print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print(\"No GPU available, using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoregressive Generation"
      ],
      "metadata": {
        "id": "v-ml4BiRPMuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLanguageModel:\n",
        "  \"\"\"Vanilla Autoregressive text generation\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size: int, context_length:int ):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_length = context_length\n",
        "\n",
        "    # Simple bigram probs as transition matrix. A â€œtransition matrixâ€ originates from Markov chain theory: A Markov chain is a stochastic process where the probability of the next state depends only on the current state (the Markov property). In a Markov chain with ð‘› n states, the transition matrix encodes all the probabilities of moving from one state to another. Entry ð‘ƒ ð‘– ð‘— P ij â€‹ = probability of transitioning from state ð‘– i to state ð‘— j\n",
        "    self.transition_matrix = np.random.dirichlet(np.ones(vocab_size), size=vocab_size) # [V, V] stochastic matrix\n",
        "    print(f\"Transition matrix shape: {self.transition_matrix.shape}\")\n",
        "    print(f\"Sample row (sums to 1): {self.transition_matrix[0,:]}  sum={self.transition_matrix[0].sum():.3f}\")\n",
        "\n",
        "  def get_next_token_probs(self, context: List[int]) -> np.ndarray:\n",
        "    \"\"\"Get prob distrib for next token given the context\"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "      ## If no context apply Uniform distrib\n",
        "      return(np.ones(self.vocab_size)/self.vocab_size)\n",
        "\n",
        "    last_token = context[-1] # Bigram uses just the latest token to predict the next\n",
        "    print(f\"Last token: {last_token}, vocab_size: {self.vocab_size}\")\n",
        "\n",
        "    probs = self.transition_matrix[last_token] # prob P(next|last)\n",
        "    print(f\"\\nContext: {context} -> Last token: {last_token}\")\n",
        "    print(f\"Next token probs: {probs[:5]} ... (showing first 5)\")\n",
        "    return probs\n",
        "\n",
        "  def generate(self, prompt: List[int], max_new_tokens: int, strategy: str = 'greedy') -> List[int]:\n",
        "    \"\"\"Generate tokens autoregressively\"\"\"\n",
        "\n",
        "    context = prompt.copy()\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "      # get prob distrib for nxt token\n",
        "      probs = self.get_next_token_probs(context[-self.context_length:]) # slicing and using the last context_length tokens in the prompt(=context)\n",
        "\n",
        "\n",
        "      if   strategy == 'greedy':\n",
        "        next_token = np.argmax(probs) # Choose most likely, deterministic\n",
        "        print(f\"Step {i}: Greedy selected token {next_token} with p={probs[next_token]:.3f}\")\n",
        "\n",
        "      elif strategy == 'sample':\n",
        "        next_token = np.random.choice(self.vocab_size, p=probs) # Sample from ditrib\n",
        "        print(f\"Step {i}: Sampled token {next_token} with p={probs[next_token]:.3f}\")\n",
        "\n",
        "\n",
        "      context.append(next_token) # Starts from the prompt/context and keeps generating increasing it\n",
        "\n",
        "      return(context)"
      ],
      "metadata": {
        "id": "4h-qDiZ9PJTu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple Test"
      ],
      "metadata": {
        "id": "t49e3oa7RSXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test vanilla implementation\n",
        "vocab_size = 10  # Small vocab for visualization\n",
        "model = SimpleLanguageModel(vocab_size=vocab_size, context_length=5)\n",
        "\n",
        "prompt = [1, 2, 3]\n",
        "print(f\"\\nGenerating from prompt: {prompt}\\n\")\n",
        "\n",
        "# Compare greedy vs sampling\n",
        "greedy_output = model.generate(prompt, max_new_tokens=5, strategy='greedy')\n",
        "print(f\"\\nGreedy output: {greedy_output}\")\n",
        "\n",
        "sampled_output = model.generate(prompt, max_new_tokens=5, strategy='sample')\n",
        "print(f\"\\nSampled output: {sampled_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTmII5v7PeY_",
        "outputId": "7c045193-00d9-42a4-d4f4-30e4fd1d3a85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition matrix shape: (10, 10)\n",
            "Sample row (sums to 1): [0.04569443 0.29310702 0.12821656 0.0888967  0.01651702 0.01651424\n",
            " 0.00582673 0.19584123 0.08949454 0.11989152]  sum=1.000\n",
            "\n",
            "Generating from prompt: [1, 2, 3]\n",
            "\n",
            "Last token: 3, vocab_size: 10\n",
            "\n",
            "Context: [1, 2, 3] -> Last token: 3\n",
            "Next token probs: [0.08215403 0.01642155 0.00590806 0.26119085 0.2960559 ] ... (showing first 5)\n",
            "Step 0: Greedy selected token 4 with p=0.296\n",
            "\n",
            "Greedy output: [1, 2, 3, np.int64(4)]\n",
            "Last token: 3, vocab_size: 10\n",
            "\n",
            "Context: [1, 2, 3] -> Last token: 3\n",
            "Next token probs: [0.08215403 0.01642155 0.00590806 0.26119085 0.2960559 ] ... (showing first 5)\n",
            "Step 0: Sampled token 0 with p=0.082\n",
            "\n",
            "Sampled output: [1, 2, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoregressive Generation with torch.compile and HuggingFace tokenizers"
      ],
      "metadata": {
        "id": "rUY7ORr2azrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler # mix precision\n",
        "from torch import amp\n",
        "from transformers import AutoTokenizer, GPT2Tokenizer\n",
        "\n",
        "class OptimizedLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size:int, d_model:int, context_length:int):\n",
        "    super(OptimizedLanguageModel, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.context_length = context_length\n",
        "\n",
        "    # Model components\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional_embedding = nn.Embedding(context_length, d_model)\n",
        "    self.attention = nn.MultiheadAttention(d_model, num_heads=4, batch_first=True)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    print(f\"Model params: {sum(p.numel() for p in self.parameters()):,}\")\n",
        "\n",
        "  def forward(self, idx:torch.Tensor) -> torch.Tensor:\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding(idx) # [B,T,d_model]\n",
        "\n",
        "    pos = torch.arange(T, device=idx.device)\n",
        "    pos_emb = self.positional_embedding(pos)\n",
        "\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    # Causal mask\n",
        "    mask = torch.triu(torch.ones(T,T,device=idx.device),diagonal=1).bool()\n",
        "    x, _ = self.attention(x,x,x, attn_mask=mask) # scaled dot-product attention mechanism: [Q, K,V]\n",
        "\n",
        "    logits = self.lm_head(x) # [B, T, vocab_size]\n",
        "\n",
        "    return(logits)\n",
        "\n"
      ],
      "metadata": {
        "id": "j_hSDlQuRP-1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using HF pretrained tokenizers\n",
        "- Rust backend = Fast\n",
        "- Manages special tokens, padding and truncation automatically.\n",
        "- Good for edge cases, multilingual, fast."
      ],
      "metadata": {
        "id": "8PxAPXKbrcUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"\\nTokenizer details:\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"  Model max length: {tokenizer.model_max_length}\")\n",
        "print(f\"  Special tokens: {tokenizer.special_tokens_map}\")\n",
        "print(f\"  Example encoding: 'Hello world!' -> {tokenizer.encode('Hello world!')}\")\n",
        "print(f\"  Decoded back: {tokenizer.decode(tokenizer.encode('Hello world!'))}\")\n",
        "\n",
        "\n",
        "# create and compile model\n",
        "model = OptimizedLanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=256,\n",
        "    context_length=128\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# torch.compile options:\n",
        "# - default: balanced optim\n",
        "# - reduce-overhead: best for small models, minimizes kernel launch overhead\n",
        "# - max_autotune: best for large models, spends more time finding optimial kernels\n",
        "# - max-autotune-no-cudagraphs: flexible, max optimization without cuda graphs\n",
        "\n",
        "if hasattr(torch, 'compile') and device.type == 'cuda':\n",
        "  # Compile only makes sense on GPU (uses Triton compiler for CUDA)\n",
        "  model_compiled = torch.compile(model, mode=\"reduce-overhead\")\n",
        "  print(\"\\n Model compiled with torch.compile (reduce-overhead mode)\")\n",
        "  print(\"  First call will be slow (compilation), subsequent calls fast\")\n",
        "else:\n",
        "  model_compiled = model\n",
        "  print(\"\\ntorch.compile not available or on CPU, using eager mode\")\n",
        "\n",
        "\n",
        "# Autom Mixed Precision (amp)\n",
        "# master weights in FLP32\n",
        "# forward in FLP16\n",
        "# Automatic casting based on operation\n",
        "# loss scaling prevents grad underflow\n",
        "# good for Tensor Cores, with too large models not fitting in mem and in Training\n",
        "\n",
        "def generate_with_amp(\n",
        "    model: nn.Module,\n",
        "    prompt: str,\n",
        "    tokenizer,\n",
        "    max_new_tokens: int=50,\n",
        "    temperature: float = 1.0) -> str:\n",
        "\n",
        "  \"\"\"Generate text with automatic mixed precision (AMP) \"\"\"\n",
        "\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device) # pt for PyTorch, there is also for np and tf\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # autocast\n",
        "    with amp.autocast(device_type=device.type, enabled=(device.type=='cuda')):\n",
        "      for i in range(max_new_tokens):\n",
        "        # runs in float16 but some ops like softmax stay in float32\n",
        "        logits = model_compiled(input_ids)\n",
        "        logits = logits[:, -1, :] / temperature # Scaling with temp\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1) # softmax in float32  for stability\n",
        "        next_token = torch.multinomial(probs, num_samples = 1)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "          break\n",
        "\n",
        "  return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BekIVtYpbUlC",
        "outputId": "78de276b-b64a-4b02-e8bc-254debc4b8b3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenizer details:\n",
            "  Vocab size: 50257\n",
            "  Model max length: 1024\n",
            "  Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
            "  Example encoding: 'Hello world!' -> [15496, 995, 0]\n",
            "  Decoded back: Hello world!\n",
            "Model params: 26,078,289\n",
            "\n",
            " Model compiled with torch.compile (reduce-overhead mode)\n",
            "  First call will be slow (compilation), subsequent calls fast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing compiled vs eager mode"
      ],
      "metadata": {
        "id": "x4tB8s3avtLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_inference(model, compiled_model, input_ids, n_runs=10):\n",
        "\n",
        "  # Warmup - triggers compilation\n",
        "  print(\"Warming up (triggering compilation)...\")\n",
        "  for _ in range(2): # When benchmarking models, the first few runs are usually slower due to: Lazy initialization (loading kernels, allocating memory). JIT compilation overhead (torch.compile, TensorRT, XLA, etc). Running a couple of warmup iterations ensures those costs are paid upfront, so later timings reflect steady-state performance\n",
        "    _ = model(input_ids)\n",
        "    _ = compiled_model(input_ids)\n",
        "\n",
        "\n",
        "  # Eager mode timing\n",
        "  if device.type == 'cuda':\n",
        "    torch.cuda.synchronize() # waiting for gpu to finish, cuda runs async\n",
        "\n",
        "  start = time.time()\n",
        "  for _ in range(n_runs): # Runs the model n_runs times on the same input_ids. Used to measure average runtime or variance across runs\n",
        "    with torch.no_grad(): # not training\n",
        "      _ = model(input_ids)\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    torch.cuda.synchronize() # waiting for gpu to finish, cuda runs async\n",
        "\n",
        "  eager_time = time.time() -start\n",
        "\n",
        "\n",
        "  # Compiled mode timing\n",
        "  if device.type == 'cuda':\n",
        "    torch.cuda.synchronize() # waiting for gpu to finish, cuda runs async\n",
        "\n",
        "  start = time.time()\n",
        "  for _ in range(n_runs): # Runs the model n_runs times on the same input_ids. Used to measure average runtime or variance across runs\n",
        "    with torch.no_grad(): # not training\n",
        "      _ = compiled_model(input_ids)\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "  compiled_time = time.time() - start\n",
        "\n",
        "  print(f\"\\nInference benchmark ({n_runs} runs):\")\n",
        "  print(f\"  Eager mode: {eager_time:.3f}s ({1000*eager_time/n_runs:.1f}ms per call)\")\n",
        "  print(f\"  Compiled: {compiled_time:.3f}s ({1000*compiled_time/n_runs:.1f}ms per call)\")\n",
        "  if compiled_time > 0:\n",
        "    print(f\"  Speedup: {eager_time/compiled_time:.2f}x\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "H9umyfSMrkui"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on realistic input"
      ],
      "metadata": {
        "id": "T-zidyygyee4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.randint(0, tokenizer.vocab_size, (1, 32)).to(device)\n",
        "benchmark_inference(model, model_compiled, test_input)\n",
        "\n",
        "# generate\n",
        "prompt = \"The future of AI is\"\n",
        "generated = generate_with_amp(model_compiled, prompt, tokenizer, max_new_tokens=20)\n",
        "print(f\"\\nGenerated: {generated}\")\n",
        "\n",
        "print(\"\\nNote: Have to do the benchmark with longer sequences or multiple prompts to see real gains.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzl4OUXIydj9",
        "outputId": "8bb59560-a466-46b2-a474-dc566c03e7fb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming up (triggering compilation)...\n",
            "\n",
            "Inference benchmark (10 runs):\n",
            "  Eager mode: 0.008s (0.8ms per call)\n",
            "  Compiled: 0.007s (0.7ms per call)\n",
            "  Speedup: 1.10x\n",
            "\n",
            "Generated: The future of AI is suspect Education Ib sympathetic finRumPassword kins viewers phantomraseDead microbiotacknow FedExØ§ï¿½ pun baconalk leaders\n",
            "\n",
            "Note: Have to do the benchmark with longer sequences or multiple prompts to see real gains.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piRKtnBIylrh"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}