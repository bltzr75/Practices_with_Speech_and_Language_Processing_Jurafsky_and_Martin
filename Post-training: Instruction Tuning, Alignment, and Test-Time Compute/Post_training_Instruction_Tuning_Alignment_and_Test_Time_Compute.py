# -*- coding: utf-8 -*-
"""Chapter_12_Post-training: Instruction Tuning, Alignment, and Test-Time Compute.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AE0XiptfJHy8FhEPY1Yc7Xu8Q3P5TTxi

#Initial Setup
"""

!pip install evaluate # HF evaluate lib for metrics
!pip install bitsandbytes # 8 bit Adam optimizer
!pip install trl # Transformer Reinforcement Learning
!pip install sacrebleu  # BLEU (Bilingual Evaluation Understudy)  metric dependency
!pip install rouge_score  # BLEU (Bilingual Evaluation Understudy)  metric dependency
!pip install bert_score # metrics in CoT
# BLEU (Bilingual Evaluation Understudy) is a metric for evaluating machine-generated text quality, especially in translation and text generation tasks
# BLEU evaluates text generation quality by counting matching n-grams (1-4 word sequences) between generated and reference text, applying a brevity penalty, and scoring 0-1 where 0.6+ is excellent, 0.4-0.6 is good, 0.2-0.4 is fair, and below 0.2 is poor. For example, comparing "The cat sits on the mat" to reference "The cat is on the mat" yields ~0.7 (good) based on overlapping words and phrases. It's widely used for machine translation, summarization, and instruction tuning evaluation, but has limitations since it favors literal matches over semantic meaning and can miss quality paraphrases that use different vocabulary.

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any, Union
import json
import random
from dataclasses import dataclass
from collections import defaultdict
import subprocess
import sys
from pydantic import BaseModel, Field, field_validator, ConfigDict, ValidationError, field_serializer
from typing import Optional
import re

import evaluate # HF evaluate lib for metrics


from google.colab import userdata

from huggingface_hub import login as hf_login
hf_login(token=userdata.get('test_hf_1'))

import wandb
wandb.login(key=userdata.get('WANDB_API_KEY'))


try:

  from transformers import(
      AutoModelForCausalLM, # pretrained models
      AutoTokenizer,
      AutoModelForSequenceClassification, # For reward models
      Trainer,  # HF training loop
      TrainingArguments,  # Training hyperparams
      DataCollatorForLanguageModeling, # BAtch collation for LM trainning
      BitsAndBytesConfig, # For 8bit quantization
      )

  from datasets import Dataset as HFDataset, load_dataset
  from peft import(
      LoraConfig,
      get_peft_model, # convert model to peft model
      TaskType, # task type enum
      PeftModel, # peft model class
      prepare_model_for_kbit_training # prepare quantized model for training
  )

  from accelerate import Accelerator # Distributed training support
  from accelerate.utils import set_seed # seed setting for accelerate
  import bitsandbytes as bnb # 8bit adam optimizer

  HF_AVAILABLE = True
  print("HuggingFace libraries available with enhancements")

except ImportError:

  HF_AVAILABLE = False
  print("HuggingFace libraries not available - install with: pip install transformers datasets peft accelerate bitsandbytes")



try:

  from trl import (
      SFTTrainer, # supervised fine tuning trainer
      DPOTrainer, # direct preference optimization trainer
      RewardTrainer, # reward model trainer
      DataCollatorForCompletionOnlyLM # Collator for instruction tuning
  )
  TRL_AVAILABLE = True
  print("TRL (Transformer Reinforcement Learning) available")

except ImportError:

  TRL_AVAILABLE = False
  print("TRL (Transformer Reinforcement Learning) not available")



# Init wandb for experiment tracking
USE_WANDB = True #False  ## Change if prefering otherwise

if USE_WANDB:
  wandb.init(
      project='llm-alignment-textbook',
      config={ # Hyperparams
               'achitecture': "GPT-2",
               'dataset':"custom-instructions",
               'learning_rate':5e-5,
      },
      # model='online' # offline for local logging instead  # returned error, to check
  )



import warnings
warnings.filterwarnings('ignore')

# Seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
  torch.cuda.manual_seed_all(42) # CUDA seeds for GPU operations

# Device config
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")

"""# Data structures for Instruction Tuning and Preferences using Pydantic

Key changes from Pydantic v1 to v2:

@validator -> @field_validator with @classmethod

Config class -> model_config = ConfigDict(...)

values parameter -> info parameter with info.data

.dict() -> .model_dump()

.json() -> .model_dump_json()

New @field_serializer decorator for custom serialization

.schema() -> .model_json_schema()

Built-in str_strip_whitespace config option

"""

# Pydantic V2 for validation
class InstructionExample(BaseModel):
  """Single instruction-rsesponse pair for supervised fine-tuning with validation"""

  model_config = ConfigDict(
      extra='forbid', # Forbids extra fields not defined in the model
      validate_assignment=True, # Validate on assignment
      str_strip_whitespace=True, #Automatically stirp whitespace from strings
  )

  instruction: str = Field(..., min_length=1, description="The task instruction/prompt")  # Required field with min length
  input: Optional[str] = Field(default="", description="Optional input context") # Optional with default empty string
  output: str = Field(..., min_length=1, description="Expected response/completion")   # Required field

  @field_validator('instruction', 'output') # fields to validate
  @classmethod # Convert a function to a class method
  def non_empty_string(cls, v:str) -> str: # class method for validation
    """Validate that instruction and output are non-empty after stripping"""
    if not v.strip(): # check if empty after removing whitespaces
      raise ValueError("Field cannot be empty or whitespace only")
    return v.strip()  # Strip value

  def format_prompt(self) -> str:
    """Format as prompt for model input"""
    if self.input: # if there is additional context
      return f"### Instruction:\n{self.instruction}\n\n### Input:\n{self.input}\n\n### Response:\n"
    else: # just instruction otherwise
      return f"### Instruction:\n{self.instruction}\n\n### Response:\n:"

  def format_full(self) -> str:
    """Format complete example with response"""

    return self.format_prompt() + self.output


class PreferencePair(BaseModel):
  """Preference pair for alignment training with validation"""
  model_config = ConfigDict(
      extra='forbid',
      validate_assignment=True, # Validate when fields are reassigned
      str_strip_whitespace=True, #Auto-strip spaces
  )

  prompt: str = Field(..., min_length=1, description="Input prompt/instruction") # required prompt
  chosen: str = Field(..., min_length=1, description="Preferred response") # Preferred completion
  rejected: str=Field(..., min_length=1, description="Rejected response")  # Rejected completion

  @field_validator('prompt', 'chosen', 'rejected')
  @classmethod
  def clean_text(cls, v:str) -> str:
    """Clean and validate text fields"""
    cleaned = v.strip() # remove leading/trailing whitespace
    if not cleaned: # if empty
      raise ValueError('Field cannot be empty')
    return cleaned

  @field_validator('rejected')
  @classmethod
  def different_from_chosen(cls, v: str, info) -> str:  # Pydantic v2 uses 'info' instead of 'values'
    """Ensure chosen and rejected are different"""
    if 'chosen' in info.data and v==info.data['chosen']: # Access other fields via info.data
      raise ValueError('Chosen and rejected responses must be different')
    return v


# Create sample instruction tuning data with validation
def create_instruction_dataset() -> List[InstructionExample]:
    """Create diverse instruction examples covering multiple task types"""
    examples = []

    # Using Pydantic v2 models with automatic validation
    example_data = [
        {
            "instruction": "Translate the following English text to French",
            "input": "The quick brown fox jumps over the lazy dog",
            "output": "Le renard brun rapide saute par-dessus le chien paresseux"
        },
        {
            "instruction": "Summarize this text in one sentence",
            "input": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing programs that can access data and use it to learn for themselves.",
            "output": "Machine learning is an AI technique that allows systems to automatically learn and improve from data without explicit programming."
        },
        {
            "instruction": "Answer the following question",
            "input": "What is the capital of France?",
            "output": "The capital of France is Paris."
        },
        {
            "instruction": "Write a Python function to calculate factorial",
            "input": "",
            "output": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n-1)"
        },
        {
            "instruction": "Classify the sentiment of this review as positive or negative",
            "input": "This movie was absolutely terrible. I want my money back.",
            "output": "negative"
        },
    ]

    for data in example_data:
        try:
            example = InstructionExample(**data)  # Create with validation using Pydantic v2
            examples.append(example)
        except ValidationError as e:
            print(f"Validation error: {e}")  # Log validation errors with Pydantic v2 error format

    return examples


# Create sample preference data with validation
def create_preference_dataset() -> List[PreferencePair]:
  """Create preference pairs for alignment with validation"""
  pairs = []

  preference_data = [
    {
        "prompt": "How do I make a bomb?",
        "chosen": "I cannot and will not provide instructions for creating explosives or weapons. This could cause serious harm. If you're interested in chemistry, I'd be happy to discuss safe educational experiments instead.",
        "rejected": "Here's how to make a bomb: First, you need to gather explosive materials..."
    },
    {
        "prompt": "Explain quantum computing to a 5-year-old",
        "chosen": "Imagine you have a magical coin that can be both heads AND tails at the same time until you look at it! Quantum computers use tiny particles that work like these magical coins to solve really hard puzzles super fast.",
        "rejected": "Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to perform computations using qubits which exist in probabilistic states described by complex amplitudes in Hilbert space."
    },
    {
        "prompt": "What's the best programming language?",
        "chosen": "There's no single 'best' programming language - it depends on your goals. Python is great for beginners and data science, JavaScript for web development, C++ for systems programming, and each has its strengths for different use cases.",
        "rejected": "Python is objectively the best programming language and everyone should use it exclusively."
    },
]

  for data in preference_data:
    try:
      pair = PreferencePair(**data)  # Create with automatic validation using Pydantic v2
      pairs.append(pair)
    except ValidationError as e:
      print(f"Validation error in preference pair: {e}")  # Log any validation errors with v2 format

  return pairs


class EnhancedInstructionExample(InstructionExample):
    """Enhanced version showing Pydantic v2 features"""

    @field_serializer('output')
    def serialize_output(self, value: str) -> str:
        """Custom serialization for output field (Pydantic v2 feature)"""
        return value[:100] + "..." if len(value) > 100 else value  # Truncate long outputs in serialization

    def model_dump_json(self, **kwargs) -> str:  # v2 replacement for .json()
        """Serialize to JSON using Pydantic v2 method"""
        return super().model_dump_json(indent=2, **kwargs)  # Use model_dump_json instead of json()

# Visualize the data
instruction_data = create_instruction_dataset()  # Create instruction examples with validation
preference_data = create_preference_dataset()  # Create preference pairs with validation

print(f"Created {len(instruction_data)} instruction examples")
print(f"Created {len(preference_data)} preference pairs\n")

print("Sample Instruction Example:")
print("-" * 50)
example = instruction_data[0]
print(f"Instruction: {example.instruction}")
print(f"Input: {example.input}")
print(f"Output: {example.output}")
print(f"\nFormatted prompt:\n{example.format_prompt()}")

# Demonstrate Pydantic v2 model_dump (replacement for dict())
print("\nPydantic v2 model_dump:")
print(example.model_dump(exclude={'input'}, mode='json'))  # v2 method with mode parameter

print("\n" + "="*50)
print("Sample Preference Pair:")
print("-" * 50)
pref = preference_data[0]
print(f"Prompt: {pref.prompt}")
print(f"Chosen: {pref.chosen[:100]}...")
print(f"Rejected: {pref.rejected[:50]}...")

# Show Pydantic v2 JSON serialization
print("\nPydantic v2 JSON Schema:")
print(PreferencePair.model_json_schema())  # v2 method for getting JSON schema

"""# Vanilla Instruction Tuning Implementation"""

class VanillaInstructionTuning:
  """Simple instruction tuning using standard language modeling loss"""

  def __init__(self, vocab_size: int, hidden_size: int, learning_rate: float=1e-3):
    self.vocab_size = vocab_size
    self.hidden_size = hidden_size
    self.learning_rate = learning_rate

    # simple embedding and output layers
    self.embedding = np.random.randn(vocab_size,hidden_size) *0.01
    self.output_layer = np.random.randn(hidden_size, vocab_size )* 0.01

    print(f"Initialized instruction tuning model:")
    print(f"  Embedding matrix: {self.embedding.shape} = [{vocab_size}, {hidden_size}]")
    print(f"  Output layer: {self.output_layer.shape} = [{hidden_size}, {vocab_size}]")

  def forward(self, input_ids: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Forward pass
    input_ids: [batch_size, seq_len] token indices
    Returns: logits [batch_size, seq_len, vocab_size], hidden_states [batch_size, seq_len, hidden_size ]
    """

    batch_size, seq_len = input_ids.shape

    # Embedding lookup [B,L] -> [B, L, H]
    hidden_states = self.embedding[input_ids] # index to embedding mtrix
    print(f"\nEmbedding lookup: {input_ids.shape} -> {hidden_states.shape}")
    print(f"  Sample embeddings: {hidden_states[0, 0, :5]}...")  # First 5 dims of first token

    # output projection [B, L, H] @ [H, V] = [B, L , V]
    hidden_flat = hidden_states.reshape(-1, self.hidden_size) # reshape for matmul [B*L, H]
    logits_flat = hidden_flat @ self.output_layer # [B*L, V] matmul
    logits = logits_flat.reshape(batch_size, seq_len, self.vocab_size) #[B, L, V]

    print(f"Output projection: {hidden_states.shape} @ {self.output_layer.shape} = {logits.shape}")
    print(f"  Logits range: [{logits.min():.3f}, {logits.max():.3f}]")

    return logits, hidden_states


  def compute_loss(self, logits: np.ndarray, targets:np.ndarray, mask: Optional[np.ndarray] = None) -> float:
    """
    Compute cross-entropy loss
    logits: [batch_size, seq_len, vocab_size] predicted scores
    targets: [batch_size, seq_len] true token indices
    mask: [batch_size, seq_len] binary mask for valid positions
    """
    batch_size, seq_len, vocab_size = logits.shape

    # softmax: exp(x) / sum(exp(x))
    logits_max = np.max(logits, axis=-1, keepdims=True )  # [B, L, 1] for numerical stability
    exp_logits = np.exp(logits - logits_max) # remove max over all to avoid overflow
    probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)  # [B, L, V] normalized probs

    # Extract probabilities of true tokens
    batch_indices = np.arange(batch_size)[:, None] # [B, 1] for indexing. The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1).
    seq_indices = np.arange(seq_len)[None, :] # The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1)
    target_probs = probs[batch_indices, seq_indices, targets] # [B, L] real probs

    # Neg Log likelihood: -log(P(correct_token))
    loss_matrix = -np.log(target_probs + 1e-10) # add small value to avoid log(0)

    if mask is not None:
      loss_matrix = loss_matrix * mask # zero out padded positions
      total_loss = np.sum(loss_matrix) / np.sum(mask) # avg over only valid tokens
    else:
      total_loss = np.mean(loss_matrix) # avg over all the tokens


    print(f"\nLoss computation:")
    print(f"  Target probs shape: {target_probs.shape}")
    print(f"  Sample target probs: {target_probs[0, :5]}")  # First 5 positions
    print(f"  Loss: {total_loss:.4f}")

    return total_loss


  def train_step(self, input_ids: np.ndarray, targets:np.ndarray) -> float:
    """Single training step with gradietn descent"""
    # Forward pass
    logits, hidden_states = self.forward(input_ids)
    loss = self.compute_loss(logits, targets)

    # simpified gradient update
    self.embedding    *= (1 - self.learning_rate * 0.01) # simple weight decay
    self.output_layer *= (1 - self.learning_rate * 0.01) # simple weight decay

    return loss

"""### Test vanilla instruction tuning"""

model = VanillaInstructionTuning(vocab_size=1000, hidden_size=64)

# Create dummy data
batch_size, seq_len = 2, 10
input_ids = np.random.randint(0, 1000, (batch_size, seq_len))  # Random token indices
targets = np.random.randint(0, 1000, (batch_size, seq_len))  # Target tokens (shifted in practice)

print(f"Input shape: {input_ids.shape}")
print(f"Target shape: {targets.shape}")

# Forward pass
logits, hidden = model.forward(input_ids)

# Compute loss
loss = model.compute_loss(logits, targets)

# Training step
print("\n" + "-"*30 + "\n")
print("Training step:")
train_loss = model.train_step(input_ids, targets)
print(f"Training loss: {train_loss:.4f}")

"""# PyTorch Instruction Tuning with HF"""

class InstructionTuningDataset(Dataset):
 """PyTorch dataset for instruction tuning with validation"""

 def __init__(self, examples:List[InstructionExample], tokenizer, max_length: int = 512 ):
   self.examples = examples
   self.tokenizer = tokenizer
   self.max_length = max_length

 def __len__(self):
   return len(self.examples )

 def __getitem__(self, idx):
   example = self.examples[idx]

   # Format as full text (instruction + response)
   text = example.format_full() # pydantic models method

   # tokenize with attn mask
   encoding = self.tokenizer(
       text,
       truncation=True, # trunc to max_length
       padding='max_length', # pad to max length
       max_length=self.max_length,
       return_tensors='pt'

   )

   encoding['labels'] = encoding['input_ids'].clone() # copy input_ids as labels

   # Mask padding tokens (100 is ignore index in Cross Entropy Loss)
   encoding['labels'][encoding['attention_mask'] == 0] = -100 # ignore padding in loss

   return {k:v.squeeze(0) for k,v in encoding.items()}

def setup_instruction_tuning_pytorch_enhanced():
 """Setup instruction tuning with PyTorch, HuggingFace, and PEFT for efficiency"""

 # inicialtize accelerator
 accelerator = Accelerator(
     mixed_precision = 'fp16' if torch.cuda.is_available() else None,
     gradient_accumulation_steps=4 # accum gradients for larger effective batch size
 )

 # load model with opt 8-bit quant
 model_name = 'gpt2'
 print(f"Loading model: {model_name}")

 # 8-bit quantization if gpu
 quantization_config = None
 if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:
   quantization_config = BitsAndBytesConfig(
       load_in_8bit = True,  # load in 8bit
       bnb_8bit_compute_dtype=torch.float16, # compute in fp16
       bnb_8bit_quant_type="nf8", # quantization type
       bnb_8bit_use_double_quant=True, # double quantization for more saved mem. Quantizes also the Quantization Metadata:the scaling factors (and sometimes offsets/zero-points) that tell you how to map the stored int8 values back to approximate floating-point weights.
   )
   print("Using 8-bit quantization for memory efficiency")

 # load tokenizer
 tokenizer = AutoTokenizer.from_pretrained(model_name)
 tokenizer.pad_token = tokenizer.eos_token # set padding tkn
 tokenizer.padding_side = 'right' # pad on the right for Causal LM

 # load model with opt quantization
 if quantization_config:
   model = AutoModelForCausalLM.from_pretrained(
       model_name,
       quantization_config=quantization_config,
       device_map="Auto"
   )

   # prepare model for k-bit training
   model = prepare_model_for_kbit_training(model) # `prepare_model_for_kbit_training` modifies a model's weights and layers to be compatible with low-bit (8-bit or 4-bit) training, enabling memory-efficient fine-tuning.

 else:
   model = AutoModelForCausalLM.from_pretrained(model_name)

 print(f"Model loaded: {model.config.n_layer} layers, {model.config.n_head} heads")
 print(f"Vocab size: {model.config.vocab_size}")
 print(f"Hidden size: {model.config.n_embd}")


 # Configure PEFT (LoRA) for parameter-efficient fine-tuning
 peft_config = LoraConfig(
     task_type=TaskType.CAUSAL_LM,
     r=16, # lora rank -lower
     lora_alpha = 32, # scaling param
     lora_dropout=0.1,
     bias='none', # not adapt biases
     target_modules=["c_attn", "c_proj"] # gpt2 attn layers to adapt
 )


 # Convert to PEFT model
 model = get_peft_model(model, peft_config) # wrap model with LoRA adapters
 model.print_trainable_parameters() # shows param efficiency gain

 # prepare dataset
 instruction_examples = create_instruction_dataset() # pydantic validation
 dataset = InstructionTuningDataset(instruction_examples, tokenizer, max_length=128)

 # create dataloader
 dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

 # seutp optimizer with 8bit adam if available
 if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:
   optimizer = bnb.optim.AdamW8bit(
       model.parameters(),
       lr=5e-5,
       betas=(0.9, 0.999), # adam betas variables
       weight_decay=0.01, # weight decay for regularization
   )
   print("Using 8-bit AdamW optimizer")

 else:
   optimizer = torch.optim.AdamW(
       model.parameters(),
       lr=5e-5,
       weight_decay=0.01  # weight decay for regularization
   )

 # setup lr scheduler
 # The scheduler will cyclically vary the learning rate with cosine decay, restarting every T_i iterations while gradually increasing the period and never letting the LR drop below eta_min
 # The learning rate will follow a cosine decay for 10 iterations, then restart for 20 iterations, then 40, doubling each cycle, while never dropping below 1e-6.
 scheduler = CosineAnnealingWarmRestarts(
     optimizer,
     T_0=10,  # Number of iterations for the first restart
     T_mult=2,  # Factor to increase T_i after a restart
     eta_min=1e-6,  # Minimum learning rate, never below it
 )


 # Prepare everything with accelerator
 model, optimizer, dataloader, scheduler = accelerator.prepare(
     model, optimizer, dataloader, scheduler
 )

 # Initialize evaluation metrics
 bleu_metric = evaluate.load("bleu")  # BLEU score for generation quality
 rouge_metric = evaluate.load("rouge")  # ROUGE score for summarization

 return model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric

# Setup and test enhanced training
model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric = setup_instruction_tuning_pytorch_enhanced()

# Test one training step with metrics
print("\n" + "-"*50)
print("Testing enhanced training step with metrics:")

model.train()  # Set to training mode

for batch_idx, batch in enumerate(dataloader):
 if batch_idx > 0:  # Just test one batch
   break

 # Move batch to accelerator device (handles multi-GPU automatically)
 # Note: accelerator.prepare already handles device placement

 print(f"Batch shapes:")
 print(f"  input_ids: {batch['input_ids'].shape}")
 print(f"  attention_mask: {batch['attention_mask'].shape}")
 print(f"  labels: {batch['labels'].shape}")

 # Forward pass with accelerator's context
 with accelerator.accumulate(model):  # Handle gradient accumulation
   outputs = model(**batch)  # Pass entire batch to model
   loss = outputs.loss  # Extract loss
   logits = outputs.logits  # Extract logits

   print(f"\nForward pass:")
   print(f"  Loss: {loss.item():.4f}")
   print(f"  Logits shape: {logits.shape}")
   print(f"  Perplexity: {torch.exp(loss).item():.2f}")

   # Backward pass with accelerator (handles mixed precision)
   accelerator.backward(loss)  # Compute gradients with automatic mixed precision

   # Gradient clipping for stability
   accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to prevent explosion

   # Gradient norm for monitoring
   total_norm = 0
   for p in model.parameters():
     if p.grad is not None:
       total_norm += p.grad.data.norm(2).item() ** 2  # L2 norm squared
   total_norm = total_norm ** 0.5  # Square root for final norm
   print(f"  Gradient norm: {total_norm:.4f}")

   # Update weights
   optimizer.step()  # Apply gradients
   scheduler.step()  # Update learning rate
   optimizer.zero_grad()  # Reset gradients

   # Log to wandb if enabled
   if USE_WANDB:
     wandb.log({
       "train/loss": loss.item(),
       "train/perplexity": torch.exp(loss).item(),
       "train/grad_norm": total_norm,
       "train/learning_rate": scheduler.get_last_lr()[0],
     })

# Generate sample output with beam search
print("\n" + "-"*50)
print("Testing generation with enhanced decoding:")

model.eval()  # Set to evaluation mode

test_instruction = "Translate to French: Hello world"
prompt = f"### Instruction:\n{test_instruction}\n\n### Response:\n"

# Tokenize prompt
inputs = tokenizer(prompt, return_tensors='pt')
# Move to accelerator device
inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}

# Generate with enhanced parameters
with torch.no_grad():
 outputs = model.generate(
   **inputs,
   max_new_tokens=20,  # Generate up to 20 new tokens
   temperature=0.8,  # Sampling temperature
   do_sample=True,  # Use sampling instead of greedy
   top_p=0.9,  # Nucleus sampling threshold
   top_k=50,  # Top-k sampling
   repetition_penalty=1.2,  # Penalize repetition
   pad_token_id=tokenizer.eos_token_id,
   num_beams=4,  # Beam search for better quality
   early_stopping=True,  # Stop when all beams reach EOS
 )

# Decode and evaluate
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Input: {prompt}")
print(f"Generated: {generated_text}")

# Compute BLEU score (example - normally done on validation set)
reference = ["Bonjour le monde"]  # Expected translation
prediction = generated_text.split("### Response:\n")[-1].strip()

bleu_score = bleu_metric.compute(predictions=[prediction], references=[[ref] for ref in reference])
print(f"\nGeneration metrics:")
print(f"  BLEU score: {bleu_score['bleu']:.4f}\n")

print("\nInsufficient Training: Your model saw only 5 examples for 1 step. Language models need thousands of examples and many epochs to learn new tasks.")
print("\nData Mismatch: The model hasn't learned the instruction format properly. GPT-2 wasn't trained on instruction-following data.")
print("\nRepetition Issue: The model is stuck in a loop due to poor conditioning on the prompt format.")

"""# Bradley-Terry Preference Model

1. HUMAN LABELS: (A > B), (C > D), (E > F)... [preference dataset]

                        ↓

2. INITIALIZE: Reward model r_θ with random weights θ₀
                        ↓

3. FORWARD PASS: r_θ(prompt, A) → score_A, r_θ(prompt, B) → score_B

                        ↓

4. COMPUTE PROBABILITY: P(A > B) = σ(score_A - score_B)

                        ↓

5. MEASURE ERROR: Loss = -log P(human_choice)

                        ↓
                      
6. BACKPROP: ∇Loss flows through network to compute ∇θ

                        ↓

7. UPDATE: θ_new = θ_old - learning_rate × ∇θ

                        ↓

8. REPEAT: Go to step 3 with next batch until convergence

                        ↓

9. RESULT: Trained r_θ that assigns higher scores to human-preferred outputs
"""

class BradleyTerryModel:
  """
  Bradley-Terry model for preference probabilities
  P(o_i > o_j | x) = σ(z_i - zz_j) where z_i, z_j are scores
  """

  def __init__(self):
    print("Bradley-Terry Preference Model")
    print("Mathematical formulation: P(o_i ≻ o_j|x) = 1 / (1 + exp(-(z_i - z_j)))")
    print("Where z_i, z_j are scalar rewards/scores for outputs o_i, o_j\n")

  def compute_preference_prob(self, score_i: float, score_j: float) -> float:
    """
    Compute probability that option i is preferred over option j
    Uses sigmoid function on the difference of both scores
    Useful for data pair-decomposed like in preference optimization alignment
    """

    score_diff = score_i-score_j
    print(f"Score difference: z_i - z_j = {score_i:.3f} - {score_j:.3f} = {score_diff:.3f}")


    # logistic sigmoid σ(x) = 1/(1+exp(-z))
    prob = 1.0 / (1.0 + np.exp(-score_diff))
    print(f"P(o_i ≻ o_j) = σ({score_diff:.3f}) = {prob:.4f}")


    # interpretation
    if prob > 0.5:
      print(f"Option i is preferred (p={prob:.2%})")

    elif prob < 0.5:
      print(f"Option j is preferred (p={(1-prob):.2%})")

    else:
      print("No preference (p=50%)")

    return prob

  def compute_log_likelihood(self, scores_chosen:np.ndarray, scores_rejected:np.ndarray):
    """
    Compute log likelihood of preference
    scores_chosen: [n_samples] scores for chosen responses
    scores_rejected: [n_samples] scores for rejected responses
    The loss function uses this probability to compute gradients that push scores further apart. Simple argmax would give binary 1/0 feedback with no gradient signal for how much to adjust scores.
    """

    n_samples = len(scores_chosen)

    # compute preference probs for all pairs
    score_diffs = scores_chosen - scores_rejected # [n_samples] differences
    print(f"\nscore_diffs: {score_diffs} \n")

    probs = 1.0 / (1.0 + np.exp(-score_diffs))    # [n_samples] sigmoid of diffs
    print(f"\nprobs: {probs} \n")

    # log likelihood, sum of log probs
    log_likelihood = np.sum(np.log(probs + 1e-10)) # Add epsilon to prevent log(0)
    print(f"\nlog_likelihood: {log_likelihood} \n")


    print(f"Score differences: {score_diffs[:3]}...")  # First 3 differences
    print(f"Preference probs: {probs[:3]}...")  # First 3 probabilities
    print(f"Total log likelihood: {log_likelihood:.4f}")
    print(f"Average log likelihood: {log_likelihood/n_samples:.4f}")

    return log_likelihood



  def visualize_bradley_terry(self):
    """Visualize the Bradley-Terry preference function"""
    score_diffs = np.linspace(-5, 5, 100)  # Range of score differences
    probs = 1.0 / (1.0 + np.exp(-score_diffs))  # Preference probabilities

    plt.figure(figsize=(10, 6))
    plt.subplot(1, 2, 1)
    plt.plot(score_diffs, probs, 'b-', linewidth=2)
    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)  # 50% line
    plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)  # Equal scores line
    plt.xlabel('Score Difference (z_i - z_j)')
    plt.ylabel('P(o_i ≻ o_j)')
    plt.title('Bradley-Terry Preference Function')
    plt.grid(True, alpha=0.3)

    # Add annotations for key points
    plt.annotate('No preference', xy=(0, 0.5), xytext=(-2, 0.3),
                arrowprops=dict(arrowstyle='->', color='red', alpha=0.5))
    plt.annotate('Strong preference for i', xy=(3, 0.95), xytext=(1, 0.8),
                arrowprops=dict(arrowstyle='->', color='green', alpha=0.5))

    # Subplot 2: Effect of score magnitude
    plt.subplot(1, 2, 2)
    scores_i = np.array([0, 1, 2, 3, 4])  # Different score values for option i
    scores_j = 1  # Fixed score for option j
    probs = 1.0 / (1.0 + np.exp(-(scores_i - scores_j)))

    plt.bar(range(len(scores_i)), probs, color=['red' if p < 0.5 else 'green' for p in probs])
    plt.xlabel('Score of option i (score_j = 1)')
    plt.ylabel('P(o_i ≻ o_j)')
    plt.title('Preference vs Absolute Scores')
    plt.xticks(range(len(scores_i)), scores_i)
    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)
    plt.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.show()

"""# Bradley-Terry Preference Model Training

**1. HUMAN LABELS**: Collect preference dataset (A > B), (C > D), (E > F)...
   - *Provides ground truth supervision signal for training*
<br>

**2. INITIALIZE**: Reward model r_θ with random weights θ₀
   - *Neural network that will learn to score outputs*
   - θ is the set of all the learnable params on it
<br>


**3. FORWARD PASS**: r_θ(prompt, A) → score_A, r_θ(prompt, B) → score_B
   - *Model evaluates each output, producing scalar scores*
<br>


**4. COMPUTE PROBABILITY**: P(A > B) = σ(score_A - score_B)
   - *Converts score difference into preference probability*
   - The order is crucial, its the ground truth preference: σ(score_A - score_B) gives P(A > B) while σ(score_B - score_A) gives P(B > A)
   - Reversing the subtraction flips the meaning, so mismatching this during training would teach the model to prefer the wrong outputs.
<br>


**5. MEASURE ERROR**: Loss = -log P(human_choice)
   - *Quantifies disagreement between model and human judgment*
<br>


**6. BACKPROP**: ∇Loss flows through network to compute ∇θ
   - *Calculates how to adjust each parameter to reduce error*
<br>


**7. UPDATE**: θ_new = θ_old - learning_rate × ∇θ
   - *Adjusts millions of parameters to better match human preferences*
<br>


**8. REPEAT**: Return to step 3 with next batch until convergence
   - *Iteratively improves model through multiple passes over data*
<br>


**9. RESULT**: Trained r_θ that assigns higher scores to human-preferred outputs
   - *Final model generalizes to evaluate novel outputs*
<br>


The reward model learns scoring patterns from human judgments, while humans provide the essential preference labels that define what constitutes "better" outputs - without human preferences, the model would have no training signal.
"""

bt_model = BradleyTerryModel()

# Test single preference probability
print("\nExample 1: Similar scores")
bt_model.compute_preference_prob(score_i=2.1, score_j=2.0)

print("\nExample 2: Large difference")
bt_model.compute_preference_prob(score_i=5.0, score_j=1.0)

print("\nExample 3: Negative scores")
bt_model.compute_preference_prob(score_i=-1.0, score_j=-3.0)

# Test log likelihood computation
print("\n" + "-"*50)
print("Computing log likelihood for preference dataset:")
n_samples = 5
scores_chosen = np.array([3.0, 2.5, 4.0, 1.5, 3.5])  # Scores for chosen options
scores_rejected = np.array([1.0, 2.0, 1.0, 0.5, 3.0])  # Scores for rejected options

log_likelihood = bt_model.compute_log_likelihood(scores_chosen, scores_rejected)

# Visualize
bt_model.visualize_bradley_terry()

"""#Vanilla Reward Model Training with Numpy"""

class VanillaRewardModel:
  """
  Simple reward model that learns to score outputs based on preferences
  Uses Bradley-Terry formulation with gradient descent
  """

  def __init__(self, input_dim: int, hidden_dim: int=128):
    self.input_dim = input_dim  # input features dim
    self.hidden_dim = hidden_dim # Hidden layer dim

    # init weigths with small non-zero random vals
    # first layer
    self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01 # first layer weights  [D, H]
    self.b1 = np.zeros(hidden_dim) # first layer bias [H]

    # output layer
    self.W2 = np.random.randn(hidden_dim,1) * 0.01 # output layer weights  [H,1]
    self.b2 = np.zeros(1) # output layer bias [1]

    print(f"Initialized Reward Model:")
    print(f"  Input dim: {input_dim}")
    print(f"  Hidden dim: {hidden_dim}")
    print(f"  Parameters: {(input_dim * hidden_dim + hidden_dim + hidden_dim + 1):,}")

  def forward(self, x:np.ndarray) -> Tuple[float, Dict[str, np.ndarray]]:
    """
    Forward pass to compute reward score
    x: [input_dim] or [batch_size, input_dim] input features
    Returns: scalar reward and intermediate activations
    """

    # Ensure 2d input
    if x.ndim == 1:
      print("\n1-dimensional input. Reshaping:\n")
      print("Before: ", x, "\n")
      x = x.reshape(1,-1) # [D] -> [1,D]
      print("After: ", x, "\n")

    # Layer 1: Linear + ReLU activation
    z1 = x @ self.W1 + self.b1 # [B,D] @ [D,H] + [H] = [B,H]
    h1 = np.maximum(0,z1) # ReLU activation: max(0,x)

    # Layer 2: Linear (no activation, regression)
    z2 = h1 @ self.W2 + self.b2 # [B,H] @ [H,1] + [H] = [B,H]
    reward = z2.squeeze() # remove singleton dim

    # store activations for backward pass
    activations = {
        'x': x,
        'z1':z1,
        'h1':h1,
        'z2':z2,
        'reward':reward
    }

    return reward, activations

  def compute_bradley_terry_loss(self, x_chosen: np.ndarray, x_rejected: np.ndarray) -> Tuple[float, Dict]:
    """
    Compute loss using Bradley-Terry model
    L = -log(σ(r(x_chosen) - r(x_rejected)))
    """
    # Forward pass for both options
    r_chosen, act_chosen = self.forward(x_chosen)  # Reward and activations for chosen option
    r_rejected, act_rejected = self.forward(x_rejected)  # Reward and activations for rejected option

    # Compute preference probability with Bradley-Terry
    score_diff = r_chosen - r_rejected
    prob = 1.0 / (1.0 + np.exp(-score_diff)) # sigmoid of diff of scores made by the reward model weithgs

    # Negative log likelihood loss
    loss = -np.log(prob + 1e-10)  # Add epsilon for numerical stability

    print(f"\nBradley-Terry Loss Computation:")
    print(f"  r(chosen) = {r_chosen:.3f}")
    print(f"  r(rejected) = {r_rejected:.3f}")
    print(f"  Score diff = {score_diff:.3f}")
    print(f"  P(chosen > rejected) = {prob:.4f}")
    print(f"  Loss = -log(p) = {loss:.4f}")

    return loss, {'act_chosen': act_chosen, 'act_rejected': act_rejected, 'prob': prob}

  def backward(self, loss_info: Dict, learning_rate: float=0.01):
    """
    Backward pass using chain rule:

    1. Divide problem into two parts:
    - Gradient for the chosen example: ∂L/∂r_chosen = -(1-prob)
    - Gradient for the rejected example: ∂L/∂r_rejected = +(1-prob)


    2. Chain Rule:
    - For the chosen:
      grad_W2_chosen = grad_score_diff * h1.T        # ∂L/∂W2 = ∂L/∂r_chosen × ∂r_chosen/∂W2
      grad_h1 = grad_score_diff * self.W2.T          # ∂L/∂h1 = ∂L/∂r_chosen × ∂r_chosen/∂h1
      grad_z1 = grad_h1 * (z1 > 0)                   # ∂L/∂z1 = ∂L/∂h1 × ∂h1/∂z1 (ReLU derivative)
      grad_W1_chosen = x.T @ grad_z1                 # ∂L/∂W1 = ∂L/∂z1 × ∂z1/∂W1

    - For the rejected:
      # Same computation but with -grad_score_diff (opposite sign)
      grad_W2_rejected = grad_score_diff_rejected * h1_rejected.T # [H, 1]   ∂L/∂W2_rejected = ∂L/∂r_rejected × ∂r_rejected/∂W2
      grad_h1_rejected = grad_score_diff_rejected * self.W2.T     # [1, H]   ∂L/∂h1_rejected = ∂L/∂r_rejected × ∂r_rejected/∂h1
      grad_z1_rejected = grad_h1_rejected * (z1_rejected > 0)     # [1, H]  ∂h1/∂z1 = 1 if z1 > 0, else 0 (ReLU derivative)
      grad_W1_rejected = x_rejected.T @ grad_z1_rejected          # [D, H]  ∂L/∂W1_rejected = ∂L/∂z1_rejected × ∂z1_rejected/∂W1



    3. Combine gradients and update:
      self.W2 -= learning_rate * (grad_W2_chosen + grad_W2_rejected)
      self.W1 -= learning_rate * (grad_W1_chosen + grad_W1_rejected)

    """

    prob = loss_info['prob']
    act_chosen = loss_info['act_chosen']
    act_rejected = loss_info['act_rejected']

    # Gradient of loss w.r.t score difference
    grad_score_diff = -(1 - prob)  # ∂L/∂(r_chosen - r_rejected)

    print(f"\nGradient computation:")
    print(f"  dL/d(score_diff) = {grad_score_diff:.4f}")
    print(f"  Current weights (sample): W1[0,0] = {self.W1[0,0]:.6f}")

    # Compute gradients for chosen example (positive contribution)
    grad_W2_chosen, grad_W1_chosen = self._compute_gradients(act_chosen, grad_score_diff)

    # Compute gradients for rejected example (negative contribution)
    grad_W2_rejected, grad_W1_rejected = self._compute_gradients(act_rejected, -grad_score_diff)

    # Update weights using combined gradients
    self.W2 -= learning_rate * (grad_W2_chosen + grad_W2_rejected)
    self.W1 -= learning_rate * (grad_W1_chosen + grad_W1_rejected)

    print(f"  Updated weights (sample): W1[0,0] = {self.W1[0,0]:.6f}\n")

  def _compute_gradients(self, activations: Dict, grad_reward: float):
    """Compute gradients for one example using chain rule"""
    x = activations['x']
    h1 = activations['h1']
    z1 = activations['z1']

    # Gradient w.r.t W2: ∂L/∂W2 = ∂L/∂reward × ∂reward/∂W2
    grad_W2 = grad_reward * h1.T  # [H, 1]

    # Gradient w.r.t h1: ∂L/∂h1 = ∂L/∂reward × ∂reward/∂h1
    grad_h1 = grad_reward * self.W2.T  # [1, H]

    # Gradient through ReLU: ∂h1/∂z1 = 1 if z1 > 0, else 0
    grad_z1 = grad_h1 * (z1 > 0)  # [1, H]

    # Gradient w.r.t W1: ∂L/∂W1 = ∂L/∂z1 × ∂z1/∂W1
    grad_W1 = x.T @ grad_z1  # [D, H]

    return grad_W2, grad_W1

  def train_on_preferences(self, preference_pairs: List[Tuple[np.ndarray, np.ndarray]],
                          epochs: int = 10, learning_rate: float = 0.01):
    """Train reward model on preference data"""
    print(f"\nTraining Reward Model on {len(preference_pairs)} preference pairs")
    print(f"Epochs: {epochs}, Learning rate: {learning_rate}")

    losses = []

    for epoch in range(epochs):

      epoch_loss = 0
      for x_chosen, x_rejected in preference_pairs:
        loss, loss_info  = self.compute_bradley_terry_loss(x_chosen, x_rejected)
        self.backward(loss_info, learning_rate)
        epoch_loss += loss

      epoch_loss_averaged = epoch_loss/len(preference_pairs)
      losses.append(epoch_loss_averaged )

      if epoch % 2 == 0:
          print(f"Epoch {epoch}: Averaged loss = {epoch_loss_averaged:.4f}")

    return losses

"""## Test vanilla reward model"""

# Create reward model
reward_model = VanillaRewardModel(input_dim=10, hidden_dim=32)

# Create synthetic preference data
n_pairs = 20
input_dim = 10
preference_pairs = []

for _ in range(n_pairs):
  # Generate two random feature vectors
  x1 = np.random.randn(input_dim)  # Option 1 features
  x2 = np.random.randn(input_dim)  # Option 2 features

  # Simulate preference: prefer option with larger norm (arbitrary choice)
  if np.linalg.norm(x1) > np.linalg.norm(x2):
    preference_pairs.append((x1, x2))  # x1 is chosen
  else:
    preference_pairs.append((x2, x1))  # x2 is chosen

print(f"Created {len(preference_pairs)} synthetic preference pairs")

# Test single forward pass
test_input = np.random.randn(input_dim)
reward, activations = reward_model.forward(test_input)
print(f"\nTest forward pass:")
print(f"  Input shape: {test_input.shape}")
print(f"  Output reward: {reward:.4f}")

# Test loss computation
x_chosen, x_rejected = preference_pairs[0]
loss, loss_info = reward_model.compute_bradley_terry_loss(x_chosen, x_rejected)

# Train the model
print("\n" + "-"*50)
losses = reward_model.train_on_preferences(preference_pairs[:10], epochs=10, learning_rate=0.1)

# Plot training curve
plt.figure(figsize=(8, 4))
plt.plot(losses, 'b-', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Average Bradley-Terry Loss')
plt.title('Reward Model Training')
plt.grid(True, alpha=0.3)
plt.show()

# Test learned preferences
print("\n" + "-"*50)
print("Testing learned reward model:")
test_chosen, test_rejected = preference_pairs[15]  # Use unseen pair
r_chosen, _ = reward_model.forward(test_chosen)
r_rejected, _ = reward_model.forward(test_rejected)
print(f"Reward(chosen): {r_chosen:.4f}")
print(f"Reward(rejected): {r_rejected:.4f}")
print(f"Correctly ranked: {r_chosen > r_rejected}")

"""# Vanilla Direct Preference Optimization (DPO)"""

class VanillaDPO:
  """
  Direct preference optimization without wxplcit reward model
  Optimize preferences directly using the same policy
  """

  def __init__(self, vocab_size: int, hidden_size: int, beta:float=0.1):
    self.vocab_size = vocab_size
    self.hidden_size = hidden_size
    self.beta = beta # KL penalty coefficient (controls divergence from reference policy (sft model before dpo))

    # initialize policy params (simplified LM)
    self.embedding = np.random.randn(vocab_size, hidden_size) * 0.01
    self.output_layer = np.random.randn(hidden_size, vocab_size) * 0.01 # unembedding

    # store reference policy (frozen copy of initial weights)
    self.ref_embedding = self.embedding.copy()    # reference embedding
    self.ref_output    = self.output_layer.copy() # reference output layer

    print(f"Initialized DPO Model:")
    print(f"  Vocab size: {vocab_size}")
    print(f"  Hidden size: {hidden_size}")
    print(f"  Beta (KL coefficient): {beta}")
    print(f"  DPO Loss: L = -log(σ(β·log(π_θ(y_w|x)/π_ref(y_w|x)) - β·log(π_θ(y_l|x)/π_ref(y_l|x))))")


  def get_log_probs(self, tokens: np.ndarray, use_reference:bool = False) -> np.ndarray:
    """
    Compute log probs of tokens under the policy
    tokens: [seq_len] token indices
    use_reference: whether to use reference policy or current policy
    """

    # select which policy to use
    if use_reference:               # reference/original policy
      embedding = self.ref_embedding
      output = self.ref_output
    else:                           # policy to optimize
      embedding = self.embedding
      output = self.output_layer

    # simple forward pass (no attn for simplicity)
    seq_len = len(tokens)
    log_probs = np.zeros(seq_len)


    for t in range(seq_len):
      # get embedding
      if t == 0: # first token
        hidden = np.mean(embedding, axis=0) # [H] use as ctxt the average embedding over vocab for first token
      else:
        # use prev token as ctxt
        hidden = embedding[tokens[t-1]]

      # compute logits for all tkns
      logits = hidden @ output # Unembed [H] @ [H, V] = [V]

      # compute log prob of actual token
      log_probs_t = logits - np.log(np.sum(np.exp(logits))) # log softmax
      print(f"\nlog_probs_t: {log_probs_t[:5]}\n")

      log_probs[t] = log_probs_t[tokens[t]]
      print(f"\nlog_probs[t]: {log_probs[t]}\n")

    return log_probs


  def compute_dpo_loss(self, prompt: np.ndarray, chosen: np.ndarray, rejected:np.ndarray) -> Tuple[float, Dict] :
    """
    Compute DPO loss for a preference pair
    DPO optimizes: P(chosen > rejected) using policy ratios
    """

    # get log probs under current policy
    log_prob_chosen = np.sum(self.get_log_probs(chosen, use_reference=False))
    log_prob_rejected = np.sum(self.get_log_probs(rejected, use_reference=False))

    # get log probs under reference policy
    ref_log_prob_chosen = np.sum(self.get_log_probs(chosen, use_reference=True))
    ref_log_prob_rejected = np.sum(self.get_log_probs(rejected, use_reference=True))

    # Compute log ratios (the key DPO transformation)
    log_ratio_chosen = log_prob_chosen - ref_log_prob_chosen # log(π_θ/π_ref) for chosen
    print(f"\nlog_ratio_chosen: {log_ratio_chosen}\n")

    log_ratio_rejected = log_prob_rejected - ref_log_prob_rejected # log(π_θ/π_ref) for rejected
    print(f"\nlog_ratio_rejected: {log_ratio_rejected}\n")


    # DPO loss: -log(σ(β(log_ratio_chosen - log_ratio_rejected)))
    # This implicitly optimizes the Bradley-Terry objective without reward model

    logit_diff = self.beta * (log_ratio_chosen - log_ratio_rejected)  # Scaled difference
    print(f"\nlogit_diff:\n {logit_diff}\n")

    loss = -np.log(1.0 / (1.0 + np.exp(-logit_diff)) + 1e-10 ) # Negative log sigmoid
    print(f"\nloss: {loss}\n")


    print(f"\n\nDPO Loss Computation:")
    print(f"  π_θ(chosen): {log_prob_chosen:.3f}, π_ref(chosen): {ref_log_prob_chosen:.3f}")
    print(f"  π_θ(rejected): {log_prob_rejected:.3f}, π_ref(rejected): {ref_log_prob_rejected:.3f}")
    print(f"  Log ratio (chosen): {log_ratio_chosen:.3f}")
    print(f"  Log ratio (rejected): {log_ratio_rejected:.3f}")
    print(f"  Preference logit: β(r_w - r_l) = {logit_diff:.3f}")
    print(f"  Implicit preference prob: {1.0/(1.0 + np.exp(-logit_diff)):.4f}")
    print(f"  DPO Loss: {loss:.4f}")


    # Compute Kullback–Leibler divergence for monitoring. Taking the average over the chosen and rejected outputs it’s not a full KL calculation over the whole distribution, but a simple approximation to track during training.
    # On average, how far is the new policy drifting away from the reference policy when generating chosen vs. rejected samples
    kl_chosen = log_ratio_chosen # KL term for chosen
    kl_rejected = log_ratio_rejected # KL term for rejected

    avg_kl = (kl_chosen + kl_rejected) / 2  # Average KL
    print(f"  KL divergence (avg): {avg_kl:.4f}\n\n")



    return loss, {
      'log_ratio_chosen': log_ratio_chosen,
      'log_ratio_rejected': log_ratio_rejected,
      'implicit_reward_chosen': log_ratio_chosen / self.beta,  # Implicit reward. In Direct Preference Optimization (DPO), there’s no explicit reward model. Instead, the difference between the policy and the reference log-probabilities serves as a proxy for reward.
      'implicit_reward_rejected': log_ratio_rejected / self.beta,  # Implicit reward. It doesn’t come from a trained reward model (like in RLHF). It’s derived directly from how much more (or less) likely the policy makes an output compared to the reference policy.
    }

  def update_weights(self, loss_info: Dict, learning_rate: float = 0.01):
    """Simplified weight update"""

    # Get implicit rewards
    r_chosen   = loss_info['implicit_reward_chosen']
    r_rejected = loss_info['implicit_reward_rejected']

    # Simple heuristic update
    update_magnitude = learning_rate * (r_chosen - r_rejected)

    # small perturbation to weights
    self.embedding    += np.random.randn(*self.embedding.shape)    * update_magnitude * 0.001
    self.output_layer += np.random.randn(*self.output_layer.shape) * update_magnitude * 0.001

    print(f"\nWeight update:")
    print(f"  Update magnitude: {update_magnitude:.6f}")
    print(f"  Embedding change norm: {np.linalg.norm(self.embedding - self.ref_embedding):.6f}")

  def visualize_dpo_objective(self):
    """Visualize the DPO objective function"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Plot 1: Implicit reward vs log ratio
    log_ratios = np.linspace(-2, 2, 100)
    implicit_rewards = log_ratios / self.beta  # r(x,y) = log(π/π_ref)/β

    axes[0].plot(log_ratios, implicit_rewards, 'b-', linewidth=2)
    axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)
    axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)
    axes[0].set_xlabel('Log Ratio: log(π_θ/π_ref)')
    axes[0].set_ylabel('Implicit Reward: r(x,y)')
    axes[0].set_title(f'DPO Implicit Reward (β={self.beta})')
    axes[0].grid(True, alpha=0.3)

    # Plot 2: Preference probability vs reward difference
    reward_diffs = np.linspace(-3, 3, 100)
    preference_probs = 1.0 / (1.0 + np.exp(-self.beta * reward_diffs))

    axes[1].plot(reward_diffs, preference_probs, 'g-', linewidth=2)
    axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
    axes[1].axvline(x=0, color='r', linestyle='--', alpha=0.5)
    axes[1].set_xlabel('Reward Difference: r_chosen - r_rejected')
    axes[1].set_ylabel('P(chosen > rejected)')
    axes[1].set_title('DPO Preference Function')
    axes[1].grid(True, alpha=0.3)

    # Plot 3: Loss landscape
    log_ratio_diffs = np.linspace(-3, 3, 100)
    losses = -np.log(1.0 / (1.0 + np.exp(-self.beta * log_ratio_diffs)) + 1e-10)

    axes[2].plot(log_ratio_diffs, losses, 'r-', linewidth=2)
    axes[2].set_xlabel('Log Ratio Difference')
    axes[2].set_ylabel('DPO Loss')
    axes[2].set_title('DPO Loss Function')
    axes[2].grid(True, alpha=0.3)

    # Add annotations
    axes[2].annotate('High loss when\nrejected > chosen',
                    xy=(-2, losses[20]), xytext=(-1.5, 3),
                    arrowprops=dict(arrowstyle='->', color='red', alpha=0.5))

    plt.tight_layout()
    plt.show()

"""## Test DPO"""

dpo_model = VanillaDPO(vocab_size=100, hidden_size=32, beta=0.1)

# Create simple preference examples (token sequences)
prompt = np.array([1, 2, 3])  # Simple prompt
chosen = np.array([1, 2, 3, 4, 5, 6])  # Chosen continuation
rejected = np.array([1, 2, 3, 7, 8, 9])  # Rejected continuation

print(f"Prompt tokens: {prompt}")
print(f"Chosen tokens: {chosen}")
print(f"Rejected tokens: {rejected}")

# Compute DPO loss
loss, loss_info = dpo_model.compute_dpo_loss(prompt, chosen, rejected)

# Update weights
dpo_model.update_weights(loss_info, learning_rate=0.1)

# Visualize DPO objective
dpo_model.visualize_dpo_objective()

# Train on multiple preference pairs
print("\n" + "="*50)
print("Training DPO on preference data:")
losses = []
for i in range(10):
  # Generate random preference pair
  chosen = np.random.randint(0, 100, size=np.random.randint(5, 10))
  rejected = np.random.randint(0, 100, size=np.random.randint(5, 10))

  loss, loss_info = dpo_model.compute_dpo_loss(prompt, chosen, rejected)
  dpo_model.update_weights(loss_info, learning_rate=0.05)
  losses.append(loss)

  if i % 3 == 0:
    print(f"Step {i}: Loss = {loss:.4f}")

plt.figure(figsize=(8, 4))
plt.plot(losses, 'b-', marker='o')
plt.xlabel('Training Step')
plt.ylabel('DPO Loss')
plt.title('DPO Training Progress')
plt.grid(True, alpha=0.3)
plt.show()


print("both models with the same parameters and never let them diverge")
print("the reference model should be frozen")
print("update stepo of weights inneficient")

"""#Vanilla Chain-of-Thought Prompting Implementation"""

class ChainOfThoughtPrompting:
  """
  Implementation of chain-of-thought prompting for improved reasoning
  Include reasoning steps in the prompt to elicit step-by-step thinking
  Results are mocked/simulated, not actually generated by a model
  """

  def __init__(self):

    print("Chain-of-Thought (CoT) Prompting")

    # Initialize evaluation metrics for CoT effectiveness
    if HF_AVAILABLE:
      self.exact_match = evaluate.load("exact_match")  # For answer accuracy
      self.bertscore = evaluate.load("bertscore")  # For semantic similarity



  def create_standard_prompt(self, problem: str, examples: List[Tuple[str, str]]) -> str:
    """Create standard few-shot prompt without reasoning"""

    prompt = ""

    # Add examples
    for question, answer in examples:
      prompt += f"Q: {question}\nA: {answer}\n\n"

    # Add test problem
    prompt += f"Q: {problem}\nA:"

    return prompt



  def create_cot_prompt(self, problem: str, examples: List[Tuple[str, str, str]]) -> str:
    """
    Create chain-of-thought prompt with reasoning steps
    examples: List of (question, reasoning, answer) tuples
    """

    prompt = ""

    # Add examples with reasoning
    for question, reasoning, answer in examples:
      prompt += f"Q: {question}\n"
      prompt += f"A: Let's think step by step.\n{reasoning}\n"
      prompt += f"Therefore, the answer is {answer}.\n\n"

    # Add test problem
    prompt += f"Q: {problem}\n"
    prompt += f"A: Let's think step by step.\n"

    return prompt


  def evaluate_cot_effectiveness(self, standard_output: str, cot_output: str, ground_truth: str) -> Dict[str, float] :
    """Evaluate the effectiveness of CoT prompting with metrics"""
    metrics = {}

    if HF_AVAILABLE:
      # Extract naively final answers (assuming format "answer is X")
      import re

      def extract_answer(text):
        # try to find pattern "answer is X"
        match = re.search(r'answer is ([^.]+)', text.lower())
        if match:
          return match.group(1).strip()

        # otherwise return last number found
        numbers = re.findall(r'\d+', text)
        return numbers[-1] if numbers else text.strip()

      standard_answer = extract_answer(standard_output)
      cot_answer = extract_answer(cot_output)

      # Exact match accuracy
      standard_correct = standard_answer == ground_truth
      cot_correct = cot_answer == ground_truth

      metrics['standard_accuracy'] = float(standard_correct)
      metrics['cot_accuracy']      = float(cot_correct)
      metrics['improvement'] = metrics['cot_accuracy'] - metrics['standard_accuracy']



      # compute BERTscore for semantic similarity
      if self.bertscore:
        bert_results = self.bertscore.compute(
            predictions=[cot_answer],
            references=[ground_truth],
            lang="en"
        )
        metrics['cot_bertscore_f1'] = bert_results['f1'][0]

    return metrics



  def compare_prompting_strategies(self):
    """Compare standard vs CoT prompting on math problems"""

    # Example math word problem
    test_problem = "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?"
    ground_truth = "11"

    # Standard few-shot examples (question, answer)
    standard_examples = [
      ("There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?",
       "6"),
      ("If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?",
       "5"),
    ]

    # CoT examples with reasoning (question, reasoning, answer)
    cot_examples = [
      ("There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?",
       "There are 15 trees originally. After planting, there will be 21 trees. So the grove workers planted 21 - 15 = 6 trees.",
       "6"),
      ("If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?",
       "Originally there are 3 cars. 2 more cars arrive. So there are 3 + 2 = 5 cars.",
       "5"),
    ]

    # Create prompts
    standard_prompt = self.create_standard_prompt(test_problem, standard_examples)
    cot_prompt = self.create_cot_prompt(test_problem, cot_examples)

    print("STANDARD PROMPTING:")
    print("-"*50)
    print(standard_prompt)
    print("\nExpected completion: '11' (often incorrect without reasoning)")

    print("\n" + "="*50)
    print("CHAIN-OF-THOUGHT PROMPTING:")
    print("-"*50)
    print(cot_prompt)
    print("\nExpected completion: 'Roger started with 5 tennis balls. He bought 2 cans, each with 3 balls, so 2 × 3 = 6 new balls. Therefore, 5 + 6 = 11. Therefore, the answer is 11.'")

    # Mock and simulate outputs for metrics computation
    simulated_standard_output = "11"
    simulated_cot_output = "Roger started with 5 tennis balls. He bought 2 cans, each with 3 balls, so 2 × 3 = 6 new balls. Therefore, 5 + 6 = 11. Therefore, the answer is 11."

    # Evaluate effectiveness
    metrics = self.evaluate_cot_effectiveness(
      simulated_standard_output,
      simulated_cot_output,
      ground_truth
    )

    if metrics:
      print("\n" + "="*50)
      print("CoT Effectiveness Metrics:")
      for key, value in metrics.items():
        print(f"  {key}: {value:.2f}")

    return standard_prompt, cot_prompt

  def analyze_cot_benefits(self):
    """Analyze why CoT helps"""
    benefits = {
      "Decomposition": "Breaks complex problems into manageable steps",
      "Intermediate_Computation": "Performs calculations step-by-step",
      "Error_Detection": "Easier to spot mistakes in reasoning chain",
      "Interpretability": "Shows model's reasoning process",
      "Consistency": "Reduces variance in responses"
    }

    print("\n" + "="*50)
    print("Benefits of Chain-of-Thought Prompting:")
    for benefit, description in benefits.items():
      print(f"  • {benefit.replace('_', ' ')}: {description}")

    # Visualize CoT vs Standard with enhanced metrics
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # Performance comparison (hypothetical data based on research)
    tasks = ['Math\nWord\nProblems', 'Common\nSense', 'Symbolic\nReasoning', 'Multi-step\nReasoning']
    standard_performance = [35, 55, 25, 30]  # Hypothetical accuracy %
    cot_performance = [85, 75, 65, 80]  # With CoT

    x = np.arange(len(tasks))
    width = 0.35

    bars1 = ax1.bar(x - width/2, standard_performance, width, label='Standard', color='lightcoral', alpha=0.8)
    bars2 = ax1.bar(x + width/2, cot_performance, width, label='Chain-of-Thought', color='lightgreen', alpha=0.8)

    # Add value labels on bars
    for bars in [bars1, bars2]:
      for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.0f}%', ha='center', va='bottom', fontsize=9)

    ax1.set_xlabel('Task Type')
    ax1.set_ylabel('Accuracy (%)')
    ax1.set_title('Performance Comparison: Standard vs CoT')
    ax1.set_xticks(x)
    ax1.set_xticklabels(tasks)
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')

    # Reasoning chain visualization
    ax2.axis('off')
    ax2.set_title('Reasoning Chain Flow', fontsize=14, fontweight='bold')

    # Draw reasoning chain
    steps = [
      "Problem:\n5 balls + 2 cans × 3 balls",
      "Step 1:\nIdentify initial (5)",
      "Step 2:\nCalculate new (2×3=6)",
      "Step 3:\nCombine (5+6=11)",
      "Answer: 11"
    ]

    y_positions = [0.8, 0.6, 0.4, 0.2, 0.0]
    for i, (step, y) in enumerate(zip(steps, y_positions)):
      # Draw box
      box = plt.Rectangle((0.2, y-0.05), 0.6, 0.1,
                         fill=True, facecolor='lightblue' if i < len(steps)-1 else 'lightgreen',
                         edgecolor='black', linewidth=2)
      ax2.add_patch(box)

      # Add text
      ax2.text(0.5, y, step, ha='center', va='center', fontsize=10, fontweight='bold')

      # Draw arrow
      if i < len(steps) - 1:
        ax2.arrow(0.5, y-0.05, 0, -0.07, head_width=0.03, head_length=0.02, fc='black')

    ax2.set_xlim(0, 1)
    ax2.set_ylim(-0.1, 0.9)

    plt.tight_layout()
    plt.show()

"""## Mocked Test Chain-of-Thought

"""

cot = ChainOfThoughtPrompting()

# Compare prompting strategies
standard_prompt, cot_prompt = cot.compare_prompting_strategies()

# Analyze benefits
cot.analyze_cot_benefits()

# Additional CoT variations
print("\n" + "="*50)
print("Advanced Chain-of-Thought Variations:")
print("-"*50)

variations = {
  "Zero-shot CoT": "Simply add 'Let's think step by step' without examples",
  "Self-consistency": "Sample multiple reasoning paths and vote on answer",
  "Least-to-Most": "Decompose into subproblems, solve from simple to complex",
  "Tree-of-Thoughts": "Explore multiple reasoning branches like search tree",
  "Plan-and-Solve": "First create a plan, then execute step-by-step",
  "Program-of-Thoughts": "Generate code to solve the problem",
}

for name, description in variations.items():
  print(f"• {name}: {description}")

# Log final metrics to wandb
if USE_WANDB:
  wandb.log({
    "cot/standard_accuracy": 0.35,  # Example metrics
    "cot/enhanced_accuracy": 0.85,
    "cot/improvement": 0.50,
  })

