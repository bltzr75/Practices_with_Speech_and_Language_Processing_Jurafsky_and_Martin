{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d73b5ad73680422cbbd2f7571a6aceda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_692743ff8bbb4aa7bf3d03b4dee0a6ee",
              "IPY_MODEL_a9d5ab677aa648a8b31995b8bb8501eb",
              "IPY_MODEL_5531c51bf52f46cba3c566339ec6ce84"
            ],
            "layout": "IPY_MODEL_a5232553d98b45a2b5992f945d51baaf"
          }
        },
        "692743ff8bbb4aa7bf3d03b4dee0a6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7f4d1ff1454274b796473703d7e0fb",
            "placeholder": "​",
            "style": "IPY_MODEL_c3fa4cd303ee44818f7b93883d9a356a",
            "value": "Downloading builder script: "
          }
        },
        "a9d5ab677aa648a8b31995b8bb8501eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae52dafde74a436eaf53e6afb555e907",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e331eeaf9e84a2287404ba4e7bda576",
            "value": 1
          }
        },
        "5531c51bf52f46cba3c566339ec6ce84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea7052fd734047c48b774f7c1c7f2d57",
            "placeholder": "​",
            "style": "IPY_MODEL_38cb1d9c82a34354bd0fe110855a55cc",
            "value": " 5.94k/? [00:00&lt;00:00, 131kB/s]"
          }
        },
        "a5232553d98b45a2b5992f945d51baaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7f4d1ff1454274b796473703d7e0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fa4cd303ee44818f7b93883d9a356a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae52dafde74a436eaf53e6afb555e907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7e331eeaf9e84a2287404ba4e7bda576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea7052fd734047c48b774f7c1c7f2d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38cb1d9c82a34354bd0fe110855a55cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e619d48df6254d1dbf68d5b0443dee58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fb68ffeb4854a47b7f796627bddd393",
              "IPY_MODEL_c590c3b8f14049498c4f07484a44bc05",
              "IPY_MODEL_4b18c0efebbc4b1f823d23c199eff8dd"
            ],
            "layout": "IPY_MODEL_a0ffd521c9914b2da45e806c64b45ea6"
          }
        },
        "7fb68ffeb4854a47b7f796627bddd393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb2f564eb8c4699a426725ba10049bd",
            "placeholder": "​",
            "style": "IPY_MODEL_ce6839d820984280815ed40650fa5189",
            "value": "Downloading extra modules: "
          }
        },
        "c590c3b8f14049498c4f07484a44bc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb828bfd8dc4f018a9c5fe08e74805d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85bf903ace374efb85f8225d44370a1c",
            "value": 1
          }
        },
        "4b18c0efebbc4b1f823d23c199eff8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df006aed72b4dccbc013cce749b9006",
            "placeholder": "​",
            "style": "IPY_MODEL_8377605098af47f9855e571e669abaed",
            "value": " 3.34k/? [00:00&lt;00:00, 122kB/s]"
          }
        },
        "a0ffd521c9914b2da45e806c64b45ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb2f564eb8c4699a426725ba10049bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6839d820984280815ed40650fa5189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bb828bfd8dc4f018a9c5fe08e74805d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "85bf903ace374efb85f8225d44370a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6df006aed72b4dccbc013cce749b9006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8377605098af47f9855e571e669abaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e15793bf8a24417b9540e60cd5292a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8263fa11dec849729019aa3a5807ab27",
              "IPY_MODEL_e01f2ff24dcc42b6a8268f6e53fa4164",
              "IPY_MODEL_05a0991e8060423ab93f5bc2d30d8b77"
            ],
            "layout": "IPY_MODEL_91d5df0c204b4d43abd2fecdb8f4fdad"
          }
        },
        "8263fa11dec849729019aa3a5807ab27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eefbc2894d0b447b980d32b1292e6c58",
            "placeholder": "​",
            "style": "IPY_MODEL_4dba45861a7a40fbb31c0909f5565480",
            "value": "Downloading builder script: "
          }
        },
        "e01f2ff24dcc42b6a8268f6e53fa4164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cdac29b3160482d85fcd1cdf2bdbc26",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87e3cba652a141ceb32b60debe2aaa00",
            "value": 1
          }
        },
        "05a0991e8060423ab93f5bc2d30d8b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5ff55079db443eb208ad7aaece2478",
            "placeholder": "​",
            "style": "IPY_MODEL_f5e2ba634a694a7da535f9a2b2fe1a85",
            "value": " 6.27k/? [00:00&lt;00:00, 110kB/s]"
          }
        },
        "91d5df0c204b4d43abd2fecdb8f4fdad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eefbc2894d0b447b980d32b1292e6c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dba45861a7a40fbb31c0909f5565480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cdac29b3160482d85fcd1cdf2bdbc26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "87e3cba652a141ceb32b60debe2aaa00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd5ff55079db443eb208ad7aaece2478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5e2ba634a694a7da535f9a2b2fe1a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Setup"
      ],
      "metadata": {
        "id": "_r4BdmnTJWkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate # HF evaluate lib for metrics\n",
        "!pip install bitsandbytes # 8 bit Adam optimizer\n",
        "!pip install trl # Transformer Reinforcement Learning\n",
        "!pip install sacrebleu  # BLEU (Bilingual Evaluation Understudy)  metric dependency\n",
        "!pip install rouge_score  # BLEU (Bilingual Evaluation Understudy)  metric dependency\n",
        "# BLEU (Bilingual Evaluation Understudy) is a metric for evaluating machine-generated text quality, especially in translation and text generation tasks\n",
        "# BLEU evaluates text generation quality by counting matching n-grams (1-4 word sequences) between generated and reference text, applying a brevity penalty, and scoring 0-1 where 0.6+ is excellent, 0.4-0.6 is good, 0.2-0.4 is fair, and below 0.2 is poor. For example, comparing \"The cat sits on the mat\" to reference \"The cat is on the mat\" yields ~0.7 (good) based on overlapping words and phrases. It's widely used for machine translation, summarization, and instruction tuning evaluation, but has limitations since it favors literal matches over semantic meaning and can miss quality paraphrases that use different vocabulary."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_E5wkLfLOJQ",
        "outputId": "06f50eeb-a1ea-4fc1-ee8f-c58d2a6ff0ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.56.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl) (0.22.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "C3ip6ZQeHxL4",
        "outputId": "44b82c3d-a9a8-4102-a104-1536e11a6dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace libraries available with enhancements\n",
            "TRL (Transformer Reinforcement Learning) not available\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/perplexity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/grad_norm</td><td>0.30405</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>3.63862</td></tr><tr><td>train/perplexity</td><td>38.03923</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-sun-2</strong> at: <a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook/runs/wgf7yv7f' target=\"_blank\">https://wandb.ai/blitalan-freelancer/llm-alignment-textbook/runs/wgf7yv7f</a><br> View project at: <a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook' target=\"_blank\">https://wandb.ai/blitalan-freelancer/llm-alignment-textbook</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250906_123024-wgf7yv7f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250906_133500-59hp68oo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook/runs/59hp68oo' target=\"_blank\">gallant-blaze-3</a></strong> to <a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook' target=\"_blank\">https://wandb.ai/blitalan-freelancer/llm-alignment-textbook</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/blitalan-freelancer/llm-alignment-textbook/runs/59hp68oo' target=\"_blank\">https://wandb.ai/blitalan-freelancer/llm-alignment-textbook/runs/59hp68oo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import subprocess\n",
        "import sys\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict, ValidationError, field_serializer\n",
        "from typing import Optional\n",
        "\n",
        "import evaluate # HF evaluate lib for metrics\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "from huggingface_hub import login as hf_login\n",
        "hf_login(token=userdata.get('test_hf_1'))\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  from transformers import(\n",
        "      AutoModelForCausalLM, # pretrained models\n",
        "      AutoTokenizer,\n",
        "      AutoModelForSequenceClassification, # For reward models\n",
        "      Trainer,  # HF training loop\n",
        "      TrainingArguments,  # Training hyperparams\n",
        "      DataCollatorForLanguageModeling, # BAtch collation for LM trainning\n",
        "      BitsAndBytesConfig, # For 8bit quantization\n",
        "      )\n",
        "\n",
        "  from datasets import Dataset as HFDataset, load_dataset\n",
        "  from peft import(\n",
        "      LoraConfig,\n",
        "      get_peft_model, # convert model to peft model\n",
        "      TaskType, # task type enum\n",
        "      PeftModel, # peft model class\n",
        "      prepare_model_for_kbit_training # prepare quantized model for training\n",
        "  )\n",
        "\n",
        "  from accelerate import Accelerator # Distributed training support\n",
        "  from accelerate.utils import set_seed # seed setting for accelerate\n",
        "  import bitsandbytes as bnb # 8bit adam optimizer\n",
        "\n",
        "  HF_AVAILABLE = True\n",
        "  print(\"HuggingFace libraries available with enhancements\")\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  HF_AVAILABLE = False\n",
        "  print(\"HuggingFace libraries not available - install with: pip install transformers datasets peft accelerate bitsandbytes\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  from trl import (\n",
        "      SFTTrainer, # supervised fine tuning trainer\n",
        "      DPOTrainer, # direct preference optimization trainer\n",
        "      RewardTrainer, # reward model trainer\n",
        "      DataCollatorForCompletionOnlyLM # Collator for instruction tuning\n",
        "  )\n",
        "  TRL_AVAILABLE = True\n",
        "  print(\"TRL (Transformer Reinforcement Learning) available\")\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  TRL_AVAILABLE = False\n",
        "  print(\"TRL (Transformer Reinforcement Learning) not available\")\n",
        "\n",
        "\n",
        "\n",
        "# Init wandb for experiment tracking\n",
        "USE_WANDB = True #False  ## Change if prefering otherwise\n",
        "\n",
        "if USE_WANDB:\n",
        "  wandb.init(\n",
        "      project='llm-alignment-textbook',\n",
        "      config={ # Hyperparams\n",
        "               'achitecture': \"GPT-2\",\n",
        "               'dataset':\"custom-instructions\",\n",
        "               'learning_rate':5e-5,\n",
        "      },\n",
        "      # model='online' # offline for local logging instead  # returned error, to check\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(42) # CUDA seeds for GPU operations\n",
        "\n",
        "# Device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data structures for Instruction Tuning and Preferences using Pydantic\n",
        "\n",
        "Key changes from Pydantic v1 to v2:\n",
        "\n",
        "@validator -> @field_validator with @classmethod\n",
        "\n",
        "Config class -> model_config = ConfigDict(...)\n",
        "\n",
        "values parameter -> info parameter with info.data\n",
        "\n",
        ".dict() -> .model_dump()\n",
        "\n",
        ".json() -> .model_dump_json()\n",
        "\n",
        "New @field_serializer decorator for custom serialization\n",
        "\n",
        ".schema() -> .model_json_schema()\n",
        "\n",
        "Built-in str_strip_whitespace config option\n"
      ],
      "metadata": {
        "id": "ptTSWZce9fbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic V2 for validation\n",
        "class InstructionExample(BaseModel):\n",
        "  \"\"\"Single instruction-rsesponse pair for supervised fine-tuning with validation\"\"\"\n",
        "\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid', # Forbids extra fields not defined in the model\n",
        "      validate_assignment=True, # Validate on assignment\n",
        "      str_strip_whitespace=True, #Automatically stirp whitespace from strings\n",
        "  )\n",
        "\n",
        "  instruction: str = Field(..., min_length=1, description=\"The task instruction/prompt\")  # Required field with min length\n",
        "  input: Optional[str] = Field(default=\"\", description=\"Optional input context\") # Optional with default empty string\n",
        "  output: str = Field(..., min_length=1, description=\"Expected response/completion\")   # Required field\n",
        "\n",
        "  @field_validator('instruction', 'output') # fields to validate\n",
        "  @classmethod # Convert a function to a class method\n",
        "  def non_empty_string(cls, v:str) -> str: # class method for validation\n",
        "    \"\"\"Validate that instruction and output are non-empty after stripping\"\"\"\n",
        "    if not v.strip(): # check if empty after removing whitespaces\n",
        "      raise ValueError(\"Field cannot be empty or whitespace only\")\n",
        "    return v.strip()  # Strip value\n",
        "\n",
        "  def format_prompt(self) -> str:\n",
        "    \"\"\"Format as prompt for model input\"\"\"\n",
        "    if self.input: # if there is additional context\n",
        "      return f\"### Instruction:\\n{self.instruction}\\n\\n### Input:\\n{self.input}\\n\\n### Response:\\n\"\n",
        "    else: # just instruction otherwise\n",
        "      return f\"### Instruction:\\n{self.instruction}\\n\\n### Response:\\n:\"\n",
        "\n",
        "  def format_full(self) -> str:\n",
        "    \"\"\"Format complete example with response\"\"\"\n",
        "\n",
        "    return self.format_prompt() + self.output\n",
        "\n",
        "\n",
        "class PreferencePair(BaseModel):\n",
        "  \"\"\"Preference pair for alignment training with validation\"\"\"\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid',\n",
        "      validate_assignment=True, # Validate when fields are reassigned\n",
        "      str_strip_whitespace=True, #Auto-strip spaces\n",
        "  )\n",
        "\n",
        "  prompt: str = Field(..., min_length=1, description=\"Input prompt/instruction\") # required prompt\n",
        "  chosen: str = Field(..., min_length=1, description=\"Preferred response\") # Preferred completion\n",
        "  rejected: str=Field(..., min_length=1, description=\"Rejected response\")  # Rejected completion\n",
        "\n",
        "  @field_validator('prompt', 'chosen', 'rejected')\n",
        "  @classmethod\n",
        "  def clean_text(cls, v:str) -> str:\n",
        "    \"\"\"Clean and validate text fields\"\"\"\n",
        "    cleaned = v.strip() # remove leading/trailing whitespace\n",
        "    if not cleaned: # if empty\n",
        "      raise ValueError('Field cannot be empty')\n",
        "    return cleaned\n",
        "\n",
        "  @field_validator('rejected')\n",
        "  @classmethod\n",
        "  def different_from_chosen(cls, v: str, info) -> str:  # Pydantic v2 uses 'info' instead of 'values'\n",
        "    \"\"\"Ensure chosen and rejected are different\"\"\"\n",
        "    if 'chosen' in info.data and v==info.data['chosen']: # Access other fields via info.data\n",
        "      raise ValueError('Chosen and rejected responses must be different')\n",
        "    return v\n",
        "\n",
        "\n",
        "# Create sample instruction tuning data with validation\n",
        "def create_instruction_dataset() -> List[InstructionExample]:\n",
        "    \"\"\"Create diverse instruction examples covering multiple task types\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    # Using Pydantic v2 models with automatic validation\n",
        "    example_data = [\n",
        "        {\n",
        "            \"instruction\": \"Translate the following English text to French\",\n",
        "            \"input\": \"The quick brown fox jumps over the lazy dog\",\n",
        "            \"output\": \"Le renard brun rapide saute par-dessus le chien paresseux\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Summarize this text in one sentence\",\n",
        "            \"input\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing programs that can access data and use it to learn for themselves.\",\n",
        "            \"output\": \"Machine learning is an AI technique that allows systems to automatically learn and improve from data without explicit programming.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Answer the following question\",\n",
        "            \"input\": \"What is the capital of France?\",\n",
        "            \"output\": \"The capital of France is Paris.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Write a Python function to calculate factorial\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Classify the sentiment of this review as positive or negative\",\n",
        "            \"input\": \"This movie was absolutely terrible. I want my money back.\",\n",
        "            \"output\": \"negative\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    for data in example_data:\n",
        "        try:\n",
        "            example = InstructionExample(**data)  # Create with validation using Pydantic v2\n",
        "            examples.append(example)\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error: {e}\")  # Log validation errors with Pydantic v2 error format\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "# Create sample preference data with validation\n",
        "def create_preference_dataset() -> List[PreferencePair]:\n",
        "  \"\"\"Create preference pairs for alignment with validation\"\"\"\n",
        "  pairs = []\n",
        "\n",
        "  preference_data = [\n",
        "    {\n",
        "        \"prompt\": \"How do I make a bomb?\",\n",
        "        \"chosen\": \"I cannot and will not provide instructions for creating explosives or weapons. This could cause serious harm. If you're interested in chemistry, I'd be happy to discuss safe educational experiments instead.\",\n",
        "        \"rejected\": \"Here's how to make a bomb: First, you need to gather explosive materials...\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain quantum computing to a 5-year-old\",\n",
        "        \"chosen\": \"Imagine you have a magical coin that can be both heads AND tails at the same time until you look at it! Quantum computers use tiny particles that work like these magical coins to solve really hard puzzles super fast.\",\n",
        "        \"rejected\": \"Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to perform computations using qubits which exist in probabilistic states described by complex amplitudes in Hilbert space.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What's the best programming language?\",\n",
        "        \"chosen\": \"There's no single 'best' programming language - it depends on your goals. Python is great for beginners and data science, JavaScript for web development, C++ for systems programming, and each has its strengths for different use cases.\",\n",
        "        \"rejected\": \"Python is objectively the best programming language and everyone should use it exclusively.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "  for data in preference_data:\n",
        "    try:\n",
        "      pair = PreferencePair(**data)  # Create with automatic validation using Pydantic v2\n",
        "      pairs.append(pair)\n",
        "    except ValidationError as e:\n",
        "      print(f\"Validation error in preference pair: {e}\")  # Log any validation errors with v2 format\n",
        "\n",
        "  return pairs\n",
        "\n",
        "\n",
        "class EnhancedInstructionExample(InstructionExample):\n",
        "    \"\"\"Enhanced version showing Pydantic v2 features\"\"\"\n",
        "\n",
        "    @field_serializer('output')\n",
        "    def serialize_output(self, value: str) -> str:\n",
        "        \"\"\"Custom serialization for output field (Pydantic v2 feature)\"\"\"\n",
        "        return value[:100] + \"...\" if len(value) > 100 else value  # Truncate long outputs in serialization\n",
        "\n",
        "    def model_dump_json(self, **kwargs) -> str:  # v2 replacement for .json()\n",
        "        \"\"\"Serialize to JSON using Pydantic v2 method\"\"\"\n",
        "        return super().model_dump_json(indent=2, **kwargs)  # Use model_dump_json instead of json()\n",
        "\n",
        "# Visualize the data\n",
        "instruction_data = create_instruction_dataset()  # Create instruction examples with validation\n",
        "preference_data = create_preference_dataset()  # Create preference pairs with validation\n",
        "\n",
        "print(f\"Created {len(instruction_data)} instruction examples\")\n",
        "print(f\"Created {len(preference_data)} preference pairs\\n\")\n",
        "\n",
        "print(\"Sample Instruction Example:\")\n",
        "print(\"-\" * 50)\n",
        "example = instruction_data[0]\n",
        "print(f\"Instruction: {example.instruction}\")\n",
        "print(f\"Input: {example.input}\")\n",
        "print(f\"Output: {example.output}\")\n",
        "print(f\"\\nFormatted prompt:\\n{example.format_prompt()}\")\n",
        "\n",
        "# Demonstrate Pydantic v2 model_dump (replacement for dict())\n",
        "print(\"\\nPydantic v2 model_dump:\")\n",
        "print(example.model_dump(exclude={'input'}, mode='json'))  # v2 method with mode parameter\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Sample Preference Pair:\")\n",
        "print(\"-\" * 50)\n",
        "pref = preference_data[0]\n",
        "print(f\"Prompt: {pref.prompt}\")\n",
        "print(f\"Chosen: {pref.chosen[:100]}...\")\n",
        "print(f\"Rejected: {pref.rejected[:50]}...\")\n",
        "\n",
        "# Show Pydantic v2 JSON serialization\n",
        "print(\"\\nPydantic v2 JSON Schema:\")\n",
        "print(PreferencePair.model_json_schema())  # v2 method for getting JSON schema"
      ],
      "metadata": {
        "id": "yY7K9E9jQSIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c7d8a-8626-43b0-b34e-e3166a415557"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 5 instruction examples\n",
            "Created 3 preference pairs\n",
            "\n",
            "Sample Instruction Example:\n",
            "--------------------------------------------------\n",
            "Instruction: Translate the following English text to French\n",
            "Input: The quick brown fox jumps over the lazy dog\n",
            "Output: Le renard brun rapide saute par-dessus le chien paresseux\n",
            "\n",
            "Formatted prompt:\n",
            "### Instruction:\n",
            "Translate the following English text to French\n",
            "\n",
            "### Input:\n",
            "The quick brown fox jumps over the lazy dog\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "Pydantic v2 model_dump:\n",
            "{'instruction': 'Translate the following English text to French', 'output': 'Le renard brun rapide saute par-dessus le chien paresseux'}\n",
            "\n",
            "==================================================\n",
            "Sample Preference Pair:\n",
            "--------------------------------------------------\n",
            "Prompt: How do I make a bomb?\n",
            "Chosen: I cannot and will not provide instructions for creating explosives or weapons. This could cause seri...\n",
            "Rejected: Here's how to make a bomb: First, you need to gath...\n",
            "\n",
            "Pydantic v2 JSON Schema:\n",
            "{'additionalProperties': False, 'description': 'Preference pair for alignment training with validation', 'properties': {'prompt': {'description': 'Input prompt/instruction', 'minLength': 1, 'title': 'Prompt', 'type': 'string'}, 'chosen': {'description': 'Preferred response', 'minLength': 1, 'title': 'Chosen', 'type': 'string'}, 'rejected': {'description': 'Rejected response', 'minLength': 1, 'title': 'Rejected', 'type': 'string'}}, 'required': ['prompt', 'chosen', 'rejected'], 'title': 'PreferencePair', 'type': 'object'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Instruction Tuning Implementation"
      ],
      "metadata": {
        "id": "324itrnpG_ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaInstructionTuning:\n",
        "  \"\"\"Simple instruction tuning using standard language modeling loss\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size: int, hidden_size: int, learning_rate: float=1e-3):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # simple embedding and output layers\n",
        "    self.embedding = np.random.randn(vocab_size,hidden_size) *0.01\n",
        "    self.output_layer = np.random.randn(hidden_size, vocab_size )* 0.01\n",
        "\n",
        "    print(f\"Initialized instruction tuning model:\")\n",
        "    print(f\"  Embedding matrix: {self.embedding.shape} = [{vocab_size}, {hidden_size}]\")\n",
        "    print(f\"  Output layer: {self.output_layer.shape} = [{hidden_size}, {vocab_size}]\")\n",
        "\n",
        "  def forward(self, input_ids: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Forward pass\n",
        "    input_ids: [batch_size, seq_len] token indices\n",
        "    Returns: logits [batch_size, seq_len, vocab_size], hidden_states [batch_size, seq_len, hidden_size ]\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "\n",
        "    # Embedding lookup [B,L] -> [B, L, H]\n",
        "    hidden_states = self.embedding[input_ids] # index to embedding mtrix\n",
        "    print(f\"\\nEmbedding lookup: {input_ids.shape} -> {hidden_states.shape}\")\n",
        "    print(f\"  Sample embeddings: {hidden_states[0, 0, :5]}...\")  # First 5 dims of first token\n",
        "\n",
        "    # output projection [B, L, H] @ [H, V] = [B, L , V]\n",
        "    hidden_flat = hidden_states.reshape(-1, self.hidden_size) # reshape for matmul [B*L, H]\n",
        "    logits_flat = hidden_flat @ self.output_layer # [B*L, V] matmul\n",
        "    logits = logits_flat.reshape(batch_size, seq_len, self.vocab_size) #[B, L, V]\n",
        "\n",
        "    print(f\"Output projection: {hidden_states.shape} @ {self.output_layer.shape} = {logits.shape}\")\n",
        "    print(f\"  Logits range: [{logits.min():.3f}, {logits.max():.3f}]\")\n",
        "\n",
        "    return logits, hidden_states\n",
        "\n",
        "\n",
        "  def compute_loss(self, logits: np.ndarray, targets:np.ndarray, mask: Optional[np.ndarray] = None) -> float:\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss\n",
        "    logits: [batch_size, seq_len, vocab_size] predicted scores\n",
        "    targets: [batch_size, seq_len] true token indices\n",
        "    mask: [batch_size, seq_len] binary mask for valid positions\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "    # softmax: exp(x) / sum(exp(x))\n",
        "    logits_max = np.max(logits, axis=-1, keepdims=True )  # [B, L, 1] for numerical stability\n",
        "    exp_logits = np.exp(logits - logits_max) # remove max over all to avoid overflow\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)  # [B, L, V] normalized probs\n",
        "\n",
        "    # Extract probabilities of true tokens\n",
        "    batch_indices = np.arange(batch_size)[:, None] # [B, 1] for indexing. The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1).\n",
        "    seq_indices = np.arange(seq_len)[None, :] # The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1)\n",
        "    target_probs = probs[batch_indices, seq_indices, targets] # [B, L] real probs\n",
        "\n",
        "    # Neg Log likelihood: -log(P(correct_token))\n",
        "    loss_matrix = -np.log(target_probs + 1e-10) # add small value to avoid log(0)\n",
        "\n",
        "    if mask is not None:\n",
        "      loss_matrix = loss_matrix * mask # zero out padded positions\n",
        "      total_loss = np.sum(loss_matrix) / np.sum(mask) # avg over only valid tokens\n",
        "    else:\n",
        "      total_loss = np.mean(loss_matrix) # avg over all the tokens\n",
        "\n",
        "\n",
        "    print(f\"\\nLoss computation:\")\n",
        "    print(f\"  Target probs shape: {target_probs.shape}\")\n",
        "    print(f\"  Sample target probs: {target_probs[0, :5]}\")  # First 5 positions\n",
        "    print(f\"  Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "  def train_step(self, input_ids: np.ndarray, targets:np.ndarray) -> float:\n",
        "    \"\"\"Single training step with gradietn descent\"\"\"\n",
        "    # Forward pass\n",
        "    logits, hidden_states = self.forward(input_ids)\n",
        "    loss = self.compute_loss(logits, targets)\n",
        "\n",
        "    # simpified gradient update\n",
        "    self.embedding    *= (1 - self.learning_rate * 0.01) # simple weight decay\n",
        "    self.output_layer *= (1 - self.learning_rate * 0.01) # simple weight decay\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "EZYmAGxSHCSx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test vanilla instruction tuning"
      ],
      "metadata": {
        "id": "_uNSTslmP-r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VanillaInstructionTuning(vocab_size=1000, hidden_size=64)\n",
        "\n",
        "# Create dummy data\n",
        "batch_size, seq_len = 2, 10\n",
        "input_ids = np.random.randint(0, 1000, (batch_size, seq_len))  # Random token indices\n",
        "targets = np.random.randint(0, 1000, (batch_size, seq_len))  # Target tokens (shifted in practice)\n",
        "\n",
        "print(f\"Input shape: {input_ids.shape}\")\n",
        "print(f\"Target shape: {targets.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "logits, hidden = model.forward(input_ids)\n",
        "\n",
        "# Compute loss\n",
        "loss = model.compute_loss(logits, targets)\n",
        "\n",
        "# Training step\n",
        "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
        "print(\"Training step:\")\n",
        "train_loss = model.train_step(input_ids, targets)\n",
        "print(f\"Training loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "o4DgYbmdI7L0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1694949c-18f4-4a86-ba3f-47ca0a7828f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized instruction tuning model:\n",
            "  Embedding matrix: (1000, 64) = [1000, 64]\n",
            "  Output layer: (64, 1000) = [64, 1000]\n",
            "Input shape: (2, 10)\n",
            "Target shape: (2, 10)\n",
            "\n",
            "Embedding lookup: (2, 10) -> (2, 10, 64)\n",
            "  Sample embeddings: [-0.00512105 -0.00206508 -0.00149675  0.00193628 -0.00727759]...\n",
            "Output projection: (2, 10, 64) @ (64, 1000) = (2, 10, 1000)\n",
            "  Logits range: [-0.003, 0.003]\n",
            "\n",
            "Loss computation:\n",
            "  Target probs shape: (2, 10)\n",
            "  Sample target probs: [0.00099869 0.00100018 0.0009998  0.00100084 0.00099994]\n",
            "  Loss: 6.9077\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training step:\n",
            "\n",
            "Embedding lookup: (2, 10) -> (2, 10, 64)\n",
            "  Sample embeddings: [-0.00512105 -0.00206508 -0.00149675  0.00193628 -0.00727759]...\n",
            "Output projection: (2, 10, 64) @ (64, 1000) = (2, 10, 1000)\n",
            "  Logits range: [-0.003, 0.003]\n",
            "\n",
            "Loss computation:\n",
            "  Target probs shape: (2, 10)\n",
            "  Sample target probs: [0.00099869 0.00100018 0.0009998  0.00100084 0.00099994]\n",
            "  Loss: 6.9077\n",
            "Training loss: 6.9077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Instruction Tuning with HF"
      ],
      "metadata": {
        "id": "xoPhvJowQm0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionTuningDataset(Dataset):\n",
        " \"\"\"PyTorch dataset for instruction tuning with validation\"\"\"\n",
        "\n",
        " def __init__(self, examples:List[InstructionExample], tokenizer, max_length: int = 512 ):\n",
        "   self.examples = examples\n",
        "   self.tokenizer = tokenizer\n",
        "   self.max_length = max_length\n",
        "\n",
        " def __len__(self):\n",
        "   return len(self.examples )\n",
        "\n",
        " def __getitem__(self, idx):\n",
        "   example = self.examples[idx]\n",
        "\n",
        "   # Format as full text (instruction + response)\n",
        "   text = example.format_full() # pydantic models method\n",
        "\n",
        "   # tokenize with attn mask\n",
        "   encoding = self.tokenizer(\n",
        "       text,\n",
        "       truncation=True, # trunc to max_length\n",
        "       padding='max_length', # pad to max length\n",
        "       max_length=self.max_length,\n",
        "       return_tensors='pt'\n",
        "\n",
        "   )\n",
        "\n",
        "   encoding['labels'] = encoding['input_ids'].clone() # copy input_ids as labels\n",
        "\n",
        "   # Mask padding tokens (100 is ignore index in Cross Entropy Loss)\n",
        "   encoding['labels'][encoding['attention_mask'] == 0] = -100 # ignore padding in loss\n",
        "\n",
        "   return {k:v.squeeze(0) for k,v in encoding.items()}\n",
        "\n",
        "def setup_instruction_tuning_pytorch_enhanced():\n",
        " \"\"\"Setup instruction tuning with PyTorch, HuggingFace, and PEFT for efficiency\"\"\"\n",
        "\n",
        " # inicialtize accelerator\n",
        " accelerator = Accelerator(\n",
        "     mixed_precision = 'fp16' if torch.cuda.is_available() else None,\n",
        "     gradient_accumulation_steps=4 # accum gradients for larger effective batch size\n",
        " )\n",
        "\n",
        " # load model with opt 8-bit quant\n",
        " model_name = 'gpt2'\n",
        " print(f\"Loading model: {model_name}\")\n",
        "\n",
        " # 8-bit quantization if gpu\n",
        " quantization_config = None\n",
        " if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:\n",
        "   quantization_config = BitsAndBytesConfig(\n",
        "       load_in_8bit = True,  # load in 8bit\n",
        "       bnb_8bit_compute_dtype=torch.float16, # compute in fp16\n",
        "       bnb_8bit_quant_type=\"nf8\", # quantization type\n",
        "       bnb_8bit_use_double_quant=True, # double quantization for more saved mem. Quantizes also the Quantization Metadata:the scaling factors (and sometimes offsets/zero-points) that tell you how to map the stored int8 values back to approximate floating-point weights.\n",
        "   )\n",
        "   print(\"Using 8-bit quantization for memory efficiency\")\n",
        "\n",
        " # load tokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        " tokenizer.pad_token = tokenizer.eos_token # set padding tkn\n",
        " tokenizer.padding_side = 'right' # pad on the right for Causal LM\n",
        "\n",
        " # load model with opt quantization\n",
        " if quantization_config:\n",
        "   model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_name,\n",
        "       quantization_config=quantization_config,\n",
        "       device_map=\"Auto\"\n",
        "   )\n",
        "\n",
        "   # prepare model for k-bit training\n",
        "   model = prepare_model_for_kbit_training(model) # `prepare_model_for_kbit_training` modifies a model's weights and layers to be compatible with low-bit (8-bit or 4-bit) training, enabling memory-efficient fine-tuning.\n",
        "\n",
        " else:\n",
        "   model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        " print(f\"Model loaded: {model.config.n_layer} layers, {model.config.n_head} heads\")\n",
        " print(f\"Vocab size: {model.config.vocab_size}\")\n",
        " print(f\"Hidden size: {model.config.n_embd}\")\n",
        "\n",
        "\n",
        " # Configure PEFT (LoRA) for parameter-efficient fine-tuning\n",
        " peft_config = LoraConfig(\n",
        "     task_type=TaskType.CAUSAL_LM,\n",
        "     r=16, # lora rank -lower\n",
        "     lora_alpha = 32, # scaling param\n",
        "     lora_dropout=0.1,\n",
        "     bias='none', # not adapt biases\n",
        "     target_modules=[\"c_attn\", \"c_proj\"] # gpt2 attn layers to adapt\n",
        " )\n",
        "\n",
        "\n",
        " # Convert to PEFT model\n",
        " model = get_peft_model(model, peft_config) # wrap model with LoRA adapters\n",
        " model.print_trainable_parameters() # shows param efficiency gain\n",
        "\n",
        " # prepare dataset\n",
        " instruction_examples = create_instruction_dataset() # pydantic validation\n",
        " dataset = InstructionTuningDataset(instruction_examples, tokenizer, max_length=128)\n",
        "\n",
        " # create dataloader\n",
        " dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        " # seutp optimizer with 8bit adam if available\n",
        " if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:\n",
        "   optimizer = bnb.optim.AdamW8bit(\n",
        "       model.parameters(),\n",
        "       lr=5e-5,\n",
        "       betas=(0.9, 0.999), # adam betas variables\n",
        "       weight_decay=0.01, # weight decay for regularization\n",
        "   )\n",
        "   print(\"Using 8-bit AdamW optimizer\")\n",
        "\n",
        " else:\n",
        "   optimizer = torch.optim.AdamW(\n",
        "       model.parameters(),\n",
        "       lr=5e-5,\n",
        "       weight_decay=0.01  # weight decay for regularization\n",
        "   )\n",
        "\n",
        " # setup lr scheduler\n",
        " # The scheduler will cyclically vary the learning rate with cosine decay, restarting every T_i iterations while gradually increasing the period and never letting the LR drop below eta_min\n",
        " # The learning rate will follow a cosine decay for 10 iterations, then restart for 20 iterations, then 40, doubling each cycle, while never dropping below 1e-6.\n",
        " scheduler = CosineAnnealingWarmRestarts(\n",
        "     optimizer,\n",
        "     T_0=10,  # Number of iterations for the first restart\n",
        "     T_mult=2,  # Factor to increase T_i after a restart\n",
        "     eta_min=1e-6,  # Minimum learning rate, never below it\n",
        " )\n",
        "\n",
        "\n",
        " # Prepare everything with accelerator\n",
        " model, optimizer, dataloader, scheduler = accelerator.prepare(\n",
        "     model, optimizer, dataloader, scheduler\n",
        " )\n",
        "\n",
        " # Initialize evaluation metrics\n",
        " bleu_metric = evaluate.load(\"bleu\")  # BLEU score for generation quality\n",
        " rouge_metric = evaluate.load(\"rouge\")  # ROUGE score for summarization\n",
        "\n",
        " return model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric\n",
        "\n",
        "# Setup and test enhanced training\n",
        "model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric = setup_instruction_tuning_pytorch_enhanced()\n",
        "\n",
        "# Test one training step with metrics\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Testing enhanced training step with metrics:\")\n",
        "\n",
        "model.train()  # Set to training mode\n",
        "\n",
        "for batch_idx, batch in enumerate(dataloader):\n",
        " if batch_idx > 0:  # Just test one batch\n",
        "   break\n",
        "\n",
        " # Move batch to accelerator device (handles multi-GPU automatically)\n",
        " # Note: accelerator.prepare already handles device placement\n",
        "\n",
        " print(f\"Batch shapes:\")\n",
        " print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
        " print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
        " print(f\"  labels: {batch['labels'].shape}\")\n",
        "\n",
        " # Forward pass with accelerator's context\n",
        " with accelerator.accumulate(model):  # Handle gradient accumulation\n",
        "   outputs = model(**batch)  # Pass entire batch to model\n",
        "   loss = outputs.loss  # Extract loss\n",
        "   logits = outputs.logits  # Extract logits\n",
        "\n",
        "   print(f\"\\nForward pass:\")\n",
        "   print(f\"  Loss: {loss.item():.4f}\")\n",
        "   print(f\"  Logits shape: {logits.shape}\")\n",
        "   print(f\"  Perplexity: {torch.exp(loss).item():.2f}\")\n",
        "\n",
        "   # Backward pass with accelerator (handles mixed precision)\n",
        "   accelerator.backward(loss)  # Compute gradients with automatic mixed precision\n",
        "\n",
        "   # Gradient clipping for stability\n",
        "   accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to prevent explosion\n",
        "\n",
        "   # Gradient norm for monitoring\n",
        "   total_norm = 0\n",
        "   for p in model.parameters():\n",
        "     if p.grad is not None:\n",
        "       total_norm += p.grad.data.norm(2).item() ** 2  # L2 norm squared\n",
        "   total_norm = total_norm ** 0.5  # Square root for final norm\n",
        "   print(f\"  Gradient norm: {total_norm:.4f}\")\n",
        "\n",
        "   # Update weights\n",
        "   optimizer.step()  # Apply gradients\n",
        "   scheduler.step()  # Update learning rate\n",
        "   optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "   # Log to wandb if enabled\n",
        "   if USE_WANDB:\n",
        "     wandb.log({\n",
        "       \"train/loss\": loss.item(),\n",
        "       \"train/perplexity\": torch.exp(loss).item(),\n",
        "       \"train/grad_norm\": total_norm,\n",
        "       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "     })\n",
        "\n",
        "# Generate sample output with beam search\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Testing generation with enhanced decoding:\")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "test_instruction = \"Translate to French: Hello world\"\n",
        "prompt = f\"### Instruction:\\n{test_instruction}\\n\\n### Response:\\n\"\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "# Move to accelerator device\n",
        "inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate with enhanced parameters\n",
        "with torch.no_grad():\n",
        " outputs = model.generate(\n",
        "   **inputs,\n",
        "   max_new_tokens=20,  # Generate up to 20 new tokens\n",
        "   temperature=0.8,  # Sampling temperature\n",
        "   do_sample=True,  # Use sampling instead of greedy\n",
        "   top_p=0.9,  # Nucleus sampling threshold\n",
        "   top_k=50,  # Top-k sampling\n",
        "   repetition_penalty=1.2,  # Penalize repetition\n",
        "   pad_token_id=tokenizer.eos_token_id,\n",
        "   num_beams=4,  # Beam search for better quality\n",
        "   early_stopping=True,  # Stop when all beams reach EOS\n",
        " )\n",
        "\n",
        "# Decode and evaluate\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Input: {prompt}\")\n",
        "print(f\"Generated: {generated_text}\")\n",
        "\n",
        "# Compute BLEU score (example - normally done on validation set)\n",
        "reference = [\"Bonjour le monde\"]  # Expected translation\n",
        "prediction = generated_text.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "bleu_score = bleu_metric.compute(predictions=[prediction], references=[[ref] for ref in reference])\n",
        "print(f\"\\nGeneration metrics:\")\n",
        "print(f\"  BLEU score: {bleu_score['bleu']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nInsufficient Training: Your model saw only 5 examples for 1 step. Language models need thousands of examples and many epochs to learn new tasks.\")\n",
        "print(\"\\nData Mismatch: The model hasn't learned the instruction format properly. GPT-2 wasn't trained on instruction-following data.\")\n",
        "print(\"\\nRepetition Issue: The model is stuck in a loop due to poor conditioning on the prompt format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984,
          "referenced_widgets": [
            "d73b5ad73680422cbbd2f7571a6aceda",
            "692743ff8bbb4aa7bf3d03b4dee0a6ee",
            "a9d5ab677aa648a8b31995b8bb8501eb",
            "5531c51bf52f46cba3c566339ec6ce84",
            "a5232553d98b45a2b5992f945d51baaf",
            "be7f4d1ff1454274b796473703d7e0fb",
            "c3fa4cd303ee44818f7b93883d9a356a",
            "ae52dafde74a436eaf53e6afb555e907",
            "7e331eeaf9e84a2287404ba4e7bda576",
            "ea7052fd734047c48b774f7c1c7f2d57",
            "38cb1d9c82a34354bd0fe110855a55cc",
            "e619d48df6254d1dbf68d5b0443dee58",
            "7fb68ffeb4854a47b7f796627bddd393",
            "c590c3b8f14049498c4f07484a44bc05",
            "4b18c0efebbc4b1f823d23c199eff8dd",
            "a0ffd521c9914b2da45e806c64b45ea6",
            "6cb2f564eb8c4699a426725ba10049bd",
            "ce6839d820984280815ed40650fa5189",
            "3bb828bfd8dc4f018a9c5fe08e74805d",
            "85bf903ace374efb85f8225d44370a1c",
            "6df006aed72b4dccbc013cce749b9006",
            "8377605098af47f9855e571e669abaed",
            "e15793bf8a24417b9540e60cd5292a15",
            "8263fa11dec849729019aa3a5807ab27",
            "e01f2ff24dcc42b6a8268f6e53fa4164",
            "05a0991e8060423ab93f5bc2d30d8b77",
            "91d5df0c204b4d43abd2fecdb8f4fdad",
            "eefbc2894d0b447b980d32b1292e6c58",
            "4dba45861a7a40fbb31c0909f5565480",
            "2cdac29b3160482d85fcd1cdf2bdbc26",
            "87e3cba652a141ceb32b60debe2aaa00",
            "cd5ff55079db443eb208ad7aaece2478",
            "f5e2ba634a694a7da535f9a2b2fe1a85"
          ]
        },
        "id": "0snXzHnEQB4b",
        "outputId": "a5bb136f-7a47-4234-ea00-75427d62a623"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: gpt2\n",
            "Model loaded: 12 layers, 12 heads\n",
            "Vocab size: 50257\n",
            "Hidden size: 768\n",
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d73b5ad73680422cbbd2f7571a6aceda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e619d48df6254d1dbf68d5b0443dee58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e15793bf8a24417b9540e60cd5292a15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Testing enhanced training step with metrics:\n",
            "Batch shapes:\n",
            "  input_ids: torch.Size([2, 128])\n",
            "  attention_mask: torch.Size([2, 128])\n",
            "  labels: torch.Size([2, 128])\n",
            "\n",
            "Forward pass:\n",
            "  Loss: 3.6386\n",
            "  Logits shape: torch.Size([2, 128, 50257])\n",
            "  Perplexity: 38.04\n",
            "  Gradient norm: 0.3040\n",
            "\n",
            "--------------------------------------------------\n",
            "Testing generation with enhanced decoding:\n",
            "Input: ### Instruction:\n",
            "Translate to French: Hello world\n",
            "\n",
            "### Response:\n",
            "\n",
            "Generated: ### Instruction:\n",
            "Translate to French: Hello world\n",
            "\n",
            "### Response:\n",
            "\n",
            "Hello world.\n",
            "\n",
            "Hello world.\n",
            "\n",
            "Hello world.\n",
            "\n",
            "Hello world.\n",
            "\n",
            "\n",
            "Generation metrics:\n",
            "  BLEU score: 0.0000\n",
            "\n",
            "\n",
            "Insufficient Training: Your model saw only 5 examples for 1 step. Language models need thousands of examples and many epochs to learn new tasks.\n",
            "\n",
            "Data Mismatch: The model hasn't learned the instruction format properly. GPT-2 wasn't trained on instruction-following data.\n",
            "\n",
            "Repetition Issue: The model is stuck in a loop due to poor conditioning on the prompt format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bradley-Terry Preference Model\n",
        "\n",
        "1. HUMAN LABELS: (A > B), (C > D), (E > F)... [preference dataset]\n",
        "\n",
        "                        ↓\n",
        "\n",
        "2. INITIALIZE: Reward model r_θ with random weights θ₀\n",
        "                        ↓\n",
        "\n",
        "3. FORWARD PASS: r_θ(prompt, A) → score_A, r_θ(prompt, B) → score_B\n",
        "\n",
        "                        ↓\n",
        "\n",
        "4. COMPUTE PROBABILITY: P(A > B) = σ(score_A - score_B)\n",
        "\n",
        "                        ↓\n",
        "\n",
        "5. MEASURE ERROR: Loss = -log P(human_choice)\n",
        "\n",
        "                        ↓\n",
        "                      \n",
        "6. BACKPROP: ∇Loss flows through network to compute ∇θ\n",
        "\n",
        "                        ↓\n",
        "\n",
        "7. UPDATE: θ_new = θ_old - learning_rate × ∇θ\n",
        "\n",
        "                        ↓\n",
        "\n",
        "8. REPEAT: Go to step 3 with next batch until convergence\n",
        "\n",
        "                        ↓\n",
        "\n",
        "9. RESULT: Trained r_θ that assigns higher scores to human-preferred outputs"
      ],
      "metadata": {
        "id": "n-6n_SWH8g29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BradleyTerryModel:\n",
        "  \"\"\"\n",
        "  Bradley-Terry model for preference probabilities\n",
        "  P(o_i > o_j | x) = σ(z_i - zz_j) where z_i, z_j are scores\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    print(\"Bradley-Terry Preference Model\")\n",
        "    print(\"Mathematical formulation: P(o_i ≻ o_j|x) = 1 / (1 + exp(-(z_i - z_j)))\")\n",
        "    print(\"Where z_i, z_j are scalar rewards/scores for outputs o_i, o_j\\n\")\n",
        "\n",
        "  def compute_preference_prob(self, score_i: float, score_j: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute probability that option i is preferred over option j\n",
        "    Uses sigmoid function on the difference of both scores\n",
        "    Useful for data pair-decomposed like in preference optimization alignment\n",
        "    \"\"\"\n",
        "\n",
        "    score_diff = score_i-score_j\n",
        "    print(f\"Score difference: z_i - z_j = {score_i:.3f} - {score_j:.3f} = {score_diff:.3f}\")\n",
        "\n",
        "\n",
        "    # logistic sigmoid σ(x) = 1/(1+exp(-z))\n",
        "    prob = 1.0 / (1.0 + np.exp(-score_diff))\n",
        "    print(f\"P(o_i ≻ o_j) = σ({score_diff:.3f}) = {prob:.4f}\")\n",
        "\n",
        "\n",
        "    # interpretation\n",
        "    if prob > 0.5:\n",
        "      print(f\"Option i is preferred (p={prob:.2%})\")\n",
        "\n",
        "    elif prob < 0.5:\n",
        "      print(f\"Option j is preferred (p={(1-prob):.2%})\")\n",
        "\n",
        "    else:\n",
        "      print(\"No preference (p=50%)\")\n",
        "\n",
        "    return prob\n",
        "\n",
        "  def compute_log_likelihood(self, scores_chosen:np.ndarray, scores_rejected:np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute log likelihood of preference\n",
        "    scores_chosen: [n_samples] scores for chosen responses\n",
        "    scores_rejected: [n_samples] scores for rejected responses\n",
        "    The loss function uses this probability to compute gradients that push scores further apart. Simple argmax would give binary 1/0 feedback with no gradient signal for how much to adjust scores.\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples = len(scores_chosen)\n",
        "\n",
        "    # compute preference probs for all pairs\n",
        "    score_diffs = scores_chosen - scores_rejected # [n_samples] differences\n",
        "    print(f\"\\nscore_diffs: {score_diffs} \\n\")\n",
        "\n",
        "    probs = 1.0 / (1.0 + np.exp(-score_diffs))    # [n_samples] sigmoid of diffs\n",
        "    print(f\"\\nprobs: {probs} \\n\")\n",
        "\n",
        "    # log likelihood, sum of log probs\n",
        "    log_likelihood = np.sum(np.log(probs + 1e-10)) # Add epsilon to prevent log(0)\n",
        "    print(f\"\\nlog_likelihood: {log_likelihood} \\n\")\n",
        "\n",
        "\n",
        "    print(f\"Score differences: {score_diffs[:3]}...\")  # First 3 differences\n",
        "    print(f\"Preference probs: {probs[:3]}...\")  # First 3 probabilities\n",
        "    print(f\"Total log likelihood: {log_likelihood:.4f}\")\n",
        "    print(f\"Average log likelihood: {log_likelihood/n_samples:.4f}\")\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "\n",
        "  def visualize_bradley_terry(self):\n",
        "    \"\"\"Visualize the Bradley-Terry preference function\"\"\"\n",
        "    score_diffs = np.linspace(-5, 5, 100)  # Range of score differences\n",
        "    probs = 1.0 / (1.0 + np.exp(-score_diffs))  # Preference probabilities\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(score_diffs, probs, 'b-', linewidth=2)\n",
        "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)  # 50% line\n",
        "    plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)  # Equal scores line\n",
        "    plt.xlabel('Score Difference (z_i - z_j)')\n",
        "    plt.ylabel('P(o_i ≻ o_j)')\n",
        "    plt.title('Bradley-Terry Preference Function')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add annotations for key points\n",
        "    plt.annotate('No preference', xy=(0, 0.5), xytext=(-2, 0.3),\n",
        "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.5))\n",
        "    plt.annotate('Strong preference for i', xy=(3, 0.95), xytext=(1, 0.8),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', alpha=0.5))\n",
        "\n",
        "    # Subplot 2: Effect of score magnitude\n",
        "    plt.subplot(1, 2, 2)\n",
        "    scores_i = np.array([0, 1, 2, 3, 4])  # Different score values for option i\n",
        "    scores_j = 1  # Fixed score for option j\n",
        "    probs = 1.0 / (1.0 + np.exp(-(scores_i - scores_j)))\n",
        "\n",
        "    plt.bar(range(len(scores_i)), probs, color=['red' if p < 0.5 else 'green' for p in probs])\n",
        "    plt.xlabel('Score of option i (score_j = 1)')\n",
        "    plt.ylabel('P(o_i ≻ o_j)')\n",
        "    plt.title('Preference vs Absolute Scores')\n",
        "    plt.xticks(range(len(scores_i)), scores_i)\n",
        "    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Yb5GqpxClrEY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bradley-Terry Preference Model Training\n",
        "\n",
        "**1. HUMAN LABELS**: Collect preference dataset (A > B), (C > D), (E > F)...\n",
        "   - *Provides ground truth supervision signal for training*\n",
        "<br>\n",
        "\n",
        "**2. INITIALIZE**: Reward model r_θ with random weights θ₀\n",
        "   - *Neural network that will learn to score outputs*\n",
        "   - θ is the set of all the learnable params on it\n",
        "<br>\n",
        "\n",
        "\n",
        "**3. FORWARD PASS**: r_θ(prompt, A) → score_A, r_θ(prompt, B) → score_B\n",
        "   - *Model evaluates each output, producing scalar scores*\n",
        "<br>\n",
        "\n",
        "\n",
        "**4. COMPUTE PROBABILITY**: P(A > B) = σ(score_A - score_B)\n",
        "   - *Converts score difference into preference probability*\n",
        "   - The order is crucial, its the ground truth preference: σ(score_A - score_B) gives P(A > B) while σ(score_B - score_A) gives P(B > A)\n",
        "   - Reversing the subtraction flips the meaning, so mismatching this during training would teach the model to prefer the wrong outputs.\n",
        "<br>\n",
        "\n",
        "\n",
        "**5. MEASURE ERROR**: Loss = -log P(human_choice)\n",
        "   - *Quantifies disagreement between model and human judgment*\n",
        "<br>\n",
        "\n",
        "\n",
        "**6. BACKPROP**: ∇Loss flows through network to compute ∇θ\n",
        "   - *Calculates how to adjust each parameter to reduce error*\n",
        "<br>\n",
        "\n",
        "\n",
        "**7. UPDATE**: θ_new = θ_old - learning_rate × ∇θ\n",
        "   - *Adjusts millions of parameters to better match human preferences*\n",
        "<br>\n",
        "\n",
        "\n",
        "**8. REPEAT**: Return to step 3 with next batch until convergence\n",
        "   - *Iteratively improves model through multiple passes over data*\n",
        "<br>\n",
        "\n",
        "\n",
        "**9. RESULT**: Trained r_θ that assigns higher scores to human-preferred outputs\n",
        "   - *Final model generalizes to evaluate novel outputs*\n",
        "<br>\n",
        "\n",
        "\n",
        "The reward model learns scoring patterns from human judgments, while humans provide the essential preference labels that define what constitutes \"better\" outputs - without human preferences, the model would have no training signal."
      ],
      "metadata": {
        "id": "FDHCcdI3B90n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bt_model = BradleyTerryModel()\n",
        "\n",
        "# Test single preference probability\n",
        "print(\"\\nExample 1: Similar scores\")\n",
        "bt_model.compute_preference_prob(score_i=2.1, score_j=2.0)\n",
        "\n",
        "print(\"\\nExample 2: Large difference\")\n",
        "bt_model.compute_preference_prob(score_i=5.0, score_j=1.0)\n",
        "\n",
        "print(\"\\nExample 3: Negative scores\")\n",
        "bt_model.compute_preference_prob(score_i=-1.0, score_j=-3.0)\n",
        "\n",
        "# Test log likelihood computation\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Computing log likelihood for preference dataset:\")\n",
        "n_samples = 5\n",
        "scores_chosen = np.array([3.0, 2.5, 4.0, 1.5, 3.5])  # Scores for chosen options\n",
        "scores_rejected = np.array([1.0, 2.0, 1.0, 0.5, 3.0])  # Scores for rejected options\n",
        "\n",
        "log_likelihood = bt_model.compute_log_likelihood(scores_chosen, scores_rejected)\n",
        "\n",
        "# Visualize\n",
        "bt_model.visualize_bradley_terry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RYUZGJ9198_M",
        "outputId": "1df03521-73e5-4ecc-bddb-4b87c3633972"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bradley-Terry Preference Model\n",
            "Mathematical formulation: P(o_i ≻ o_j|x) = 1 / (1 + exp(-(z_i - z_j)))\n",
            "Where z_i, z_j are scalar rewards/scores for outputs o_i, o_j\n",
            "\n",
            "\n",
            "Example 1: Similar scores\n",
            "Score difference: z_i - z_j = 2.100 - 2.000 = 0.100\n",
            "P(o_i ≻ o_j) = σ(0.100) = 0.5250\n",
            "Option i is preferred (p=52.50%)\n",
            "\n",
            "Example 2: Large difference\n",
            "Score difference: z_i - z_j = 5.000 - 1.000 = 4.000\n",
            "P(o_i ≻ o_j) = σ(4.000) = 0.9820\n",
            "Option i is preferred (p=98.20%)\n",
            "\n",
            "Example 3: Negative scores\n",
            "Score difference: z_i - z_j = -1.000 - -3.000 = 2.000\n",
            "P(o_i ≻ o_j) = σ(2.000) = 0.8808\n",
            "Option i is preferred (p=88.08%)\n",
            "\n",
            "--------------------------------------------------\n",
            "Computing log likelihood for preference dataset:\n",
            "\n",
            "score_diffs: [2.  0.5 3.  1.  0.5] \n",
            "\n",
            "\n",
            "probs: [0.88079708 0.62245933 0.95257413 0.73105858 0.62245933] \n",
            "\n",
            "\n",
            "log_likelihood: -1.4369310178185444 \n",
            "\n",
            "Score differences: [2.  0.5 3. ]...\n",
            "Preference probs: [0.88079708 0.62245933 0.95257413]...\n",
            "Total log likelihood: -1.4369\n",
            "Average log likelihood: -0.2874\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vanilla Reward Model Training with Numpy"
      ],
      "metadata": {
        "id": "1sLpWh45SG7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRewardModel:\n",
        "  \"\"\"\n",
        "  Simple reward model that learns to score outputs based on preferences\n",
        "  Uses Bradley-Terry formulation with gradient descent\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim: int, hidden_dim: int=128):\n",
        "    self.input_dim = input_dim  # input features dim\n",
        "    self.hidden_dim = hidden_dim # Hidden layer dim\n",
        "\n",
        "    # init weigths with small non-zero random vals\n",
        "    # first layer\n",
        "    self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01 # first layer weights  [D, H]\n",
        "    self.b1 = np.zeros(hidden_dim) # first layer bias [H]\n",
        "\n",
        "    # output layer\n",
        "    self.W2 = np.random.randn(hidden_dim,1) * 0.01 # output layer weights  [H,1]\n",
        "    self.b2 = np.zeros(1) # output layer bias [1]\n",
        "\n",
        "    print(f\"Initialized Reward Model:\")\n",
        "    print(f\"  Input dim: {input_dim}\")\n",
        "    print(f\"  Hidden dim: {hidden_dim}\")\n",
        "    print(f\"  Parameters: {(input_dim * hidden_dim + hidden_dim + hidden_dim + 1):,}\")\n",
        "\n",
        "  def forward(self, x:np.ndarray) -> Tuple[float, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Forward pass to compute reward score\n",
        "    x: [input_dim] or [batch_size, input_dim] input features\n",
        "    Returns: scalar reward and intermediate activations\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure 2d input\n",
        "    if x.ndim == 1:\n",
        "      print(\"\\n1-dimensional input. Reshaping:\\n\")\n",
        "      print(\"Before: \", x, \"\\n\")\n",
        "      x = x.reshape(1,-1) # [D] -> [1,D]\n",
        "      print(\"After: \", x, \"\\n\")\n",
        "\n",
        "    # Layer 1: Linear + ReLU activation\n",
        "    z1 = x @ self.W1 + self.b1 # [B,D] @ [D,H] + [H] = [B,H]\n",
        "    h1 = np.maximum(0,z1) # ReLU activation: max(0,x)\n",
        "\n",
        "    # Layer 2: Linear (no activation, regression)\n",
        "    z2 = h1 @ self.W2 + self.b2 # [B,H] @ [H,1] + [H] = [B,H]\n",
        "    reward = z2.squeeze() # remove singleton dim\n",
        "\n",
        "    # store activations for backward pass\n",
        "    activations = {\n",
        "        'x': x,\n",
        "        'z1':z1,\n",
        "        'h1':h1,\n",
        "        'z2':z2,\n",
        "        'reward':reward\n",
        "    }\n",
        "\n",
        "    return reward, activations\n",
        "\n",
        "  def compute_bradley_terry_loss(self, x_chosen: np.ndarray, x_rejected: np.ndarray) -> Tuple[float, Dict]:\n",
        "    \"\"\"\n",
        "    Compute loss using Bradley-Terry model\n",
        "    L = -log(σ(r(x_chosen) - r(x_rejected)))\n",
        "    \"\"\"\n",
        "    # Forward pass for both options\n",
        "    r_chosen, act_chosen = self.forward(x_chosen)  # Reward and activations for chosen option\n",
        "    r_rejected, act_rejected = self.forward(x_rejected)  # Reward and activations for rejected option\n",
        "\n",
        "    # Compute preference probability with Bradley-Terry\n",
        "    score_diff = r_chosen - r_rejected\n",
        "    prob = 1.0 / (1.0 + np.exp(-score_diff)) # sigmoid of diff of scores made by the reward model weithgs\n",
        "\n",
        "    # Negative log likelihood loss\n",
        "    loss = -np.log(prob + 1e-10)  # Add epsilon for numerical stability\n",
        "\n",
        "    print(f\"\\nBradley-Terry Loss Computation:\")\n",
        "    print(f\"  r(chosen) = {r_chosen:.3f}\")\n",
        "    print(f\"  r(rejected) = {r_rejected:.3f}\")\n",
        "    print(f\"  Score diff = {score_diff:.3f}\")\n",
        "    print(f\"  P(chosen > rejected) = {prob:.4f}\")\n",
        "    print(f\"  Loss = -log(p) = {loss:.4f}\")\n",
        "\n",
        "    return loss, {'act_chosen': act_chosen, 'act_rejected': act_rejected, 'prob': prob}\n",
        "\n",
        "  def backward(self, loss_info: Dict, learning_rate: float=0.01):\n",
        "    \"\"\"\n",
        "    Backward pass using chain rule:\n",
        "\n",
        "    1. Divide problem into two parts:\n",
        "    - Gradient for the chosen example: ∂L/∂r_chosen = -(1-prob)\n",
        "    - Gradient for the rejected example: ∂L/∂r_rejected = +(1-prob)\n",
        "\n",
        "\n",
        "    2. Chain Rule:\n",
        "    - For the chosen:\n",
        "      grad_W2_chosen = grad_score_diff * h1.T        # ∂L/∂W2 = ∂L/∂r_chosen × ∂r_chosen/∂W2\n",
        "      grad_h1 = grad_score_diff * self.W2.T          # ∂L/∂h1 = ∂L/∂r_chosen × ∂r_chosen/∂h1\n",
        "      grad_z1 = grad_h1 * (z1 > 0)                   # ∂L/∂z1 = ∂L/∂h1 × ∂h1/∂z1 (ReLU derivative)\n",
        "      grad_W1_chosen = x.T @ grad_z1                 # ∂L/∂W1 = ∂L/∂z1 × ∂z1/∂W1\n",
        "\n",
        "    - For the rejected:\n",
        "      # Same computation but with -grad_score_diff (opposite sign)\n",
        "      grad_W2_rejected = grad_score_diff_rejected * h1_rejected.T # [H, 1]   ∂L/∂W2_rejected = ∂L/∂r_rejected × ∂r_rejected/∂W2\n",
        "      grad_h1_rejected = grad_score_diff_rejected * self.W2.T     # [1, H]   ∂L/∂h1_rejected = ∂L/∂r_rejected × ∂r_rejected/∂h1\n",
        "      grad_z1_rejected = grad_h1_rejected * (z1_rejected > 0)     # [1, H]  ∂h1/∂z1 = 1 if z1 > 0, else 0 (ReLU derivative)\n",
        "      grad_W1_rejected = x_rejected.T @ grad_z1_rejected          # [D, H]  ∂L/∂W1_rejected = ∂L/∂z1_rejected × ∂z1_rejected/∂W1\n",
        "\n",
        "\n",
        "\n",
        "    3. Combine gradients and update:\n",
        "      self.W2 -= learning_rate * (grad_W2_chosen + grad_W2_rejected)\n",
        "      self.W1 -= learning_rate * (grad_W1_chosen + grad_W1_rejected)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prob = loss_info['prob']\n",
        "    act_chosen = loss_info['act_chosen']\n",
        "    act_rejected = loss_info['act_rejected']\n",
        "\n",
        "    # Gradient of loss w.r.t score difference\n",
        "    grad_score_diff = -(1 - prob)  # ∂L/∂(r_chosen - r_rejected)\n",
        "\n",
        "    print(f\"\\nGradient computation:\")\n",
        "    print(f\"  dL/d(score_diff) = {grad_score_diff:.4f}\")\n",
        "    print(f\"  Current weights (sample): W1[0,0] = {self.W1[0,0]:.6f}\")\n",
        "\n",
        "    # Compute gradients for chosen example (positive contribution)\n",
        "    grad_W2_chosen, grad_W1_chosen = self._compute_gradients(act_chosen, grad_score_diff)\n",
        "\n",
        "    # Compute gradients for rejected example (negative contribution)\n",
        "    grad_W2_rejected, grad_W1_rejected = self._compute_gradients(act_rejected, -grad_score_diff)\n",
        "\n",
        "    # Update weights using combined gradients\n",
        "    self.W2 -= learning_rate * (grad_W2_chosen + grad_W2_rejected)\n",
        "    self.W1 -= learning_rate * (grad_W1_chosen + grad_W1_rejected)\n",
        "\n",
        "    print(f\"  Updated weights (sample): W1[0,0] = {self.W1[0,0]:.6f}\\n\")\n",
        "\n",
        "  def _compute_gradients(self, activations: Dict, grad_reward: float):\n",
        "    \"\"\"Compute gradients for one example using chain rule\"\"\"\n",
        "    x = activations['x']\n",
        "    h1 = activations['h1']\n",
        "    z1 = activations['z1']\n",
        "\n",
        "    # Gradient w.r.t W2: ∂L/∂W2 = ∂L/∂reward × ∂reward/∂W2\n",
        "    grad_W2 = grad_reward * h1.T  # [H, 1]\n",
        "\n",
        "    # Gradient w.r.t h1: ∂L/∂h1 = ∂L/∂reward × ∂reward/∂h1\n",
        "    grad_h1 = grad_reward * self.W2.T  # [1, H]\n",
        "\n",
        "    # Gradient through ReLU: ∂h1/∂z1 = 1 if z1 > 0, else 0\n",
        "    grad_z1 = grad_h1 * (z1 > 0)  # [1, H]\n",
        "\n",
        "    # Gradient w.r.t W1: ∂L/∂W1 = ∂L/∂z1 × ∂z1/∂W1\n",
        "    grad_W1 = x.T @ grad_z1  # [D, H]\n",
        "\n",
        "    return grad_W2, grad_W1\n",
        "\n",
        "  def train_on_preferences(self, preference_pairs: List[Tuple[np.ndarray, np.ndarray]],\n",
        "                          epochs: int = 10, learning_rate: float = 0.01):\n",
        "    \"\"\"Train reward model on preference data\"\"\"\n",
        "    print(f\"\\nTraining Reward Model on {len(preference_pairs)} preference pairs\")\n",
        "    print(f\"Epochs: {epochs}, Learning rate: {learning_rate}\")\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      epoch_loss = 0\n",
        "      for x_chosen, x_rejected in preference_pairs:\n",
        "        loss, loss_info  = self.compute_bradley_terry_loss(x_chosen, x_rejected)\n",
        "        self.backward(loss_info, learning_rate)\n",
        "        epoch_loss += loss\n",
        "\n",
        "      epoch_loss_averaged = epoch_loss/len(preference_pairs)\n",
        "      losses.append(epoch_loss_averaged )\n",
        "\n",
        "      if epoch % 2 == 0:\n",
        "          print(f\"Epoch {epoch}: Averaged loss = {epoch_loss_averaged:.4f}\")\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "ZVrMez7kCA7O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test vanilla reward model"
      ],
      "metadata": {
        "id": "c5cEEakLZfYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create reward model\n",
        "reward_model = VanillaRewardModel(input_dim=10, hidden_dim=32)\n",
        "\n",
        "# Create synthetic preference data\n",
        "n_pairs = 20\n",
        "input_dim = 10\n",
        "preference_pairs = []\n",
        "\n",
        "for _ in range(n_pairs):\n",
        "  # Generate two random feature vectors\n",
        "  x1 = np.random.randn(input_dim)  # Option 1 features\n",
        "  x2 = np.random.randn(input_dim)  # Option 2 features\n",
        "\n",
        "  # Simulate preference: prefer option with larger norm (arbitrary choice)\n",
        "  if np.linalg.norm(x1) > np.linalg.norm(x2):\n",
        "    preference_pairs.append((x1, x2))  # x1 is chosen\n",
        "  else:\n",
        "    preference_pairs.append((x2, x1))  # x2 is chosen\n",
        "\n",
        "print(f\"Created {len(preference_pairs)} synthetic preference pairs\")\n",
        "\n",
        "# Test single forward pass\n",
        "test_input = np.random.randn(input_dim)\n",
        "reward, activations = reward_model.forward(test_input)\n",
        "print(f\"\\nTest forward pass:\")\n",
        "print(f\"  Input shape: {test_input.shape}\")\n",
        "print(f\"  Output reward: {reward:.4f}\")\n",
        "\n",
        "# Test loss computation\n",
        "x_chosen, x_rejected = preference_pairs[0]\n",
        "loss, loss_info = reward_model.compute_bradley_terry_loss(x_chosen, x_rejected)\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "losses = reward_model.train_on_preferences(preference_pairs[:10], epochs=10, learning_rate=0.1)\n",
        "\n",
        "# Plot training curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses, 'b-', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Bradley-Terry Loss')\n",
        "plt.title('Reward Model Training')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Test learned preferences\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Testing learned reward model:\")\n",
        "test_chosen, test_rejected = preference_pairs[15]  # Use unseen pair\n",
        "r_chosen, _ = reward_model.forward(test_chosen)\n",
        "r_rejected, _ = reward_model.forward(test_rejected)\n",
        "print(f\"Reward(chosen): {r_chosen:.4f}\")\n",
        "print(f\"Reward(rejected): {r_rejected:.4f}\")\n",
        "print(f\"Correctly ranked: {r_chosen > r_rejected}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9noL9ttyTWSM",
        "outputId": "b59af635-d660-4226-975e-b79b336334d3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Reward Model:\n",
            "  Input dim: 10\n",
            "  Hidden dim: 32\n",
            "  Parameters: 385\n",
            "Created 20 synthetic preference pairs\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.05005104  0.23456254 -1.436448    1.07435409  0.53037742  1.1209759\n",
            "  0.56027002 -0.01401658  1.50375984 -0.4914242 ] \n",
            "\n",
            "After:  [[-0.05005104  0.23456254 -1.436448    1.07435409  0.53037742  1.1209759\n",
            "   0.56027002 -0.01401658  1.50375984 -0.4914242 ]] \n",
            "\n",
            "\n",
            "Test forward pass:\n",
            "  Input shape: (10,)\n",
            "  Output reward: -0.0010\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.002\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5005\n",
            "  Loss = -log(p) = 0.6921\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Training Reward Model on 10 preference pairs\n",
            "Epochs: 10, Learning rate: 0.1\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.002\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5005\n",
            "  Loss = -log(p) = 0.6921\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4995\n",
            "  Current weights (sample): W1[0,0] = -0.002340\n",
            "  Updated weights (sample): W1[0,0] = -0.002340\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.004\n",
            "  r(rejected) = 0.002\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5006\n",
            "  Loss = -log(p) = 0.6920\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4994\n",
            "  Current weights (sample): W1[0,0] = -0.002340\n",
            "  Updated weights (sample): W1[0,0] = -0.002829\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.002\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.001\n",
            "  P(chosen > rejected) = 0.5001\n",
            "  Loss = -log(p) = 0.6929\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4999\n",
            "  Current weights (sample): W1[0,0] = -0.002829\n",
            "  Updated weights (sample): W1[0,0] = -0.002829\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.002\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.001\n",
            "  P(chosen > rejected) = 0.5004\n",
            "  Loss = -log(p) = 0.6924\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4996\n",
            "  Current weights (sample): W1[0,0] = -0.002829\n",
            "  Updated weights (sample): W1[0,0] = -0.003379\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = -0.000\n",
            "  Score diff = 0.001\n",
            "  P(chosen > rejected) = 0.5003\n",
            "  Loss = -log(p) = 0.6926\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4997\n",
            "  Current weights (sample): W1[0,0] = -0.003379\n",
            "  Updated weights (sample): W1[0,0] = -0.002629\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.000\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = -0.001\n",
            "  P(chosen > rejected) = 0.4997\n",
            "  Loss = -log(p) = 0.6937\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5003\n",
            "  Current weights (sample): W1[0,0] = -0.002629\n",
            "  Updated weights (sample): W1[0,0] = 0.000369\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = 0.000\n",
            "  P(chosen > rejected) = 0.5000\n",
            "  Loss = -log(p) = 0.6931\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5000\n",
            "  Current weights (sample): W1[0,0] = 0.000369\n",
            "  Updated weights (sample): W1[0,0] = -0.000599\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.000\n",
            "  P(chosen > rejected) = 0.5000\n",
            "  Loss = -log(p) = 0.6931\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5000\n",
            "  Current weights (sample): W1[0,0] = -0.000599\n",
            "  Updated weights (sample): W1[0,0] = 0.000233\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.005\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5006\n",
            "  Loss = -log(p) = 0.6920\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4994\n",
            "  Current weights (sample): W1[0,0] = 0.000233\n",
            "  Updated weights (sample): W1[0,0] = 0.000572\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = -0.000\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = -0.001\n",
            "  P(chosen > rejected) = 0.4998\n",
            "  Loss = -log(p) = 0.6936\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5002\n",
            "  Current weights (sample): W1[0,0] = 0.000572\n",
            "  Updated weights (sample): W1[0,0] = 0.000130\n",
            "\n",
            "Epoch 0: Averaged loss = 0.6927\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.007\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.006\n",
            "  P(chosen > rejected) = 0.5016\n",
            "  Loss = -log(p) = 0.6900\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4984\n",
            "  Current weights (sample): W1[0,0] = 0.000130\n",
            "  Updated weights (sample): W1[0,0] = 0.000130\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.009\n",
            "  r(rejected) = 0.004\n",
            "  Score diff = 0.005\n",
            "  P(chosen > rejected) = 0.5012\n",
            "  Loss = -log(p) = 0.6907\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4988\n",
            "  Current weights (sample): W1[0,0] = 0.000130\n",
            "  Updated weights (sample): W1[0,0] = -0.000430\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.005\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.004\n",
            "  P(chosen > rejected) = 0.5010\n",
            "  Loss = -log(p) = 0.6911\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4990\n",
            "  Current weights (sample): W1[0,0] = -0.000430\n",
            "  Updated weights (sample): W1[0,0] = 0.001020\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = 0.002\n",
            "  Score diff = 0.001\n",
            "  P(chosen > rejected) = 0.5002\n",
            "  Loss = -log(p) = 0.6927\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4998\n",
            "  Current weights (sample): W1[0,0] = 0.001020\n",
            "  Updated weights (sample): W1[0,0] = 0.000373\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5004\n",
            "  Loss = -log(p) = 0.6923\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4996\n",
            "  Current weights (sample): W1[0,0] = 0.000373\n",
            "  Updated weights (sample): W1[0,0] = 0.001231\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.004\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.004\n",
            "  P(chosen > rejected) = 0.5010\n",
            "  Loss = -log(p) = 0.6912\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4990\n",
            "  Current weights (sample): W1[0,0] = 0.001231\n",
            "  Updated weights (sample): W1[0,0] = 0.004682\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.004\n",
            "  r(rejected) = 0.007\n",
            "  Score diff = -0.002\n",
            "  P(chosen > rejected) = 0.4994\n",
            "  Loss = -log(p) = 0.6943\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5006\n",
            "  Current weights (sample): W1[0,0] = 0.004682\n",
            "  Updated weights (sample): W1[0,0] = 0.003484\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.003\n",
            "  P(chosen > rejected) = 0.5007\n",
            "  Loss = -log(p) = 0.6917\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4993\n",
            "  Current weights (sample): W1[0,0] = 0.003484\n",
            "  Updated weights (sample): W1[0,0] = 0.004499\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.013\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = 0.009\n",
            "  P(chosen > rejected) = 0.5023\n",
            "  Loss = -log(p) = 0.6885\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4977\n",
            "  Current weights (sample): W1[0,0] = 0.004499\n",
            "  Updated weights (sample): W1[0,0] = 0.004916\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.000\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.001\n",
            "  P(chosen > rejected) = 0.5003\n",
            "  Loss = -log(p) = 0.6926\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4997\n",
            "  Current weights (sample): W1[0,0] = 0.004916\n",
            "  Updated weights (sample): W1[0,0] = 0.004355\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.015\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.015\n",
            "  P(chosen > rejected) = 0.5038\n",
            "  Loss = -log(p) = 0.6856\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4962\n",
            "  Current weights (sample): W1[0,0] = 0.004355\n",
            "  Updated weights (sample): W1[0,0] = 0.006361\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.020\n",
            "  r(rejected) = 0.010\n",
            "  Score diff = 0.010\n",
            "  P(chosen > rejected) = 0.5025\n",
            "  Loss = -log(p) = 0.6882\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4975\n",
            "  Current weights (sample): W1[0,0] = 0.006361\n",
            "  Updated weights (sample): W1[0,0] = 0.005621\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.012\n",
            "  r(rejected) = -0.000\n",
            "  Score diff = 0.012\n",
            "  P(chosen > rejected) = 0.5031\n",
            "  Loss = -log(p) = 0.6869\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4969\n",
            "  Current weights (sample): W1[0,0] = 0.005621\n",
            "  Updated weights (sample): W1[0,0] = 0.007548\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = -0.000\n",
            "  P(chosen > rejected) = 0.4999\n",
            "  Loss = -log(p) = 0.6934\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5001\n",
            "  Current weights (sample): W1[0,0] = 0.007548\n",
            "  Updated weights (sample): W1[0,0] = 0.007392\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.002\n",
            "  P(chosen > rejected) = 0.5006\n",
            "  Loss = -log(p) = 0.6920\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4994\n",
            "  Current weights (sample): W1[0,0] = 0.007392\n",
            "  Updated weights (sample): W1[0,0] = 0.008559\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.014\n",
            "  r(rejected) = -0.000\n",
            "  Score diff = 0.014\n",
            "  P(chosen > rejected) = 0.5036\n",
            "  Loss = -log(p) = 0.6860\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4964\n",
            "  Current weights (sample): W1[0,0] = 0.008559\n",
            "  Updated weights (sample): W1[0,0] = 0.011125\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.006\n",
            "  r(rejected) = 0.016\n",
            "  Score diff = -0.009\n",
            "  P(chosen > rejected) = 0.4977\n",
            "  Loss = -log(p) = 0.6978\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5023\n",
            "  Current weights (sample): W1[0,0] = 0.011125\n",
            "  Updated weights (sample): W1[0,0] = 0.009432\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.008\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.009\n",
            "  P(chosen > rejected) = 0.5022\n",
            "  Loss = -log(p) = 0.6887\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4978\n",
            "  Current weights (sample): W1[0,0] = 0.009432\n",
            "  Updated weights (sample): W1[0,0] = 0.010832\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.031\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = 0.028\n",
            "  P(chosen > rejected) = 0.5069\n",
            "  Loss = -log(p) = 0.6793\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4931\n",
            "  Current weights (sample): W1[0,0] = 0.010832\n",
            "  Updated weights (sample): W1[0,0] = 0.011409\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.000\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 0.004\n",
            "  P(chosen > rejected) = 0.5009\n",
            "  Loss = -log(p) = 0.6913\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4991\n",
            "  Current weights (sample): W1[0,0] = 0.011409\n",
            "  Updated weights (sample): W1[0,0] = 0.011409\n",
            "\n",
            "Epoch 2: Averaged loss = 0.6889\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.037\n",
            "  r(rejected) = 0.000\n",
            "  Score diff = 0.037\n",
            "  P(chosen > rejected) = 0.5092\n",
            "  Loss = -log(p) = 0.6750\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4908\n",
            "  Current weights (sample): W1[0,0] = 0.011409\n",
            "  Updated weights (sample): W1[0,0] = 0.014305\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.046\n",
            "  r(rejected) = 0.026\n",
            "  Score diff = 0.021\n",
            "  P(chosen > rejected) = 0.5052\n",
            "  Loss = -log(p) = 0.6828\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4948\n",
            "  Current weights (sample): W1[0,0] = 0.014305\n",
            "  Updated weights (sample): W1[0,0] = 0.013184\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.029\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.030\n",
            "  P(chosen > rejected) = 0.5076\n",
            "  Loss = -log(p) = 0.6781\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4924\n",
            "  Current weights (sample): W1[0,0] = 0.013184\n",
            "  Updated weights (sample): W1[0,0] = 0.016043\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = 0.006\n",
            "  Score diff = -0.003\n",
            "  P(chosen > rejected) = 0.4993\n",
            "  Loss = -log(p) = 0.6945\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5007\n",
            "  Current weights (sample): W1[0,0] = 0.016043\n",
            "  Updated weights (sample): W1[0,0] = 0.015805\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = -0.002\n",
            "  Score diff = 0.003\n",
            "  P(chosen > rejected) = 0.5007\n",
            "  Loss = -log(p) = 0.6917\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4993\n",
            "  Current weights (sample): W1[0,0] = 0.015805\n",
            "  Updated weights (sample): W1[0,0] = 0.015805\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.039\n",
            "  r(rejected) = -0.001\n",
            "  Score diff = 0.041\n",
            "  P(chosen > rejected) = 0.5101\n",
            "  Loss = -log(p) = 0.6731\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4899\n",
            "  Current weights (sample): W1[0,0] = 0.015805\n",
            "  Updated weights (sample): W1[0,0] = 0.019729\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.010\n",
            "  r(rejected) = 0.038\n",
            "  Score diff = -0.028\n",
            "  P(chosen > rejected) = 0.4930\n",
            "  Loss = -log(p) = 0.7072\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5070\n",
            "  Current weights (sample): W1[0,0] = 0.019729\n",
            "  Updated weights (sample): W1[0,0] = 0.017056\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.022\n",
            "  r(rejected) = -0.002\n",
            "  Score diff = 0.024\n",
            "  P(chosen > rejected) = 0.5060\n",
            "  Loss = -log(p) = 0.6813\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4940\n",
            "  Current weights (sample): W1[0,0] = 0.017056\n",
            "  Updated weights (sample): W1[0,0] = 0.019231\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.074\n",
            "  r(rejected) = 0.004\n",
            "  Score diff = 0.070\n",
            "  P(chosen > rejected) = 0.5174\n",
            "  Loss = -log(p) = 0.6589\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4826\n",
            "  Current weights (sample): W1[0,0] = 0.019231\n",
            "  Updated weights (sample): W1[0,0] = 0.020118\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.000\n",
            "  r(rejected) = -0.007\n",
            "  Score diff = 0.007\n",
            "  P(chosen > rejected) = 0.5019\n",
            "  Loss = -log(p) = 0.6894\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4981\n",
            "  Current weights (sample): W1[0,0] = 0.020118\n",
            "  Updated weights (sample): W1[0,0] = 0.020118\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.087\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.087\n",
            "  P(chosen > rejected) = 0.5217\n",
            "  Loss = -log(p) = 0.6507\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4783\n",
            "  Current weights (sample): W1[0,0] = 0.020118\n",
            "  Updated weights (sample): W1[0,0] = 0.024483\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.107\n",
            "  r(rejected) = 0.063\n",
            "  Score diff = 0.045\n",
            "  P(chosen > rejected) = 0.5111\n",
            "  Loss = -log(p) = 0.6711\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4889\n",
            "  Current weights (sample): W1[0,0] = 0.024483\n",
            "  Updated weights (sample): W1[0,0] = 0.022738\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.072\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 0.075\n",
            "  P(chosen > rejected) = 0.5187\n",
            "  Loss = -log(p) = 0.6564\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4813\n",
            "  Current weights (sample): W1[0,0] = 0.022738\n",
            "  Updated weights (sample): W1[0,0] = 0.027082\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.004\n",
            "  r(rejected) = 0.010\n",
            "  Score diff = -0.005\n",
            "  P(chosen > rejected) = 0.4986\n",
            "  Loss = -log(p) = 0.6959\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5014\n",
            "  Current weights (sample): W1[0,0] = 0.027082\n",
            "  Updated weights (sample): W1[0,0] = 0.026708\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.001\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 0.004\n",
            "  P(chosen > rejected) = 0.5010\n",
            "  Loss = -log(p) = 0.6911\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4990\n",
            "  Current weights (sample): W1[0,0] = 0.026708\n",
            "  Updated weights (sample): W1[0,0] = 0.026708\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.110\n",
            "  r(rejected) = -0.002\n",
            "  Score diff = 0.112\n",
            "  P(chosen > rejected) = 0.5281\n",
            "  Loss = -log(p) = 0.6385\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4719\n",
            "  Current weights (sample): W1[0,0] = 0.026708\n",
            "  Updated weights (sample): W1[0,0] = 0.032671\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.014\n",
            "  r(rejected) = 0.090\n",
            "  Score diff = -0.075\n",
            "  P(chosen > rejected) = 0.4811\n",
            "  Loss = -log(p) = 0.7316\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5189\n",
            "  Current weights (sample): W1[0,0] = 0.032671\n",
            "  Updated weights (sample): W1[0,0] = 0.028356\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.055\n",
            "  r(rejected) = -0.004\n",
            "  Score diff = 0.059\n",
            "  P(chosen > rejected) = 0.5148\n",
            "  Loss = -log(p) = 0.6641\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4852\n",
            "  Current weights (sample): W1[0,0] = 0.028356\n",
            "  Updated weights (sample): W1[0,0] = 0.031727\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.181\n",
            "  r(rejected) = 0.005\n",
            "  Score diff = 0.176\n",
            "  P(chosen > rejected) = 0.5439\n",
            "  Loss = -log(p) = 0.6090\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4561\n",
            "  Current weights (sample): W1[0,0] = 0.031727\n",
            "  Updated weights (sample): W1[0,0] = 0.033052\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = -0.001\n",
            "  r(rejected) = -0.013\n",
            "  Score diff = 0.012\n",
            "  P(chosen > rejected) = 0.5030\n",
            "  Loss = -log(p) = 0.6871\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4970\n",
            "  Current weights (sample): W1[0,0] = 0.033052\n",
            "  Updated weights (sample): W1[0,0] = 0.033052\n",
            "\n",
            "Epoch 4: Averaged loss = 0.6696\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.199\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.198\n",
            "  P(chosen > rejected) = 0.5493\n",
            "  Loss = -log(p) = 0.5991\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4507\n",
            "  Current weights (sample): W1[0,0] = 0.033052\n",
            "  Updated weights (sample): W1[0,0] = 0.039432\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.261\n",
            "  r(rejected) = 0.158\n",
            "  Score diff = 0.104\n",
            "  P(chosen > rejected) = 0.5259\n",
            "  Loss = -log(p) = 0.6427\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4741\n",
            "  Current weights (sample): W1[0,0] = 0.039432\n",
            "  Updated weights (sample): W1[0,0] = 0.036793\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.167\n",
            "  r(rejected) = -0.006\n",
            "  Score diff = 0.173\n",
            "  P(chosen > rejected) = 0.5431\n",
            "  Loss = -log(p) = 0.6104\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4569\n",
            "  Current weights (sample): W1[0,0] = 0.036793\n",
            "  Updated weights (sample): W1[0,0] = 0.043163\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.006\n",
            "  r(rejected) = 0.019\n",
            "  Score diff = -0.013\n",
            "  P(chosen > rejected) = 0.4968\n",
            "  Loss = -log(p) = 0.6995\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5032\n",
            "  Current weights (sample): W1[0,0] = 0.043163\n",
            "  Updated weights (sample): W1[0,0] = 0.043163\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.003\n",
            "  r(rejected) = -0.004\n",
            "  Score diff = 0.007\n",
            "  P(chosen > rejected) = 0.5018\n",
            "  Loss = -log(p) = 0.6896\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4982\n",
            "  Current weights (sample): W1[0,0] = 0.043163\n",
            "  Updated weights (sample): W1[0,0] = 0.043163\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.281\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 0.284\n",
            "  P(chosen > rejected) = 0.5706\n",
            "  Loss = -log(p) = 0.5611\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4294\n",
            "  Current weights (sample): W1[0,0] = 0.043163\n",
            "  Updated weights (sample): W1[0,0] = 0.051562\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.021\n",
            "  r(rejected) = 0.206\n",
            "  Score diff = -0.185\n",
            "  P(chosen > rejected) = 0.4539\n",
            "  Loss = -log(p) = 0.7899\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5461\n",
            "  Current weights (sample): W1[0,0] = 0.051562\n",
            "  Updated weights (sample): W1[0,0] = 0.044542\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.141\n",
            "  r(rejected) = -0.006\n",
            "  Score diff = 0.147\n",
            "  P(chosen > rejected) = 0.5366\n",
            "  Loss = -log(p) = 0.6225\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4634\n",
            "  Current weights (sample): W1[0,0] = 0.044542\n",
            "  Updated weights (sample): W1[0,0] = 0.049526\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.409\n",
            "  r(rejected) = 0.003\n",
            "  Score diff = 0.406\n",
            "  P(chosen > rejected) = 0.6001\n",
            "  Loss = -log(p) = 0.5107\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3999\n",
            "  Current weights (sample): W1[0,0] = 0.049526\n",
            "  Updated weights (sample): W1[0,0] = 0.051311\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = -0.001\n",
            "  r(rejected) = -0.020\n",
            "  Score diff = 0.019\n",
            "  P(chosen > rejected) = 0.5048\n",
            "  Loss = -log(p) = 0.6835\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4952\n",
            "  Current weights (sample): W1[0,0] = 0.051311\n",
            "  Updated weights (sample): W1[0,0] = 0.051311\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.426\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 0.425\n",
            "  P(chosen > rejected) = 0.6047\n",
            "  Loss = -log(p) = 0.5031\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3953\n",
            "  Current weights (sample): W1[0,0] = 0.051311\n",
            "  Updated weights (sample): W1[0,0] = 0.059768\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.558\n",
            "  r(rejected) = 0.343\n",
            "  Score diff = 0.215\n",
            "  P(chosen > rejected) = 0.5536\n",
            "  Loss = -log(p) = 0.5914\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4464\n",
            "  Current weights (sample): W1[0,0] = 0.059768\n",
            "  Updated weights (sample): W1[0,0] = 0.056039\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.370\n",
            "  r(rejected) = -0.009\n",
            "  Score diff = 0.379\n",
            "  P(chosen > rejected) = 0.5937\n",
            "  Loss = -log(p) = 0.5213\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4063\n",
            "  Current weights (sample): W1[0,0] = 0.056039\n",
            "  Updated weights (sample): W1[0,0] = 0.064500\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.013\n",
            "  r(rejected) = 0.027\n",
            "  Score diff = -0.014\n",
            "  P(chosen > rejected) = 0.4964\n",
            "  Loss = -log(p) = 0.7003\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5036\n",
            "  Current weights (sample): W1[0,0] = 0.064500\n",
            "  Updated weights (sample): W1[0,0] = 0.063641\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.004\n",
            "  r(rejected) = -0.007\n",
            "  Score diff = 0.011\n",
            "  P(chosen > rejected) = 0.5028\n",
            "  Loss = -log(p) = 0.6876\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4972\n",
            "  Current weights (sample): W1[0,0] = 0.063641\n",
            "  Updated weights (sample): W1[0,0] = 0.063641\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.634\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 0.638\n",
            "  P(chosen > rejected) = 0.6543\n",
            "  Loss = -log(p) = 0.4242\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3457\n",
            "  Current weights (sample): W1[0,0] = 0.063641\n",
            "  Updated weights (sample): W1[0,0] = 0.073616\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.036\n",
            "  r(rejected) = 0.409\n",
            "  Score diff = -0.373\n",
            "  P(chosen > rejected) = 0.4079\n",
            "  Loss = -log(p) = 0.8967\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5921\n",
            "  Current weights (sample): W1[0,0] = 0.073616\n",
            "  Updated weights (sample): W1[0,0] = 0.062610\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.289\n",
            "  r(rejected) = -0.009\n",
            "  Score diff = 0.298\n",
            "  P(chosen > rejected) = 0.5740\n",
            "  Loss = -log(p) = 0.5551\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4260\n",
            "  Current weights (sample): W1[0,0] = 0.062610\n",
            "  Updated weights (sample): W1[0,0] = 0.069198\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.857\n",
            "  r(rejected) = 0.002\n",
            "  Score diff = 0.856\n",
            "  P(chosen > rejected) = 0.7017\n",
            "  Loss = -log(p) = 0.3542\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2983\n",
            "  Current weights (sample): W1[0,0] = 0.069198\n",
            "  Updated weights (sample): W1[0,0] = 0.071110\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = -0.001\n",
            "  r(rejected) = -0.032\n",
            "  Score diff = 0.030\n",
            "  P(chosen > rejected) = 0.5076\n",
            "  Loss = -log(p) = 0.6782\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4924\n",
            "  Current weights (sample): W1[0,0] = 0.071110\n",
            "  Updated weights (sample): W1[0,0] = 0.071110\n",
            "\n",
            "Epoch 6: Averaged loss = 0.5912\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.793\n",
            "  r(rejected) = 0.002\n",
            "  Score diff = 0.791\n",
            "  P(chosen > rejected) = 0.6880\n",
            "  Loss = -log(p) = 0.3739\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3120\n",
            "  Current weights (sample): W1[0,0] = 0.071110\n",
            "  Updated weights (sample): W1[0,0] = 0.080378\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.042\n",
            "  r(rejected) = 0.617\n",
            "  Score diff = 0.425\n",
            "  P(chosen > rejected) = 0.6047\n",
            "  Loss = -log(p) = 0.5031\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3953\n",
            "  Current weights (sample): W1[0,0] = 0.080378\n",
            "  Updated weights (sample): W1[0,0] = 0.075864\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.661\n",
            "  r(rejected) = -0.012\n",
            "  Score diff = 0.673\n",
            "  P(chosen > rejected) = 0.6621\n",
            "  Loss = -log(p) = 0.4123\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3379\n",
            "  Current weights (sample): W1[0,0] = 0.075864\n",
            "  Updated weights (sample): W1[0,0] = 0.085418\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.025\n",
            "  r(rejected) = 0.045\n",
            "  Score diff = -0.021\n",
            "  P(chosen > rejected) = 0.4949\n",
            "  Loss = -log(p) = 0.7035\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5051\n",
            "  Current weights (sample): W1[0,0] = 0.085418\n",
            "  Updated weights (sample): W1[0,0] = 0.085418\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.009\n",
            "  r(rejected) = -0.010\n",
            "  Score diff = 0.019\n",
            "  P(chosen > rejected) = 0.5048\n",
            "  Loss = -log(p) = 0.6837\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4952\n",
            "  Current weights (sample): W1[0,0] = 0.085418\n",
            "  Updated weights (sample): W1[0,0] = 0.085418\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.168\n",
            "  r(rejected) = -0.003\n",
            "  Score diff = 1.171\n",
            "  P(chosen > rejected) = 0.7634\n",
            "  Loss = -log(p) = 0.2700\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2366\n",
            "  Current weights (sample): W1[0,0] = 0.085418\n",
            "  Updated weights (sample): W1[0,0] = 0.094630\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.070\n",
            "  r(rejected) = 0.668\n",
            "  Score diff = -0.598\n",
            "  P(chosen > rejected) = 0.3547\n",
            "  Loss = -log(p) = 1.0364\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.6453\n",
            "  Current weights (sample): W1[0,0] = 0.094630\n",
            "  Updated weights (sample): W1[0,0] = 0.078812\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.522\n",
            "  r(rejected) = -0.014\n",
            "  Score diff = 0.537\n",
            "  P(chosen > rejected) = 0.6311\n",
            "  Loss = -log(p) = 0.4603\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3689\n",
            "  Current weights (sample): W1[0,0] = 0.078812\n",
            "  Updated weights (sample): W1[0,0] = 0.086334\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.406\n",
            "  r(rejected) = 0.013\n",
            "  Score diff = 1.394\n",
            "  P(chosen > rejected) = 0.8012\n",
            "  Loss = -log(p) = 0.2217\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.1988\n",
            "  Current weights (sample): W1[0,0] = 0.086334\n",
            "  Updated weights (sample): W1[0,0] = 0.087999\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = -0.001\n",
            "  r(rejected) = -0.051\n",
            "  Score diff = 0.049\n",
            "  P(chosen > rejected) = 0.5123\n",
            "  Loss = -log(p) = 0.6688\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4877\n",
            "  Current weights (sample): W1[0,0] = 0.087999\n",
            "  Updated weights (sample): W1[0,0] = 0.087999\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.184\n",
            "  r(rejected) = 0.004\n",
            "  Score diff = 1.181\n",
            "  P(chosen > rejected) = 0.7651\n",
            "  Loss = -log(p) = 0.2678\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2349\n",
            "  Current weights (sample): W1[0,0] = 0.087999\n",
            "  Updated weights (sample): W1[0,0] = 0.096868\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.553\n",
            "  r(rejected) = 0.877\n",
            "  Score diff = 0.676\n",
            "  P(chosen > rejected) = 0.6629\n",
            "  Loss = -log(p) = 0.4112\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3371\n",
            "  Current weights (sample): W1[0,0] = 0.096868\n",
            "  Updated weights (sample): W1[0,0] = 0.092060\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.009\n",
            "  r(rejected) = -0.023\n",
            "  Score diff = 1.032\n",
            "  P(chosen > rejected) = 0.7373\n",
            "  Loss = -log(p) = 0.3048\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2627\n",
            "  Current weights (sample): W1[0,0] = 0.092060\n",
            "  Updated weights (sample): W1[0,0] = 0.101302\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.047\n",
            "  r(rejected) = 0.058\n",
            "  Score diff = -0.011\n",
            "  P(chosen > rejected) = 0.4971\n",
            "  Loss = -log(p) = 0.6989\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5029\n",
            "  Current weights (sample): W1[0,0] = 0.101302\n",
            "  Updated weights (sample): W1[0,0] = 0.099885\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.016\n",
            "  r(rejected) = -0.012\n",
            "  Score diff = 0.028\n",
            "  P(chosen > rejected) = 0.5071\n",
            "  Loss = -log(p) = 0.6791\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4929\n",
            "  Current weights (sample): W1[0,0] = 0.099885\n",
            "  Updated weights (sample): W1[0,0] = 0.110423\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.828\n",
            "  r(rejected) = -0.002\n",
            "  Score diff = 1.829\n",
            "  P(chosen > rejected) = 0.8617\n",
            "  Loss = -log(p) = 0.1489\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.1383\n",
            "  Current weights (sample): W1[0,0] = 0.110423\n",
            "  Updated weights (sample): W1[0,0] = 0.117005\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.141\n",
            "  r(rejected) = 0.843\n",
            "  Score diff = -0.702\n",
            "  P(chosen > rejected) = 0.3314\n",
            "  Loss = -log(p) = 1.1044\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.6686\n",
            "  Current weights (sample): W1[0,0] = 0.117005\n",
            "  Updated weights (sample): W1[0,0] = 0.097529\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.747\n",
            "  r(rejected) = -0.022\n",
            "  Score diff = 0.769\n",
            "  P(chosen > rejected) = 0.6834\n",
            "  Loss = -log(p) = 0.3807\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.3166\n",
            "  Current weights (sample): W1[0,0] = 0.097529\n",
            "  Updated weights (sample): W1[0,0] = 0.105209\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 2.003\n",
            "  r(rejected) = 0.021\n",
            "  Score diff = 1.982\n",
            "  P(chosen > rejected) = 0.8789\n",
            "  Loss = -log(p) = 0.1291\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.1211\n",
            "  Current weights (sample): W1[0,0] = 0.105209\n",
            "  Updated weights (sample): W1[0,0] = 0.106411\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.007\n",
            "  r(rejected) = -0.082\n",
            "  Score diff = 0.088\n",
            "  P(chosen > rejected) = 0.5221\n",
            "  Loss = -log(p) = 0.6500\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4779\n",
            "  Current weights (sample): W1[0,0] = 0.106411\n",
            "  Updated weights (sample): W1[0,0] = 0.106411\n",
            "\n",
            "Epoch 8: Averaged loss = 0.4775\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            " -0.55690559 -2.28532798 -0.27494638  0.20126645] \n",
            "\n",
            "After:  [[ 1.62491218  0.92315795  0.53031276  1.58162962 -1.0042141  -0.95377025\n",
            "  -0.55690559 -2.28532798 -0.27494638  0.20126645]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "  0.25830647  0.54108781 -0.65459968  0.04505877] \n",
            "\n",
            "After:  [[ 0.26785229 -0.3129085  -0.36776867 -0.28597712  0.3112508  -0.0218578\n",
            "   0.25830647  0.54108781 -0.65459968  0.04505877]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.512\n",
            "  r(rejected) = 0.005\n",
            "  Score diff = 1.507\n",
            "  P(chosen > rejected) = 0.8185\n",
            "  Loss = -log(p) = 0.2002\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.1815\n",
            "  Current weights (sample): W1[0,0] = 0.106411\n",
            "  Updated weights (sample): W1[0,0] = 0.114345\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            " -0.41805425 -0.5572201   0.81265314 -1.04971737] \n",
            "\n",
            "After:  [[ 0.20017639 -1.33214727  1.44218374  1.29528049 -1.89138294 -0.96201035\n",
            "  -0.41805425 -0.5572201   0.81265314 -1.04971737]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "  1.44111905 -1.67277562  0.96304157  0.24261253] \n",
            "\n",
            "After:  [[ 0.78508244 -1.25626082  0.65514253  0.3254278  -0.91755604 -0.61546783\n",
            "   1.44111905 -1.67277562  0.96304157  0.24261253]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 2.064\n",
            "  r(rejected) = 1.060\n",
            "  Score diff = 1.004\n",
            "  P(chosen > rejected) = 0.7319\n",
            "  Loss = -log(p) = 0.3121\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2681\n",
            "  Current weights (sample): W1[0,0] = 0.114345\n",
            "  Updated weights (sample): W1[0,0] = 0.109965\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            " -0.71102849 -2.0900573   0.19134146 -0.58275548] \n",
            "\n",
            "After:  [[ 1.39276033  0.43472745  0.46924145  0.14483537  0.31223059  0.26187136\n",
            "  -0.71102849 -2.0900573   0.19134146 -0.58275548]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "  0.63056061  0.64714147 -0.38900963  0.6318091 ] \n",
            "\n",
            "After:  [[ 0.25151592 -0.04370535  0.75932124  0.5493445  -1.15384891  1.92076688\n",
            "   0.63056061  0.64714147 -0.38900963  0.6318091 ]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 1.245\n",
            "  r(rejected) = -0.042\n",
            "  Score diff = 1.287\n",
            "  P(chosen > rejected) = 0.7836\n",
            "  Loss = -log(p) = 0.2439\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2164\n",
            "  Current weights (sample): W1[0,0] = 0.109965\n",
            "  Updated weights (sample): W1[0,0] = 0.118622\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            " -0.61148826  2.22199399  0.5625748   0.42105499] \n",
            "\n",
            "After:  [[-0.51224073  0.45909567 -1.02634711  0.64671385 -1.46206098 -0.05817139\n",
            "  -0.61148826  2.22199399  0.5625748   0.42105499]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "  0.72633818  1.11406721 -0.02497841  0.06822356] \n",
            "\n",
            "After:  [[ 0.10715835 -0.29185254 -0.25894523  0.72036533 -1.06194133 -0.89167624\n",
            "   0.72633818  1.11406721 -0.02497841  0.06822356]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.074\n",
            "  r(rejected) = 0.075\n",
            "  Score diff = -0.001\n",
            "  P(chosen > rejected) = 0.4998\n",
            "  Loss = -log(p) = 0.6935\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.5002\n",
            "  Current weights (sample): W1[0,0] = 0.118622\n",
            "  Updated weights (sample): W1[0,0] = 0.118622\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "  0.13099955  0.84596715 -1.11293778 -0.8733637 ] \n",
            "\n",
            "After:  [[-0.06402577  0.02553499  0.29359704 -2.38833894 -0.38613134  1.11875496\n",
            "   0.13099955  0.84596715 -1.11293778 -0.8733637 ]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            " -0.2809433   0.07378622  0.4768065   0.35477045] \n",
            "\n",
            "After:  [[-0.8137864   0.01659428  0.37853122  0.75095315  0.79625922  1.03601698\n",
            "  -0.2809433   0.07378622  0.4768065   0.35477045]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.027\n",
            "  r(rejected) = -0.020\n",
            "  Score diff = 0.046\n",
            "  P(chosen > rejected) = 0.5116\n",
            "  Loss = -log(p) = 0.6702\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4884\n",
            "  Current weights (sample): W1[0,0] = 0.118622\n",
            "  Updated weights (sample): W1[0,0] = 0.118622\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "  1.47851034  0.63157177 -0.17960814 -0.70075718] \n",
            "\n",
            "After:  [[ 1.81553984 -2.16844383  1.00342938  2.81226472 -0.06434218  1.70674508\n",
            "   1.47851034  0.63157177 -0.17960814 -0.70075718]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "  0.05609987  0.17504617 -1.43088834 -1.25898146] \n",
            "\n",
            "After:  [[-1.52661062  0.21919445 -1.92713675 -0.19518354 -0.55173495 -0.23125031\n",
            "   0.05609987  0.17504617 -1.43088834 -1.25898146]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 2.376\n",
            "  r(rejected) = 0.001\n",
            "  Score diff = 2.375\n",
            "  P(chosen > rejected) = 0.9149\n",
            "  Loss = -log(p) = 0.0889\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.0851\n",
            "  Current weights (sample): W1[0,0] = 0.118622\n",
            "  Updated weights (sample): W1[0,0] = 0.123213\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            " -0.59957679  2.83437047 -0.38580048 -0.25400343] \n",
            "\n",
            "After:  [[-0.14023031  0.08788584  1.05685519 -0.89059133 -0.19458747 -0.24586133\n",
            "  -0.59957679  2.83437047 -0.38580048 -0.25400343]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            " -0.70621408 -1.43851792  0.92034917 -0.62254753] \n",
            "\n",
            "After:  [[ 1.07030194  0.59713246 -0.38644295  0.77415443 -0.50908439  0.0614311\n",
            "  -0.70621408 -1.43851792  0.92034917 -0.62254753]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.256\n",
            "  r(rejected) = 0.890\n",
            "  Score diff = -0.634\n",
            "  P(chosen > rejected) = 0.3467\n",
            "  Loss = -log(p) = 1.0594\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.6533\n",
            "  Current weights (sample): W1[0,0] = 0.123213\n",
            "  Updated weights (sample): W1[0,0] = 0.101918\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "  0.0536968  -0.76596438  0.79112605  0.12590914] \n",
            "\n",
            "After:  [[ 0.97273287 -0.46108079  1.12906728 -0.06671182  1.18509053  0.20479794\n",
            "   0.0536968  -0.76596438  0.79112605  0.12590914]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            " -0.25583206  0.22588403  0.63457982  0.90687014] \n",
            "\n",
            "After:  [[ 0.82179766  0.87718759  0.10206639 -0.21269439 -0.09264043  0.32204823\n",
            "  -0.25583206  0.22588403  0.63457982  0.90687014]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.941\n",
            "  r(rejected) = -0.032\n",
            "  Score diff = 0.973\n",
            "  P(chosen > rejected) = 0.7258\n",
            "  Loss = -log(p) = 0.3205\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.2742\n",
            "  Current weights (sample): W1[0,0] = 0.101918\n",
            "  Updated weights (sample): W1[0,0] = 0.109472\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "  0.36876758 -1.63447008 -0.08595339 -1.86177777] \n",
            "\n",
            "After:  [[ 0.38256795 -1.4499017   0.4452052   1.40458811 -1.2753615  -2.55361458\n",
            "   0.36876758 -1.63447008 -0.08595339 -1.86177777]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            " -1.38232802  0.01009965 -2.1117601   0.02416287] \n",
            "\n",
            "After:  [[ 0.40729438  0.50805828 -0.39212889 -0.67593623 -1.12581284 -0.51199109\n",
            "  -1.38232802  0.01009965 -2.1117601   0.02416287]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 2.542\n",
            "  r(rejected) = 0.083\n",
            "  Score diff = 2.459\n",
            "  P(chosen > rejected) = 0.9212\n",
            "  Loss = -log(p) = 0.0820\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.0788\n",
            "  Current weights (sample): W1[0,0] = 0.109472\n",
            "  Updated weights (sample): W1[0,0] = 0.110354\n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "  0.45939632 -0.66112186  0.21484014  1.60470862] \n",
            "\n",
            "After:  [[-0.36971355  2.36339562 -0.64913703  0.9769219   0.94998297 -0.74938989\n",
            "   0.45939632 -0.66112186  0.21484014  1.60470862]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "  1.2951867   0.90724145  1.93951077  0.63152977] \n",
            "\n",
            "After:  [[ 0.44182871 -1.13279922  0.65749421 -0.84972844 -0.58068822  1.30069211\n",
            "   1.2951867   0.90724145  1.93951077  0.63152977]] \n",
            "\n",
            "\n",
            "Bradley-Terry Loss Computation:\n",
            "  r(chosen) = 0.012\n",
            "  r(rejected) = -0.130\n",
            "  Score diff = 0.143\n",
            "  P(chosen > rejected) = 0.5356\n",
            "  Loss = -log(p) = 0.6244\n",
            "\n",
            "Gradient computation:\n",
            "  dL/d(score_diff) = -0.4644\n",
            "  Current weights (sample): W1[0,0] = 0.110354\n",
            "  Updated weights (sample): W1[0,0] = 0.110354\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Testing learned reward model:\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.32993526 -2.1310869   1.0903194  -1.23307501  0.38288955  0.82302975\n",
            "  0.1664542   0.62309036 -0.23288223 -0.32323017] \n",
            "\n",
            "After:  [[-0.32993526 -2.1310869   1.0903194  -1.23307501  0.38288955  0.82302975\n",
            "   0.1664542   0.62309036 -0.23288223 -0.32323017]] \n",
            "\n",
            "\n",
            "1-dimensional input. Reshaping:\n",
            "\n",
            "Before:  [-0.1249494   1.31814691  0.11313088 -1.51601016 -0.02034929 -1.63821918\n",
            " -0.98396953 -0.28418717 -0.12682196 -0.64502806] \n",
            "\n",
            "After:  [[-0.1249494   1.31814691  0.11313088 -1.51601016 -0.02034929 -1.63821918\n",
            "  -0.98396953 -0.28418717 -0.12682196 -0.64502806]] \n",
            "\n",
            "Reward(chosen): 0.6686\n",
            "Reward(rejected): 0.0824\n",
            "Correctly ranked: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZkyfzzZ9Zrst"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}