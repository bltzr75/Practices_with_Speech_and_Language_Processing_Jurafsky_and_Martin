{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Setup"
      ],
      "metadata": {
        "id": "_r4BdmnTJWkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate # HF evaluate lib for metrics\n",
        "!pip install bitsandbytes # 8 bit Adam optimizer\n",
        "!pip install trl # Transformer Reinforcement Learning\n",
        "!pip install sacrebleu  # BLEU (Bilingual Evaluation Understudy)  metric dependency\n",
        "!pip install rouge_score  # BLEU (Bilingual Evaluation Understudy)  metric dependency\n",
        "# BLEU (Bilingual Evaluation Understudy) is a metric for evaluating machine-generated text quality, especially in translation and text generation tasks\n",
        "# BLEU evaluates text generation quality by counting matching n-grams (1-4 word sequences) between generated and reference text, applying a brevity penalty, and scoring 0-1 where 0.6+ is excellent, 0.4-0.6 is good, 0.2-0.4 is fair, and below 0.2 is poor. For example, comparing \"The cat sits on the mat\" to reference \"The cat is on the mat\" yields ~0.7 (good) based on overlapping words and phrases. It's widely used for machine translation, summarization, and instruction tuning evaluation, but has limitations since it favors literal matches over semantic meaning and can miss quality paraphrases that use different vocabulary."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_E5wkLfLOJQ",
        "outputId": "ad3ac8dd-1243-4ab4-f4ac-705ecaef9523"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.56.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl) (0.22.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7e1705be907f016ade334e71818df95df706ee2b82409f287516dade8227490d\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3ip6ZQeHxL4",
        "outputId": "8e93d6dc-352f-4c80-a9e6-2d9daf7162f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace libraries available with enhancements\n",
            "TRL (Transformer Reinforcement Learning) not available\n",
            "Using device: cpu\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import subprocess\n",
        "import sys\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict, ValidationError, field_serializer\n",
        "from typing import Optional\n",
        "import evaluate # HF evaluate lib for metrics\n",
        "import wandb\n",
        "\n",
        "try:\n",
        "\n",
        "  from transformers import(\n",
        "      AutoModelForCausalLM, # pretrained models\n",
        "      AutoTokenizer,\n",
        "      AutoModelForSequenceClassification, # For reward models\n",
        "      Trainer,  # HF training loop\n",
        "      TrainingArguments,  # Training hyperparams\n",
        "      DataCollatorForLanguageModeling, # BAtch collation for LM trainning\n",
        "      BitsAndBytesConfig, # For 8bit quantization\n",
        "      )\n",
        "\n",
        "  from datasets import Dataset as HFDataset, load_dataset\n",
        "  from peft import(\n",
        "      LoraConfig,\n",
        "      get_peft_model, # convert model to peft model\n",
        "      TaskType, # task type enum\n",
        "      PeftModel, # peft model class\n",
        "      prepare_model_for_kbit_training # prepare quantized model for training\n",
        "  )\n",
        "\n",
        "  from accelerate import Accelerator # Distributed training support\n",
        "  from accelerate.utils import set_seed # seed setting for accelerate\n",
        "  import bitsandbytes as bnb # 8bit adam optimizer\n",
        "\n",
        "  HF_AVAILABLE = True\n",
        "  print(\"HuggingFace libraries available with enhancements\")\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  HF_AVAILABLE = False\n",
        "  print(\"HuggingFace libraries not available - install with: pip install transformers datasets peft accelerate bitsandbytes\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  from trl import (\n",
        "      SFTTrainer, # supervised fine tuning trainer\n",
        "      DPOTrainer, # direct preference optimization trainer\n",
        "      RewardTrainer, # reward model trainer\n",
        "      DataCollatorForCompletionOnlyLM # Collator for instruction tuning\n",
        "  )\n",
        "  TRL_AVAILABLE = True\n",
        "  print(\"TRL (Transformer Reinforcement Learning) available\")\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  TRL_AVAILABLE = False\n",
        "  print(\"TRL (Transformer Reinforcement Learning) not available\")\n",
        "\n",
        "\n",
        "\n",
        "# Init wandb for experiment tracking\n",
        "USE_WANDB = False\n",
        "\n",
        "if USE_WANDB:\n",
        "  wandb.init(\n",
        "      project='llm-alignment-textbook',\n",
        "      config={ # Hyperparams\n",
        "               'achitecture': \"GPT-2\",\n",
        "               'dataset':\"custom-instructions\",\n",
        "               'learning_rate':5e-5,\n",
        "      },\n",
        "      model='online' # offline for local logging instead\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(42) # CUDA seeds for GPU operations\n",
        "\n",
        "# Device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data structures for Instruction Tuning and Preferences using Pydantic\n",
        "\n",
        "Key changes from Pydantic v1 to v2:\n",
        "\n",
        "@validator -> @field_validator with @classmethod\n",
        "\n",
        "Config class -> model_config = ConfigDict(...)\n",
        "\n",
        "values parameter -> info parameter with info.data\n",
        "\n",
        ".dict() -> .model_dump()\n",
        "\n",
        ".json() -> .model_dump_json()\n",
        "\n",
        "New @field_serializer decorator for custom serialization\n",
        "\n",
        ".schema() -> .model_json_schema()\n",
        "\n",
        "Built-in str_strip_whitespace config option\n"
      ],
      "metadata": {
        "id": "ptTSWZce9fbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic V2 for validation\n",
        "class InstructionExample(BaseModel):\n",
        "  \"\"\"Single instruction-rsesponse pair for supervised fine-tuning with validation\"\"\"\n",
        "\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid', # Forbids extra fields not defined in the model\n",
        "      validate_assignment=True, # Validate on assignment\n",
        "      str_strip_whitespace=True, #Automatically stirp whitespace from strings\n",
        "  )\n",
        "\n",
        "  instruction: str = Field(..., min_length=1, description=\"The task instruction/prompt\")  # Required field with min length\n",
        "  input: Optional[str] = Field(default=\"\", description=\"Optional input context\") # Optional with default empty string\n",
        "  output: str = Field(..., min_length=1, description=\"Expected response/completion\")   # Required field\n",
        "\n",
        "  @field_validator('instruction', 'output') # fields to validate\n",
        "  @classmethod # Convert a function to a class method\n",
        "  def non_empty_string(cls, v:str) -> str: # class method for validation\n",
        "    \"\"\"Validate that instruction and output are non-empty after stripping\"\"\"\n",
        "    if not v.strip(): # check if empty after removing whitespaces\n",
        "      raise ValueError(\"Field cannot be empty or whitespace only\")\n",
        "    return v.strip()  # Strip value\n",
        "\n",
        "  def format_prompt(self) -> str:\n",
        "    \"\"\"Format as prompt for model input\"\"\"\n",
        "    if self.input: # if there is additional context\n",
        "      return f\"### Instruction:\\n{self.instruction}\\n\\n### Input:\\n{self.input}\\n\\n### Response:\\n\"\n",
        "    else: # just instruction otherwise\n",
        "      return f\"### Instruction:\\n{self.instruction}\\n\\n### Response:\\n:\"\n",
        "\n",
        "  def format_full(self) -> str:\n",
        "    \"\"\"Format complete example with response\"\"\"\n",
        "\n",
        "    return self.format_prompt() + self.output\n",
        "\n",
        "\n",
        "class PreferencePair(BaseModel):\n",
        "  \"\"\"Preference pair for alignment training with validation\"\"\"\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid',\n",
        "      validate_assignment=True, # Validate when fields are reassigned\n",
        "      str_strip_whitespace=True, #Auto-strip spaces\n",
        "  )\n",
        "\n",
        "  prompt: str = Field(..., min_length=1, description=\"Input prompt/instruction\") # required prompt\n",
        "  chosen: str = Field(..., min_length=1, description=\"Preferred response\") # Preferred completion\n",
        "  rejected: str=Field(..., min_length=1, description=\"Rejected response\")  # Rejected completion\n",
        "\n",
        "  @field_validator('prompt', 'chosen', 'rejected')\n",
        "  @classmethod\n",
        "  def clean_text(cls, v:str) -> str:\n",
        "    \"\"\"Clean and validate text fields\"\"\"\n",
        "    cleaned = v.strip() # remove leading/trailing whitespace\n",
        "    if not cleaned: # if empty\n",
        "      raise ValueError('Field cannot be empty')\n",
        "    return cleaned\n",
        "\n",
        "  @field_validator('rejected')\n",
        "  @classmethod\n",
        "  def different_from_chosen(cls, v: str, info) -> str:  # Pydantic v2 uses 'info' instead of 'values'\n",
        "    \"\"\"Ensure chosen and rejected are different\"\"\"\n",
        "    if 'chosen' in info.data and v==info.data['chosen']: # Access other fields via info.data\n",
        "      raise ValueError('Chosen and rejected responses must be different')\n",
        "    return v\n",
        "\n",
        "\n",
        "# Create sample instruction tuning data with validation\n",
        "def create_instruction_dataset() -> List[InstructionExample]:\n",
        "    \"\"\"Create diverse instruction examples covering multiple task types\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    # Using Pydantic v2 models with automatic validation\n",
        "    example_data = [\n",
        "        {\n",
        "            \"instruction\": \"Translate the following English text to French\",\n",
        "            \"input\": \"The quick brown fox jumps over the lazy dog\",\n",
        "            \"output\": \"Le renard brun rapide saute par-dessus le chien paresseux\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Summarize this text in one sentence\",\n",
        "            \"input\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing programs that can access data and use it to learn for themselves.\",\n",
        "            \"output\": \"Machine learning is an AI technique that allows systems to automatically learn and improve from data without explicit programming.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Answer the following question\",\n",
        "            \"input\": \"What is the capital of France?\",\n",
        "            \"output\": \"The capital of France is Paris.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Write a Python function to calculate factorial\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Classify the sentiment of this review as positive or negative\",\n",
        "            \"input\": \"This movie was absolutely terrible. I want my money back.\",\n",
        "            \"output\": \"negative\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    for data in example_data:\n",
        "        try:\n",
        "            example = InstructionExample(**data)  # Create with validation using Pydantic v2\n",
        "            examples.append(example)\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error: {e}\")  # Log validation errors with Pydantic v2 error format\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "# Create sample preference data with validation\n",
        "def create_preference_dataset() -> List[PreferencePair]:\n",
        "  \"\"\"Create preference pairs for alignment with validation\"\"\"\n",
        "  pairs = []\n",
        "\n",
        "  preference_data = [\n",
        "    {\n",
        "        \"prompt\": \"How do I make a bomb?\",\n",
        "        \"chosen\": \"I cannot and will not provide instructions for creating explosives or weapons. This could cause serious harm. If you're interested in chemistry, I'd be happy to discuss safe educational experiments instead.\",\n",
        "        \"rejected\": \"Here's how to make a bomb: First, you need to gather explosive materials...\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain quantum computing to a 5-year-old\",\n",
        "        \"chosen\": \"Imagine you have a magical coin that can be both heads AND tails at the same time until you look at it! Quantum computers use tiny particles that work like these magical coins to solve really hard puzzles super fast.\",\n",
        "        \"rejected\": \"Quantum computing leverages quantum mechanical phenomena such as superposition and entanglement to perform computations using qubits which exist in probabilistic states described by complex amplitudes in Hilbert space.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What's the best programming language?\",\n",
        "        \"chosen\": \"There's no single 'best' programming language - it depends on your goals. Python is great for beginners and data science, JavaScript for web development, C++ for systems programming, and each has its strengths for different use cases.\",\n",
        "        \"rejected\": \"Python is objectively the best programming language and everyone should use it exclusively.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "  for data in preference_data:\n",
        "    try:\n",
        "      pair = PreferencePair(**data)  # Create with automatic validation using Pydantic v2\n",
        "      pairs.append(pair)\n",
        "    except ValidationError as e:\n",
        "      print(f\"Validation error in preference pair: {e}\")  # Log any validation errors with v2 format\n",
        "\n",
        "  return pairs\n",
        "\n",
        "\n",
        "class EnhancedInstructionExample(InstructionExample):\n",
        "    \"\"\"Enhanced version showing Pydantic v2 features\"\"\"\n",
        "\n",
        "    @field_serializer('output')\n",
        "    def serialize_output(self, value: str) -> str:\n",
        "        \"\"\"Custom serialization for output field (Pydantic v2 feature)\"\"\"\n",
        "        return value[:100] + \"...\" if len(value) > 100 else value  # Truncate long outputs in serialization\n",
        "\n",
        "    def model_dump_json(self, **kwargs) -> str:  # v2 replacement for .json()\n",
        "        \"\"\"Serialize to JSON using Pydantic v2 method\"\"\"\n",
        "        return super().model_dump_json(indent=2, **kwargs)  # Use model_dump_json instead of json()\n",
        "\n",
        "# Visualize the data\n",
        "instruction_data = create_instruction_dataset()  # Create instruction examples with validation\n",
        "preference_data = create_preference_dataset()  # Create preference pairs with validation\n",
        "\n",
        "print(f\"Created {len(instruction_data)} instruction examples\")\n",
        "print(f\"Created {len(preference_data)} preference pairs\\n\")\n",
        "\n",
        "print(\"Sample Instruction Example:\")\n",
        "print(\"-\" * 50)\n",
        "example = instruction_data[0]\n",
        "print(f\"Instruction: {example.instruction}\")\n",
        "print(f\"Input: {example.input}\")\n",
        "print(f\"Output: {example.output}\")\n",
        "print(f\"\\nFormatted prompt:\\n{example.format_prompt()}\")\n",
        "\n",
        "# Demonstrate Pydantic v2 model_dump (replacement for dict())\n",
        "print(\"\\nPydantic v2 model_dump:\")\n",
        "print(example.model_dump(exclude={'input'}, mode='json'))  # v2 method with mode parameter\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Sample Preference Pair:\")\n",
        "print(\"-\" * 50)\n",
        "pref = preference_data[0]\n",
        "print(f\"Prompt: {pref.prompt}\")\n",
        "print(f\"Chosen: {pref.chosen[:100]}...\")\n",
        "print(f\"Rejected: {pref.rejected[:50]}...\")\n",
        "\n",
        "# Show Pydantic v2 JSON serialization\n",
        "print(\"\\nPydantic v2 JSON Schema:\")\n",
        "print(PreferencePair.model_json_schema())  # v2 method for getting JSON schema"
      ],
      "metadata": {
        "id": "yY7K9E9jQSIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Instruction Tuning Implementation"
      ],
      "metadata": {
        "id": "324itrnpG_ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaInstructionTuning:\n",
        "  \"\"\"Simple instruction tuning using standard language modeling loss\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size: int, hidden_size: int, learning_rate: float=1e-3):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # simple embedding and output layers\n",
        "    self.embedding = np.random.randn(vocab_size,hidden_size) *0.01\n",
        "    self.output_layer = np.random.randn(hidden_size, vocab_size )* 0.01\n",
        "\n",
        "    print(f\"Initialized instruction tuning model:\")\n",
        "    print(f\"  Embedding matrix: {self.embedding.shape} = [{vocab_size}, {hidden_size}]\")\n",
        "    print(f\"  Output layer: {self.output_layer.shape} = [{hidden_size}, {vocab_size}]\")\n",
        "\n",
        "  def forward(self, input_ids: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Forward pass\n",
        "    input_ids: [batch_size, seq_len] token indices\n",
        "    Returns: logits [batch_size, seq_len, vocab_size], hidden_states [batch_size, seq_len, hidden_size ]\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "\n",
        "    # Embedding lookup [B,L] -> [B, L, H]\n",
        "    hidden_states = self.embedding[input_ids] # index to embedding mtrix\n",
        "    print(f\"\\nEmbedding lookup: {input_ids.shape} -> {hidden_states.shape}\")\n",
        "    print(f\"  Sample embeddings: {hidden_states[0, 0, :5]}...\")  # First 5 dims of first token\n",
        "\n",
        "    # output projection [B, L, H] @ [H, V] = [B, L , V]\n",
        "    hidden_flat = hidden_states.reshape(-1, self.hidden_size) # reshape for matmul [B*L, H]\n",
        "    logits_flat = hidden_flat @ self.output_layer # [B*L, V] matmul\n",
        "    logits = logits_flat.reshape(batch_size, seq_len, self.vocab_size) #[B, L, V]\n",
        "\n",
        "    print(f\"Output projection: {hidden_states.shape} @ {self.output_layer.shape} = {logits.shape}\")\n",
        "    print(f\"  Logits range: [{logits.min():.3f}, {logits.max():.3f}]\")\n",
        "\n",
        "    return logits, hidden_states\n",
        "\n",
        "\n",
        "  def compute_loss(self, logits: np.ndarray, targets:np.ndarray, mask: Optional[np.ndarray] = None) -> float:\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss\n",
        "    logits: [batch_size, seq_len, vocab_size] predicted scores\n",
        "    targets: [batch_size, seq_len] true token indices\n",
        "    mask: [batch_size, seq_len] binary mask for valid positions\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "    # softmax: exp(x) / sum(exp(x))\n",
        "    logits_max = np.max(logits, axis=-1, keepdims=True )  # [B, L, 1] for numerical stability\n",
        "    exp_logits = np.exp(logits - logits_max) # remove max over all to avoid overflow\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)  # [B, L, V] normalized probs\n",
        "\n",
        "    # Extract probabilities of true tokens\n",
        "    batch_indices = np.arange(batch_size)[:, None] # [B, 1] for indexing. The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1).\n",
        "    seq_indices = np.arange(seq_len)[None, :] # The None here is NumPy’s way of adding a new axis (same as np.newaxis). Takes shape (B,) and reshapes into (B, 1)\n",
        "    target_probs = probs[batch_indices, seq_indices, targets] # [B, L] real probs\n",
        "\n",
        "    # Neg Log likelihood: -log(P(correct_token))\n",
        "    loss_matrix = -np.log(target_probs + 1e-10) # add small value to avoid log(0)\n",
        "\n",
        "    if mask is not None:\n",
        "      loss_matrix = loss_matrix * mask # zero out padded positions\n",
        "      total_loss = np.sum(loss_matrix) / np.sum(mask) # avg over only valid tokens\n",
        "    else:\n",
        "      total_loss = np.mean(loss_matrix) # avg over all the tokens\n",
        "\n",
        "\n",
        "    print(f\"\\nLoss computation:\")\n",
        "    print(f\"  Target probs shape: {target_probs.shape}\")\n",
        "    print(f\"  Sample target probs: {target_probs[0, :5]}\")  # First 5 positions\n",
        "    print(f\"  Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "  def train_step(self, input_ids: np.ndarray, targets:np.ndarray) -> float:\n",
        "    \"\"\"Single training step with gradietn descent\"\"\"\n",
        "    # Forward pass\n",
        "    logits, hidden_states = self.forward(input_ids)\n",
        "    loss = self.compute_loss(logits, targets)\n",
        "\n",
        "    # simpified gradient update\n",
        "    self.embedding    *= (1 - self.learning_rate * 0.01) # simple weight decay\n",
        "    self.output_layer *= (1 - self.learning_rate * 0.01) # simple weight decay\n",
        "\n",
        "    return loss\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EZYmAGxSHCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test vanilla instruction tuning"
      ],
      "metadata": {
        "id": "_uNSTslmP-r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VanillaInstructionTuning(vocab_size=1000, hidden_size=64)\n",
        "\n",
        "# Create dummy data\n",
        "batch_size, seq_len = 2, 10\n",
        "input_ids = np.random.randint(0, 1000, (batch_size, seq_len))  # Random token indices\n",
        "targets = np.random.randint(0, 1000, (batch_size, seq_len))  # Target tokens (shifted in practice)\n",
        "\n",
        "print(f\"Input shape: {input_ids.shape}\")\n",
        "print(f\"Target shape: {targets.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "logits, hidden = model.forward(input_ids)\n",
        "\n",
        "# Compute loss\n",
        "loss = model.compute_loss(logits, targets)\n",
        "\n",
        "# Training step\n",
        "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
        "print(\"Training step:\")\n",
        "train_loss = model.train_step(input_ids, targets)\n",
        "print(f\"Training loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "o4DgYbmdI7L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Instruction Tuning with HF"
      ],
      "metadata": {
        "id": "xoPhvJowQm0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionTuningDataset(Dataset):\n",
        " \"\"\"PyTorch dataset for instruction tuning with validation\"\"\"\n",
        "\n",
        " def __init__(self, examples:List[InstructionExample], tokenizer, max_length: int = 512 ):\n",
        "   self.examples = examples\n",
        "   self.tokenizer = tokenizer\n",
        "   self.max_length = max_length\n",
        "\n",
        " def __len__(self):\n",
        "   return len(self.examples )\n",
        "\n",
        " def __getitem__(self, idx):\n",
        "   example = self.examples[idx]\n",
        "\n",
        "   # Format as full text (instruction + response)\n",
        "   text = example.format_full() # pydantic models method\n",
        "\n",
        "   # tokenize with attn mask\n",
        "   encoding = self.tokenizer(\n",
        "       text,\n",
        "       truncation=True, # trunc to max_length\n",
        "       padding='max_length', # pad to max length\n",
        "       max_length=self.max_length,\n",
        "       return_tensors='pt'\n",
        "\n",
        "   )\n",
        "\n",
        "   encoding['labels'] = encoding['input_ids'].clone() # copy input_ids as labels\n",
        "\n",
        "   # Mask padding tokens (100 is ignore index in Cross Entropy Loss)\n",
        "   encoding['labels'][encoding['attention_mask'] == 0] = -100 # ignore padding in loss\n",
        "\n",
        "   return {k:v.squeeze(0) for k,v in encoding.items()}\n",
        "\n",
        "def setup_instruction_tuning_pytorch_enhanced():\n",
        " \"\"\"Setup instruction tuning with PyTorch, HuggingFace, and PEFT for efficiency\"\"\"\n",
        "\n",
        " # inicialtize accelerator\n",
        " accelerator = Accelerator(\n",
        "     mixed_precision = 'fp16' if torch.cuda.is_available() else None,\n",
        "     gradient_accumulation_steps=4 # accum gradients for larger effective batch size\n",
        " )\n",
        "\n",
        " # load model with opt 8-bit quant\n",
        " model_name = 'gpt2'\n",
        " print(f\"Loading model: {model_name}\")\n",
        "\n",
        " # 8-bit quantization if gpu\n",
        " quantization_config = None\n",
        " if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:\n",
        "   quantization_config = BitsAndBytesConfig(\n",
        "       load_in_8bit = True,  # load in 8bit\n",
        "       bnb_8bit_compute_dtype=torch.float16, # compute in fp16\n",
        "       bnb_8bit_quant_type=\"nf8\", # quantization type\n",
        "       bnb_8bit_use_double_quant=True, # double quantization for more saved mem. Quantizes also the Quantization Metadata:the scaling factors (and sometimes offsets/zero-points) that tell you how to map the stored int8 values back to approximate floating-point weights.\n",
        "   )\n",
        "   print(\"Using 8-bit quantization for memory efficiency\")\n",
        "\n",
        " # load tokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        " tokenizer.pad_token = tokenizer.eos_token # set padding tkn\n",
        " tokenizer.padding_side = 'right' # pad on the right for Causal LM\n",
        "\n",
        " # load model with opt quantization\n",
        " if quantization_config:\n",
        "   model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_name,\n",
        "       quantization_config=quantization_config,\n",
        "       device_map=\"Auto\"\n",
        "   )\n",
        "\n",
        "   # prepare model for k-bit training\n",
        "   model = prepare_model_for_kbit_training(model) # `prepare_model_for_kbit_training` modifies a model's weights and layers to be compatible with low-bit (8-bit or 4-bit) training, enabling memory-efficient fine-tuning.\n",
        "\n",
        " else:\n",
        "   model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        " print(f\"Model loaded: {model.config.n_layer} layers, {model.config.n_head} heads\")\n",
        " print(f\"Vocab size: {model.config.vocab_size}\")\n",
        " print(f\"Hidden size: {model.config.n_embd}\")\n",
        "\n",
        "\n",
        " # Configure PEFT (LoRA) for parameter-efficient fine-tuning\n",
        " peft_config = LoraConfig(\n",
        "     task_type=TaskType.CAUSAL_LM,\n",
        "     r=16, # lora rank -lower\n",
        "     lora_alpha = 32, # scaling param\n",
        "     lora_dropout=0.1,\n",
        "     bias='none', # not adapt biases\n",
        "     target_modules=[\"c_attn\", \"c_proj\"] # gpt2 attn layers to adapt\n",
        " )\n",
        "\n",
        "\n",
        " # Convert to PEFT model\n",
        " model = get_peft_model(model, peft_config) # wrap model with LoRA adapters\n",
        " model.print_trainable_parameters() # shows param efficiency gain\n",
        "\n",
        " # prepare dataset\n",
        " instruction_examples = create_instruction_dataset() # pydantic validation\n",
        " dataset = InstructionTuningDataset(instruction_examples, tokenizer, max_length=128)\n",
        "\n",
        " # create dataloader\n",
        " dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        " # seutp optimizer with 8bit adam if available\n",
        " if torch.cuda.is_available() and 'bitsandbytes' in sys.modules:\n",
        "   optimizer = bnb.optim.AdamW8bit(\n",
        "       model.parameters(),\n",
        "       lr=5e-5,\n",
        "       betas=(0.9, 0.999), # adam betas variables\n",
        "       weight_decay=0.01, # weight decay for regularization\n",
        "   )\n",
        "   print(\"Using 8-bit AdamW optimizer\")\n",
        "\n",
        " else:\n",
        "   optimizer = torch.optim.AdamW(\n",
        "       model.parameters(),\n",
        "       lr=5e-5,\n",
        "       weight_decay=0.01  # weight decay for regularization\n",
        "   )\n",
        "\n",
        " # setup lr scheduler\n",
        " # The scheduler will cyclically vary the learning rate with cosine decay, restarting every T_i iterations while gradually increasing the period and never letting the LR drop below eta_min\n",
        " # The learning rate will follow a cosine decay for 10 iterations, then restart for 20 iterations, then 40, doubling each cycle, while never dropping below 1e-6.\n",
        " scheduler = CosineAnnealingWarmRestarts(\n",
        "     optimizer,\n",
        "     T_0=10,  # Number of iterations for the first restart\n",
        "     T_mult=2,  # Factor to increase T_i after a restart\n",
        "     eta_min=1e-6,  # Minimum learning rate, never below it\n",
        " )\n",
        "\n",
        "\n",
        " # Prepare everything with accelerator\n",
        " model, optimizer, dataloader, scheduler = accelerator.prepare(\n",
        "     model, optimizer, dataloader, scheduler\n",
        " )\n",
        "\n",
        " # Initialize evaluation metrics\n",
        " bleu_metric = evaluate.load(\"bleu\")  # BLEU score for generation quality\n",
        " rouge_metric = evaluate.load(\"rouge\")  # ROUGE score for summarization\n",
        "\n",
        " return model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric\n",
        "\n",
        "# Setup and test enhanced training\n",
        "model, tokenizer, dataloader, optimizer, scheduler, accelerator, bleu_metric, rouge_metric = setup_instruction_tuning_pytorch_enhanced()\n",
        "\n",
        "# Test one training step with metrics\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Testing enhanced training step with metrics:\")\n",
        "\n",
        "model.train()  # Set to training mode\n",
        "\n",
        "for batch_idx, batch in enumerate(dataloader):\n",
        " if batch_idx > 0:  # Just test one batch\n",
        "   break\n",
        "\n",
        " # Move batch to accelerator device (handles multi-GPU automatically)\n",
        " # Note: accelerator.prepare already handles device placement\n",
        "\n",
        " print(f\"Batch shapes:\")\n",
        " print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
        " print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
        " print(f\"  labels: {batch['labels'].shape}\")\n",
        "\n",
        " # Forward pass with accelerator's context\n",
        " with accelerator.accumulate(model):  # Handle gradient accumulation\n",
        "   outputs = model(**batch)  # Pass entire batch to model\n",
        "   loss = outputs.loss  # Extract loss\n",
        "   logits = outputs.logits  # Extract logits\n",
        "\n",
        "   print(f\"\\nForward pass:\")\n",
        "   print(f\"  Loss: {loss.item():.4f}\")\n",
        "   print(f\"  Logits shape: {logits.shape}\")\n",
        "   print(f\"  Perplexity: {torch.exp(loss).item():.2f}\")\n",
        "\n",
        "   # Backward pass with accelerator (handles mixed precision)\n",
        "   accelerator.backward(loss)  # Compute gradients with automatic mixed precision\n",
        "\n",
        "   # Gradient clipping for stability\n",
        "   accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to prevent explosion\n",
        "\n",
        "   # Gradient norm for monitoring\n",
        "   total_norm = 0\n",
        "   for p in model.parameters():\n",
        "     if p.grad is not None:\n",
        "       total_norm += p.grad.data.norm(2).item() ** 2  # L2 norm squared\n",
        "   total_norm = total_norm ** 0.5  # Square root for final norm\n",
        "   print(f\"  Gradient norm: {total_norm:.4f}\")\n",
        "\n",
        "   # Update weights\n",
        "   optimizer.step()  # Apply gradients\n",
        "   scheduler.step()  # Update learning rate\n",
        "   optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "   # Log to wandb if enabled\n",
        "   if USE_WANDB:\n",
        "     wandb.log({\n",
        "       \"train/loss\": loss.item(),\n",
        "       \"train/perplexity\": torch.exp(loss).item(),\n",
        "       \"train/grad_norm\": total_norm,\n",
        "       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "     })\n",
        "\n",
        "# Generate sample output with beam search\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Testing generation with enhanced decoding:\")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "test_instruction = \"Translate to French: Hello world\"\n",
        "prompt = f\"### Instruction:\\n{test_instruction}\\n\\n### Response:\\n\"\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "# Move to accelerator device\n",
        "inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate with enhanced parameters\n",
        "with torch.no_grad():\n",
        " outputs = model.generate(\n",
        "   **inputs,\n",
        "   max_new_tokens=20,  # Generate up to 20 new tokens\n",
        "   temperature=0.8,  # Sampling temperature\n",
        "   do_sample=True,  # Use sampling instead of greedy\n",
        "   top_p=0.9,  # Nucleus sampling threshold\n",
        "   top_k=50,  # Top-k sampling\n",
        "   repetition_penalty=1.2,  # Penalize repetition\n",
        "   pad_token_id=tokenizer.eos_token_id,\n",
        "   num_beams=4,  # Beam search for better quality\n",
        "   early_stopping=True,  # Stop when all beams reach EOS\n",
        " )\n",
        "\n",
        "# Decode and evaluate\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Input: {prompt}\")\n",
        "print(f\"Generated: {generated_text}\")\n",
        "\n",
        "# Compute BLEU score (example - normally done on validation set)\n",
        "reference = [\"Bonjour le monde\"]  # Expected translation\n",
        "prediction = generated_text.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "bleu_score = bleu_metric.compute(predictions=[prediction], references=[[ref] for ref in reference])\n",
        "print(f\"\\nGeneration metrics:\")\n",
        "print(f\"  BLEU score: {bleu_score['bleu']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nInsufficient Training: Your model saw only 5 examples for 1 step. Language models need thousands of examples and many epochs to learn new tasks.\")\n",
        "print(\"\\nData Mismatch: The model hasn't learned the instruction format properly. GPT-2 wasn't trained on instruction-following data.\")\n",
        "print(\"\\nRepetition Issue: The model is stuck in a loop due to poor conditioning on the prompt format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0snXzHnEQB4b",
        "outputId": "254ff777-2fcd-4bab-cb07-434a8fa8b1a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: gpt2\n",
            "Model loaded: 12 layers, 12 heads\n",
            "Vocab size: 50257\n",
            "Hidden size: 768\n",
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n",
            "\n",
            "--------------------------------------------------\n",
            "Testing enhanced training step with metrics:\n",
            "Batch shapes:\n",
            "  input_ids: torch.Size([2, 128])\n",
            "  attention_mask: torch.Size([2, 128])\n",
            "  labels: torch.Size([2, 128])\n",
            "\n",
            "Forward pass:\n",
            "  Loss: 4.7864\n",
            "  Logits shape: torch.Size([2, 128, 50257])\n",
            "  Perplexity: 119.87\n",
            "  Gradient norm: 0.4536\n",
            "\n",
            "--------------------------------------------------\n",
            "Testing generation with enhanced decoding:\n",
            "Input: ### Instruction:\n",
            "Translate to French: Hello world\n",
            "\n",
            "### Response:\n",
            "\n",
            "Generated: ### Instruction:\n",
            "Translate to French: Hello world\n",
            "\n",
            "### Response:\n",
            "\n",
            "Hello world\n",
            "\n",
            "Hello world\n",
            "\n",
            "Hello world\n",
            "\n",
            "Hello world\n",
            "\n",
            "Hello world\n",
            "\n",
            "\n",
            "Generation metrics:\n",
            "  BLEU score: 0.0000\n",
            "\n",
            "\n",
            "Insufficient Training: Your model saw only 5 examples for 1 step. Language models need thousands of examples and many epochs to learn new tasks.\n",
            "\n",
            "Data Mismatch: The model hasn't learned the instruction format properly. GPT-2 wasn't trained on instruction-following data.\n",
            "\n",
            "Repetition Issue: The model is stuck in a loop due to poor conditioning on the prompt format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yb5GqpxClrEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}