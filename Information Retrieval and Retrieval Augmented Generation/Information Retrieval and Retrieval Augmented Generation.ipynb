{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Setups"
      ],
      "metadata": {
        "id": "2RA6mP-CTb4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic evaluate transformers datasets accelerate bitsandbytes peft trl wandb\n",
        "!pip install -q torch torchinfo sentence-transformers faiss-cpu chromadb whoosh\n",
        "!pip install -q scikit-learn matplotlib seaborn pandas numpy\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyE_tbo_TlmZ",
        "outputId": "15b7de64-7a1b-4176-8bb8-1854cc5786e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOxZzI9xSbwl",
        "outputId": "8311ee6f-bddd-421d-a293-7cdeec7e588c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Tuple, Optional, Any, Union\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Pydantic for data validation\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "\n",
        "# HuggingFace ecosystem\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "import evaluate  # HF evaluate library for metrics\n",
        "\n",
        "# PEFT for parameter-efficient fine-tuning\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# TRL for RLHF and instruction tuning\n",
        "from trl import SFTTrainer, DPOTrainer\n",
        "\n",
        "# Accelerate for distributed training\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# 8-bit optimization\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# IR libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss  # Facebook AI Similarity Search for efficient nearest neighbor\n",
        "\n",
        "# Experiment tracking\n",
        "import wandb\n",
        "\n",
        "# Set device and seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Structures and Pydantic Validation"
      ],
      "metadata": {
        "id": "UU3vggkjVCDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Document(BaseModel):\n",
        "  \"\"\"Document with validation using pydantic v2\"\"\"\n",
        "\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid',  # no extra fields allowed\n",
        "      validate_assignment=True # validate on reassingment\n",
        "  )\n",
        "\n",
        "\n",
        "  doc_id: str = Field(..., min_length = 1, description=\"Document identifier\")\n",
        "  title: str = Field(..., min_length = 1, description=\"Document title\")\n",
        "  content: str = Field(..., min_length = 1, description=\"Document text content\")\n",
        "  metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n",
        "\n",
        "  @field_validator('content')\n",
        "  @classmethod\n",
        "  def validate_content(cls, v:str) -> str:\n",
        "    \"\"\"Ensure content is non-empty after stripping\"\"\"\n",
        "\n",
        "    if not v.strip():\n",
        "      raise ValueError(\"Content cannot be empty\")\n",
        "    return v.strip()\n",
        "\n",
        "\n",
        "  def get_tokens(self) -> List[str]:\n",
        "    \"\"\"Simple whitespace tokenization\"\"\"\n",
        "\n",
        "    return self.content.lower().split()\n",
        "\n",
        "\n",
        "class Query(BaseModel):\n",
        "  \"\"\"Query with validation\"\"\"\n",
        "\n",
        "  query_id: str = Field(..., min_length=1)\n",
        "  text: str = Field(..., min_length=1)\n",
        "  metadata: Dict[str, Any] = Field(default_factory=dict )\n",
        "\n",
        "  def get_tokens(self) -> List[str]:\n",
        "    \"\"\"Tokenize query text\"\"\"\n",
        "\n",
        "    return self.text.lower().split()\n",
        "\n",
        "\n",
        "class DocumentCollection(BaseModel):\n",
        "  \"\"\"Collection of documents with indexing capabilities\"\"\"\n",
        "\n",
        "  documents: List[Document] = Field(defualt_factory=list)\n",
        "\n",
        "  def add_document(self, doc: Document):\n",
        "    \"\"\"Add validated document to collection\"\"\"\n",
        "\n",
        "    self.documents.append(doc)\n",
        "\n",
        "\n",
        "  def get_vocabulary(self) -> List[str]:\n",
        "    \"\"\"Extract unique vocabulary from all the documents\"\"\"\n",
        "\n",
        "    vocab = set()\n",
        "    for doc in self.documents:\n",
        "      vocab.update(doc.get_tokens()) # Adding all the tokens to the set\n",
        "\n",
        "    return sorted(list(vocab))\n",
        "\n"
      ],
      "metadata": {
        "id": "I4DrB1W7Tot4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple test with Shakespeare corpus from the book"
      ],
      "metadata": {
        "id": "1Vv4tMqRYM4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_docs = [\n",
        "  Document(\n",
        "    doc_id=\"as_you_like_it\",\n",
        "    title=\"As You Like It\",\n",
        "    content=\"battle good fool wit love forest magic\",\n",
        "    metadata={\"genre\": \"comedy\", \"year\": 1599}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"twelfth_night\",\n",
        "    title=\"Twelfth Night\",\n",
        "    content=\"good fool wit love comedy mistaken identity\",\n",
        "    metadata={\"genre\": \"comedy\", \"year\": 1602}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"julius_caesar\",\n",
        "    title=\"Julius Caesar\",\n",
        "    content=\"battle battle battle good fool war rome politics\",\n",
        "    metadata={\"genre\": \"tragedy\", \"year\": 1599}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"henry_v\",\n",
        "    title=\"Henry V\",\n",
        "    content=\"battle battle battle battle good wit war king england\",\n",
        "    metadata={\"genre\": \"history\", \"year\": 1599}\n",
        "  )\n",
        "]\n",
        "\n",
        "collection = DocumentCollection(documents=shakespeare_docs)\n",
        "print(f\"Created collection with {len(collection.documents)} documents\")\n",
        "print(f\"Vocabulary size: {len(collection.get_vocabulary())} unique terms\")\n",
        "print(f\"Sample vocabulary: {collection.get_vocabulary()[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olSdWCgwUkU8",
        "outputId": "84dfabf1-d07f-4652-f4fe-13b6ec1892f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created collection with 4 documents\n",
            "Vocabulary size: 15 unique terms\n",
            "Sample vocabulary: ['battle', 'comedy', 'england', 'fool', 'forest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF with scikit-learn"
      ],
      "metadata": {
        "id": "Y5Z-hIxXaSu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFRetriever:\n",
        "  \"\"\"TF-idf retriever with pydantic validation and sklearn\"\"\"\n",
        "\n",
        "  def __init__(self, collection: DocumentCollection):\n",
        "    self.collection = collection\n",
        "    self.vectorizer = TfidfVectorizer(\n",
        "      lowercase=True,  #convert to lowercase\n",
        "      max_features=1000, # max vocab size\n",
        "      ngram_range=(1,2), # use unigrams and bigrams\n",
        "      sublinear_tf=True, # log(tf) instead of just raw tf\n",
        "      smooth_idf=True, # Laplace smoothing\n",
        "      norm='l2' # normalize with ridge for cos sim\n",
        "    )\n",
        "\n",
        "    self.tfidf_matrix = None\n",
        "    self.doc_ids = []\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    \"\"\"Build TF-IDF matrix from doc collection\"\"\"\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for doc in self.collection.documents:\n",
        "      texts.append(doc.content)\n",
        "      self.doc_ids.append(doc.doc_id) # storing doc ids\n",
        "\n",
        "\n",
        "    print(\"Documents from test\")\n",
        "    print(\"Row 0: battle good fool wit love forest magic\")\n",
        "    print(\"Row 1: good fool wit love comedy mistaken identity\")\n",
        "    print(\"Row 2: battle battle battle good fool war rome politics\")\n",
        "    print(\"Row 3: battle battle battle battle good wit war king england\")\n",
        "\n",
        "\n",
        "    # fit and transform docs to tf idf matrix\n",
        "    self.tfidf_matrix = self.vectorizer.fit_transform(texts) # [n_docs, n_features]  n_docs x vocab per doc\n",
        "    print(f\"\\nself.tfidf_matrix.shape: {self.tfidf_matrix.shape}\\n\")\n",
        "    print(f\"\\nself.tfidf_matrix:\\n {self.tfidf_matrix}\\n\")\n",
        "\n",
        "\n",
        "  def search(self,query: Query, top_k: int=3) -> List[Tuple[Document,float]]:\n",
        "    \"\"\"Search doc using tf-idf cos similarity\"\"\"\n",
        "\n",
        "    # Transform qry to tf-idf vector\n",
        "    query_vector = self.vectorizer.transform([query.text]) # [1, n_features]  single vector with all the features/words\n",
        "\n",
        "    # Compute cos similarity\n",
        "    similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "\n",
        "    print(f\"\\nQuery: '{query.text}'\")\n",
        "    print(f\"Query vector shape: {query_vector.shape}\")\n",
        "    print(f\"Similarities: {similarities}\")\n",
        "\n",
        "    # Top-k doc indices\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1] # sort desc\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "      doc = self.collection.documents[idx]\n",
        "      score = similarities[idx]\n",
        "      results.append((doc, score))\n",
        "      print(f\"  Rank {len(results)}: '{doc.title}' (score={score:.4f})\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "TgY8OgY7YPkN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple test for the TF-IDF Retriever"
      ],
      "metadata": {
        "id": "meI2znRJeVII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = TFIDFRetriever(collection)\n",
        "retriever.fit()\n",
        "\n",
        "\n",
        "# Get the feature names (what each column represents)\n",
        "print(\"\\nUnigrams and bigrams:\\n\")\n",
        "feature_names = retriever.vectorizer.get_feature_names_out()\n",
        "for idx, feature in enumerate(feature_names):\n",
        "    print(f\"Column {idx}: '{feature}'\")\n",
        "\n",
        "test_query = Query(query_id=\"q1\", text=\"battle war\")\n",
        "results = retriever.search(test_query, top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCQ0GDetbpk6",
        "outputId": "51d09ac8-3518-42b1-81b0-34812fefe397"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents from test\n",
            "Row 0: battle good fool wit love forest magic\n",
            "Row 1: good fool wit love comedy mistaken identity\n",
            "Row 2: battle battle battle good fool war rome politics\n",
            "Row 3: battle battle battle battle good wit war king england\n",
            "\n",
            "self.tfidf_matrix.shape: (4, 32)\n",
            "\n",
            "\n",
            "self.tfidf_matrix:\n",
            " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 50 stored elements and shape (4, 32)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.22325169970289915\n",
            "  (0, 11)\t0.18252289313304706\n",
            "  (0, 6)\t0.22325169970289915\n",
            "  (0, 29)\t0.22325169970289915\n",
            "  (0, 17)\t0.2757602638693091\n",
            "  (0, 9)\t0.34976692846571494\n",
            "  (0, 20)\t0.34976692846571494\n",
            "  (0, 2)\t0.22325169970289915\n",
            "  (0, 12)\t0.22325169970289915\n",
            "  (0, 8)\t0.2757602638693091\n",
            "  (0, 30)\t0.2757602638693091\n",
            "  (0, 19)\t0.34976692846571494\n",
            "  (0, 10)\t0.34976692846571494\n",
            "  (1, 11)\t0.17057535199597096\n",
            "  (1, 6)\t0.20863814180702328\n",
            "  (1, 29)\t0.20863814180702328\n",
            "  (1, 17)\t0.25770961257841624\n",
            "  (1, 12)\t0.20863814180702328\n",
            "  (1, 8)\t0.25770961257841624\n",
            "  (1, 30)\t0.25770961257841624\n",
            "  (1, 3)\t0.3268719661160508\n",
            "  (1, 21)\t0.3268719661160508\n",
            "  (1, 14)\t0.3268719661160508\n",
            "  (1, 18)\t0.3268719661160508\n",
            "  (1, 4)\t0.3268719661160508\n",
            "  (1, 22)\t0.3268719661160508\n",
            "  (2, 0)\t0.40964786723904145\n",
            "  (2, 11)\t0.15958830297517893\n",
            "  (2, 6)\t0.1951994036492684\n",
            "  (2, 2)\t0.1951994036492684\n",
            "  (2, 12)\t0.1951994036492684\n",
            "  (2, 26)\t0.2411100973882306\n",
            "  (2, 24)\t0.30581758590686003\n",
            "  (2, 23)\t0.30581758590686003\n",
            "  (2, 1)\t0.4082348815974165\n",
            "  (2, 7)\t0.30581758590686003\n",
            "  (2, 28)\t0.30581758590686003\n",
            "  (2, 25)\t0.30581758590686003\n",
            "  (3, 0)\t0.4262924719209452\n",
            "  (3, 11)\t0.14605156614348416\n",
            "  (3, 29)\t0.17864203128774342\n",
            "  (3, 2)\t0.17864203128774342\n",
            "  (3, 26)\t0.22065844852072933\n",
            "  (3, 1)\t0.4630765316640421\n",
            "  (3, 15)\t0.27987726257646384\n",
            "  (3, 5)\t0.27987726257646384\n",
            "  (3, 13)\t0.27987726257646384\n",
            "  (3, 31)\t0.27987726257646384\n",
            "  (3, 27)\t0.27987726257646384\n",
            "  (3, 16)\t0.27987726257646384\n",
            "\n",
            "\n",
            "Unigrams and bigrams:\n",
            "\n",
            "Column 0: 'battle'\n",
            "Column 1: 'battle battle'\n",
            "Column 2: 'battle good'\n",
            "Column 3: 'comedy'\n",
            "Column 4: 'comedy mistaken'\n",
            "Column 5: 'england'\n",
            "Column 6: 'fool'\n",
            "Column 7: 'fool war'\n",
            "Column 8: 'fool wit'\n",
            "Column 9: 'forest'\n",
            "Column 10: 'forest magic'\n",
            "Column 11: 'good'\n",
            "Column 12: 'good fool'\n",
            "Column 13: 'good wit'\n",
            "Column 14: 'identity'\n",
            "Column 15: 'king'\n",
            "Column 16: 'king england'\n",
            "Column 17: 'love'\n",
            "Column 18: 'love comedy'\n",
            "Column 19: 'love forest'\n",
            "Column 20: 'magic'\n",
            "Column 21: 'mistaken'\n",
            "Column 22: 'mistaken identity'\n",
            "Column 23: 'politics'\n",
            "Column 24: 'rome'\n",
            "Column 25: 'rome politics'\n",
            "Column 26: 'war'\n",
            "Column 27: 'war king'\n",
            "Column 28: 'war rome'\n",
            "Column 29: 'wit'\n",
            "Column 30: 'wit love'\n",
            "Column 31: 'wit war'\n",
            "\n",
            "Query: 'battle war'\n",
            "Query vector shape: (1, 32)\n",
            "Similarities: [0.14047611 0.         0.44515758 0.43973537]\n",
            "  Rank 1: 'Julius Caesar' (score=0.4452)\n",
            "  Rank 2: 'Henry V' (score=0.4397)\n",
            "  Rank 3: 'As You Like It' (score=0.1405)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display TF-IDF matrix"
      ],
      "metadata": {
        "id": "HNPi0ajSl6xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make df\n",
        "tfidf_df = pd.DataFrame(\n",
        "  retriever.tfidf_matrix.toarray(),\n",
        "  columns=feature_names,\n",
        "  index=[f\"Doc{i}\" for i in range(len(shakespeare_docs))]\n",
        ")\n",
        "tfidf_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "X0jvF62IebVz",
        "outputId": "95a8996b-072d-4c83-a93b-8d9a715ddf9a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        battle  battle battle  battle good    comedy  comedy mistaken  \\\n",
              "Doc0  0.223252       0.000000     0.223252  0.000000         0.000000   \n",
              "Doc1  0.000000       0.000000     0.000000  0.326872         0.326872   \n",
              "Doc2  0.409648       0.408235     0.195199  0.000000         0.000000   \n",
              "Doc3  0.426292       0.463077     0.178642  0.000000         0.000000   \n",
              "\n",
              "       england      fool  fool war  fool wit    forest  ...  \\\n",
              "Doc0  0.000000  0.223252  0.000000   0.27576  0.349767  ...   \n",
              "Doc1  0.000000  0.208638  0.000000   0.25771  0.000000  ...   \n",
              "Doc2  0.000000  0.195199  0.305818   0.00000  0.000000  ...   \n",
              "Doc3  0.279877  0.000000  0.000000   0.00000  0.000000  ...   \n",
              "\n",
              "      mistaken identity  politics      rome  rome politics       war  \\\n",
              "Doc0           0.000000  0.000000  0.000000       0.000000  0.000000   \n",
              "Doc1           0.326872  0.000000  0.000000       0.000000  0.000000   \n",
              "Doc2           0.000000  0.305818  0.305818       0.305818  0.241110   \n",
              "Doc3           0.000000  0.000000  0.000000       0.000000  0.220658   \n",
              "\n",
              "      war king  war rome       wit  wit love   wit war  \n",
              "Doc0  0.000000  0.000000  0.223252   0.27576  0.000000  \n",
              "Doc1  0.000000  0.000000  0.208638   0.25771  0.000000  \n",
              "Doc2  0.000000  0.305818  0.000000   0.00000  0.000000  \n",
              "Doc3  0.279877  0.000000  0.178642   0.00000  0.279877  \n",
              "\n",
              "[4 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ce304cc-da69-432d-a118-bba983eab371\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battle</th>\n",
              "      <th>battle battle</th>\n",
              "      <th>battle good</th>\n",
              "      <th>comedy</th>\n",
              "      <th>comedy mistaken</th>\n",
              "      <th>england</th>\n",
              "      <th>fool</th>\n",
              "      <th>fool war</th>\n",
              "      <th>fool wit</th>\n",
              "      <th>forest</th>\n",
              "      <th>...</th>\n",
              "      <th>mistaken identity</th>\n",
              "      <th>politics</th>\n",
              "      <th>rome</th>\n",
              "      <th>rome politics</th>\n",
              "      <th>war</th>\n",
              "      <th>war king</th>\n",
              "      <th>war rome</th>\n",
              "      <th>wit</th>\n",
              "      <th>wit love</th>\n",
              "      <th>wit war</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0.223252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.27576</td>\n",
              "      <td>0.349767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223252</td>\n",
              "      <td>0.27576</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.326872</td>\n",
              "      <td>0.326872</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.208638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25771</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326872</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.208638</td>\n",
              "      <td>0.25771</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0.409648</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.195199</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.195199</td>\n",
              "      <td>0.305818</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.305818</td>\n",
              "      <td>0.305818</td>\n",
              "      <td>0.305818</td>\n",
              "      <td>0.241110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.305818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc3</th>\n",
              "      <td>0.426292</td>\n",
              "      <td>0.463077</td>\n",
              "      <td>0.178642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.279877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220658</td>\n",
              "      <td>0.279877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.178642</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.279877</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ce304cc-da69-432d-a118-bba983eab371')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ce304cc-da69-432d-a118-bba983eab371 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ce304cc-da69-432d-a118-bba983eab371');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d26e6d3-2998-41cf-a8c2-76a68622395f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d26e6d3-2998-41cf-a8c2-76a68622395f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d26e6d3-2998-41cf-a8c2-76a68622395f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c39a44a4-135b-47ac-9411-867842cee772\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tfidf_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c39a44a4-135b-47ac-9411-867842cee772 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tfidf_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tfidf_df"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25 Scorer"
      ],
      "metadata": {
        "id": "GHHNf2QDquAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25Scorer:\n",
        "  \"\"\"BM25 scoring with parameter tuning\"\"\"\n",
        "\n",
        "  def __init__(self, collection: DocumentCollection, k1: float=1.2, b:float=0.75):\n",
        "    \"\"\"\n",
        "    k1: controls term frequency saturation (typically 1.2-2.0). Enough to give score to a repeated word but not making a word repeated 100 times 100 times more important.\n",
        "    b: controls length normalization (0=no normalization, 1=full normalization). Regularization over too large docs.\n",
        "    \"\"\"\n",
        "\n",
        "    self.collection = collection\n",
        "    self.k1 = k1 # Term frequency saturation param\n",
        "    self.b = b   # Length normalization param\n",
        "    self.doc_lengths = {} # Store doc lengths\n",
        "    self.avg_doc_length = 0 # Avg doc length\n",
        "    self.doc_freqs = defaultdict(int) # doc freqs per term\n",
        "    self.N = len(collection.documents) # total num of docs\n",
        "    self.term_freqs = {} # Term freqs per doc\n",
        "\n",
        "    self._compute_statistics()\n",
        "\n",
        "\n",
        "  def _compute_statistics(self):\n",
        "    \"\"\"Copmute doc statistics for BM25\"\"\"\n",
        "\n",
        "    total_length = 0\n",
        "\n",
        "    for doc in self.collection.documents:\n",
        "      tokens = doc.get_tokens()\n",
        "      doc_length = len(tokens)\n",
        "      self.doc_lengths[doc.doc_id] = doc_length\n",
        "      total_length += doc_length\n",
        "\n",
        "      # Count term freq for this doc\n",
        "      term_freq = defaultdict(int)\n",
        "      unique_terms = set()\n",
        "      for token in tokens:\n",
        "        term_freq[token] += 1 # add 1 on occurence\n",
        "        unique_terms.add(token)\n",
        "\n",
        "      self.term_freqs[doc.doc_id] = term_freq\n",
        "\n",
        "      # Update doc frequencies\n",
        "      for term in unique_terms:\n",
        "        self.doc_freqs[term] += 1 # Increment doc freq per term\n",
        "\n",
        "\n",
        "    # Compute avg length\n",
        "    self.avg_doc_length = total_length / self.N\n",
        "\n",
        "\n",
        "    print(f\"BM25 Statistics:\")\n",
        "    print(f\"  Documents: {self.N}\")\n",
        "    print(f\"  Avg doc length: {self.avg_doc_length:.2f}\")\n",
        "    print(f\"  Unique terms: {len(self.doc_freqs)}\")\n",
        "\n",
        "\n",
        "  def score(self, query: Query, doc: Document) -> float:\n",
        "    \"\"\"\n",
        "    Compute BM25 score for query-document pair.\n",
        "\n",
        "    BM25(q,d) = Σ IDF(t) * (tf(t,d) * (k1 + 1)) / (tf(t,d) + k1 * (1 - b + b * |d|/avgdl))\n",
        "\n",
        "    Where for each term t in query q:\n",
        "      - IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5)) — inverse document frequency\n",
        "      - tf(t,d) = frequency of term t in document d\n",
        "      - |d| = length of document d (number of terms)\n",
        "      - avgdl = average document length in collection\n",
        "      - k1 = term frequency saturation parameter (default 1.2). Keeps repeated more important but not in a linear scale per occureance.\n",
        "      - b = length normalization parameter (0=none, 1=full, default 0.75). Penalizes too long docs.\n",
        "\n",
        "    The formula balances term importance (IDF) with normalized term frequency,\n",
        "    preventing bias toward longer documents while rewarding term repetition up to a limit.\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0.0\n",
        "    query_terms = query.get_tokens() # query terms\n",
        "    doc_length = self.doc_lengths[doc.doc_id]\n",
        "\n",
        "    for term in query_terms:\n",
        "      if term not in self.doc_freqs:\n",
        "        continue # skip terms not in the collection\n",
        "\n",
        "\n",
        "      # Compute IDF: log((N-df + 0.5)/ (df + 0.5) )\n",
        "\n",
        "      df = self.doc_freqs[term] # Doc freq\n",
        "\n",
        "      idf = math.log((self.N - df + 0.5) / (df + 0.5)) # Inv Doc Freq with smoothing\n",
        "      print(\"\\n\\n\\nidf = math.log((self.N - df + 0.5) / (df + 0.5))\\n\")\n",
        "      print(f\"{idf:.3f} = math.log(({self.N} - {df} + 0.5) / ({df} + 0.5))\\n\")\n",
        "\n",
        "\n",
        "      # Get term freq in doc\n",
        "      tf = self.term_freqs[doc.doc_id].get(term, 0)\n",
        "      print(f\"tf = {tf}\\n\\n\")\n",
        "\n",
        "      #Compute normalized term freq\n",
        "      # tf_normalized (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |d| / avgdl ))\n",
        "\n",
        "      norm_factor = 1 - self.b + self.b * (doc_length / self.avg_doc_length)\n",
        "      tf_component = (tf * (self.k1 + 1 )) / (tf + self.k1 * norm_factor) # Normalized TF\n",
        "\n",
        "\n",
        "      # Add to score\n",
        "      term_score = idf * tf_component # idf * normalized tf\n",
        "      score += term_score\n",
        "\n",
        "      if tf > 0:  # Only print for terms that appear in document\n",
        "        print(f\"    Term '{term}': tf={tf}, df={df}, idf={idf:.3f}, contribution={term_score:.3f}\")\n",
        "        print(\"\\n\", \"-\"*50,\"\\n\")\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "  def search(self, query: Query, top_k: int = 3) -> List[Tuple[Document, float]]:\n",
        "    \"\"\"Search documents using BM25 scoring\"\"\"\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    print(f\"\\nBM25 Search for: '{query.text}'\")\n",
        "\n",
        "    for doc in self.collection.documents:\n",
        "      score = self.score(query, doc) # compute bm25 score\n",
        "      scores.append((doc, score))\n",
        "      print(f\"  '{doc.title}': {score:.4f}\")\n",
        "\n",
        "    # Sort desc by score\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return top k\n",
        "    return scores[:top_k]\n"
      ],
      "metadata": {
        "id": "5_gFdt5Hk8Hs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test BM25 scorer\n",
        "bm25 = BM25Scorer(collection, k1=1.2, b=0.75)\n",
        "query = Query(query_id=\"q2\", text=\"battle fool\")\n",
        "bm25_results = bm25.search(query, top_k=3)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"\\nThe negative IDF -0.847 here means these terms are so common they're actually penalizing the scores rather than helping.\")\n",
        "\n",
        "print(\"\\nHigh IDF = term is RARE, appears in few documents -> very distinctive/important\")\n",
        "print(\"\\nLow/Negative IDF = term is COMMON, appears in many documents -> not distinctive\\n\")\n",
        "\n",
        "\n",
        "print(\"\\nTop BM25 Results:\")\n",
        "for rank, (doc, score) in enumerate(bm25_results, 1):\n",
        "    print(f\"  {rank}. {doc.title}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AVU_06YzDhA",
        "outputId": "d5434353-dc24-4545-f095-8eae63f4aaeb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 Statistics:\n",
            "  Documents: 4\n",
            "  Avg doc length: 7.75\n",
            "  Unique terms: 15\n",
            "\n",
            "BM25 Search for: 'battle fool'\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 1\n",
            "\n",
            "\n",
            "    Term 'battle': tf=1, df=3, idf=-0.847, contribution=-0.882\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 1\n",
            "\n",
            "\n",
            "    Term 'fool': tf=1, df=3, idf=-0.847, contribution=-0.882\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "  'As You Like It': -1.7644\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 1\n",
            "\n",
            "\n",
            "    Term 'fool': tf=1, df=3, idf=-0.847, contribution=-0.882\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "  'Twelfth Night': -0.8822\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 3\n",
            "\n",
            "\n",
            "    Term 'battle': tf=3, df=3, idf=-0.847, contribution=-1.322\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 1\n",
            "\n",
            "\n",
            "    Term 'fool': tf=1, df=3, idf=-0.847, contribution=-0.836\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "  'Julius Caesar': -2.1586\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 4\n",
            "\n",
            "\n",
            "    Term 'battle': tf=4, df=3, idf=-0.847, contribution=-1.395\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "idf = math.log((self.N - df + 0.5) / (df + 0.5))\n",
            "\n",
            "-0.847 = math.log((4 - 3 + 0.5) / (3 + 0.5))\n",
            "\n",
            "tf = 0\n",
            "\n",
            "\n",
            "  'Henry V': -1.3949\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The negative IDF -0.847 here means these terms are so common they're actually penalizing the scores rather than helping.\n",
            "\n",
            "High IDF = term is RARE, appears in few documents -> very distinctive/important\n",
            "\n",
            "Low/Negative IDF = term is COMMON, appears in many documents -> not distinctive\n",
            "\n",
            "\n",
            "Top BM25 Results:\n",
            "  1. Twelfth Night: -0.8822\n",
            "  2. Henry V: -1.3949\n",
            "  3. As You Like It: -1.7644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index Implementation"
      ],
      "metadata": {
        "id": "RNYlzeBARkXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndex:\n",
        "  \"\"\"Efficient inverted index\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.index = defaultdict(list) # term -> list of (doc_id, positions)\n",
        "    self.doc_lengths = {} #  doc_id -> document lenght\n",
        "    self.doc_norms = {} # dog_id -> L2 norm for cos sim\n",
        "\n",
        "  def add_document(self, doc: Document):\n",
        "    \"\"\"Add doc to inverted index\"\"\"\n",
        "\n",
        "    tokens = doc.get_tokens()\n",
        "    self.doc_lengths[doc.doc_id] = len(tokens)\n",
        "\n",
        "    # track term freq for L2 norm\n",
        "    term_freqs = defaultdict(int)\n",
        "\n",
        "    # build postings listwith positions\n",
        "    for position, token in enumerate(tokens):\n",
        "      self.index[token].append((doc.doc_id, position))\n",
        "      term_freqs[token] += 1\n",
        "\n",
        "    # compute l2 norm for doc vector\n",
        "    norm = math.sqrt(sum(tf**2 for tf in term_freqs.values()))\n",
        "    self.doc_norms[doc.doc_id] = norm\n",
        "\n",
        "    print(f\"\\nIndexed '{doc.title}': {len(tokens)} tokens, {len(term_freqs)} unique terms\")\n",
        "\n",
        "\n",
        "  def search_term(self, term: str) -> List[Tuple[str, List[int]]]:\n",
        "    \"\"\"Search for docs containing a term\"\"\"\n",
        "\n",
        "    # if not contained term in docs retrieve empty list\n",
        "    term = term.lower()\n",
        "    if term not in self.index:\n",
        "      return []\n",
        "\n",
        "    # group positions by doc\n",
        "    doc_positions = defaultdict(list)\n",
        "    for doc_id, position in self.index[term]:\n",
        "      doc_positions[doc_id].append(position) # group positions by doc\n",
        "\n",
        "    return list(doc_positions.items()) # return (doc_id, positions) pairs\n",
        "\n",
        "\n",
        "  def boolean_and(self,terms: List[str]) -> List[str]:\n",
        "    \"\"\"Find docs containing ALL terms (AND operation)\"\"\"\n",
        "    if not terms:\n",
        "      return []\n",
        "\n",
        "\n",
        "    # get doc sets for each term\n",
        "    doc_sets = []\n",
        "    for term in terms:\n",
        "      docs = set(doc_id for doc_id, _ in self.index[term.lower()])\n",
        "      doc_sets.append(docs)\n",
        "\n",
        "\n",
        "    # intersect all docs sets\n",
        "    result = doc_sets[0] # start with tfirst set\n",
        "    for doc_set in doc_sets[1:]:\n",
        "      result = result.intersection(doc_set) # intersect subsequents\n",
        "\n",
        "    return list(result)\n",
        "\n",
        "\n",
        "  def boolean_or(self, terms: List[str]) -> List[str]:\n",
        "    \"\"\"Find documents containing ANY term (OR operation)\"\"\"\n",
        "\n",
        "    result = set()\n",
        "    for term in terms:\n",
        "      docs = set( doc_id for doc_id, _ in self.index[term.lower()] )\n",
        "      result = result.union(docs)\n",
        "\n",
        "    return list(result)\n",
        "\n",
        "\n",
        "  def print_index(self):\n",
        "      \"\"\"Print inverted index structure\"\"\"\n",
        "      print(\"\\nInverted Index:\")\n",
        "      for term in sorted(self.index.keys()):\n",
        "          postings = self.index[term]\n",
        "          doc_freq = len(set(doc_id for doc_id, _ in postings))  # Count unique documents\n",
        "          print(f\"\\n  '{term}' -> df={doc_freq}, postings={postings}\")  # Show first 5 postings\n",
        "\n"
      ],
      "metadata": {
        "id": "SOcinyHw3IAO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Inverted Index"
      ],
      "metadata": {
        "id": "uB-Sq_2GaOzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inv_index = InvertedIndex()\n",
        "for doc in collection.documents:\n",
        "  inv_index.add_document(doc)\n",
        "\n",
        "\n",
        "inv_index.print_index()\n",
        "\n",
        "# Test boolean search\n",
        "print(\"\\nBoolean Search Tests:\")\n",
        "print(f\"  Documents with 'battle': {inv_index.search_term('battle')}\")\n",
        "print(f\"  Documents with 'battle' AND 'fool': {inv_index.boolean_and(['battle', 'fool'])}\")\n",
        "print(f\"  Documents with 'love' OR 'war': {inv_index.boolean_or(['love', 'war'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuHw7KebUzyC",
        "outputId": "06afadfa-2adf-40dd-d72d-e5ca7360b783"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indexed 'As You Like It': 7 tokens, 7 unique terms\n",
            "\n",
            "Indexed 'Twelfth Night': 7 tokens, 7 unique terms\n",
            "\n",
            "Indexed 'Julius Caesar': 8 tokens, 6 unique terms\n",
            "\n",
            "Indexed 'Henry V': 9 tokens, 6 unique terms\n",
            "\n",
            "Inverted Index:\n",
            "\n",
            "  'battle' -> df=3, postings=[('as_you_like_it', 0), ('julius_caesar', 0), ('julius_caesar', 1), ('julius_caesar', 2), ('henry_v', 0), ('henry_v', 1), ('henry_v', 2), ('henry_v', 3)]\n",
            "\n",
            "  'comedy' -> df=1, postings=[('twelfth_night', 4)]\n",
            "\n",
            "  'england' -> df=1, postings=[('henry_v', 8)]\n",
            "\n",
            "  'fool' -> df=3, postings=[('as_you_like_it', 2), ('twelfth_night', 1), ('julius_caesar', 4)]\n",
            "\n",
            "  'forest' -> df=1, postings=[('as_you_like_it', 5)]\n",
            "\n",
            "  'good' -> df=4, postings=[('as_you_like_it', 1), ('twelfth_night', 0), ('julius_caesar', 3), ('henry_v', 4)]\n",
            "\n",
            "  'identity' -> df=1, postings=[('twelfth_night', 6)]\n",
            "\n",
            "  'king' -> df=1, postings=[('henry_v', 7)]\n",
            "\n",
            "  'love' -> df=2, postings=[('as_you_like_it', 4), ('twelfth_night', 3)]\n",
            "\n",
            "  'magic' -> df=1, postings=[('as_you_like_it', 6)]\n",
            "\n",
            "  'mistaken' -> df=1, postings=[('twelfth_night', 5)]\n",
            "\n",
            "  'politics' -> df=1, postings=[('julius_caesar', 7)]\n",
            "\n",
            "  'rome' -> df=1, postings=[('julius_caesar', 6)]\n",
            "\n",
            "  'war' -> df=2, postings=[('julius_caesar', 5), ('henry_v', 6)]\n",
            "\n",
            "  'wit' -> df=3, postings=[('as_you_like_it', 3), ('twelfth_night', 2), ('henry_v', 5)]\n",
            "\n",
            "Boolean Search Tests:\n",
            "  Documents with 'battle': [('as_you_like_it', [0]), ('julius_caesar', [0, 1, 2]), ('henry_v', [0, 1, 2, 3])]\n",
            "  Documents with 'battle' AND 'fool': ['as_you_like_it', 'julius_caesar']\n",
            "  Documents with 'love' OR 'war': ['as_you_like_it', 'julius_caesar', 'twelfth_night', 'henry_v']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Retrieval with BERT and PEFT"
      ],
      "metadata": {
        "id": "J-Bh8-Zk0fO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBERTRetriever:\n",
        "  \"\"\"Dense retrieval using BERT embeddings with PEFT for efficiency\"\"\"\n",
        "\n",
        "  def __init__(self, model_name:str = 'bert-base-uncased', use_peft: bool = True):\n",
        "    self.device = device\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Load model with optional 8-bit quantization\n",
        "    if use_peft and torch.cuda.is_available():\n",
        "      bnb_config = BitsAndBytesConfig(\n",
        "          load_in_8bit = True,\n",
        "          bnb_8bit_compute_dtype=torch.float16,\n",
        "          bnb_8bit_quant_type=\"nf8\", # Quantization type selected: NF8 (Normal Float 8-bit)\n",
        "          bnb_8bit_use_double_quant=True # # Double quantization for more memory savings\n",
        "          )\n",
        "\n",
        "      self.model = AutoModel.from_pretrained(\n",
        "          model_name,\n",
        "          quantization_config = bnb_config,\n",
        "          device_map=\"auto\" # It intelligently distributes the model's parameters across your system's resources, prioritizing faster devices like GPUs.\n",
        "      )\n",
        "\n",
        "      # prepare for k-bit training\n",
        "      self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "      # Add LoRA adapters for fine-tuning\n",
        "      peft_config = LoraConfig(\n",
        "          r=8,\n",
        "          lora_alpha=32, # scaling param\n",
        "          target_modules=[\"query\", \"value\"],  # Attention Layers\n",
        "          lora_dropout=0.1,\n",
        "          bias = \"none\",\n",
        "          task_type = TaskType.FEATURE_EXTRACTION, # task type for embeddings\n",
        "      )\n",
        "\n",
        "      self.model = get_peft_model(self.model, peft_config)  # Returns a Peft model object from a model and a config\n",
        "\n",
        "      self.model.print_trainable_parameters() # show parameter efficiency\n",
        "\n",
        "    else:\n",
        "      self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "\n",
        "    self.doc_embeddings = {} # Cache for doc embeddings\n",
        "\n",
        "\n",
        "\n",
        "  def encode_text(self, text:str) -> torch.Tensor:\n",
        "    \"\"\"Encode text to dense vector using BERT [CLS] special token\"\"\"\n",
        "\n",
        "    # Tokenize text\n",
        "    inputs = self.tokenizer(\n",
        "        text,\n",
        "        padding=True, # pad to max length in batch\n",
        "        truncation=True,  # truncate to max length\n",
        "        max_length=512, # max length\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(self.device)\n",
        "\n",
        "\n",
        "    # get BERT outputs\n",
        "    with torch.no_grad(): # we will do inference to only extract the embeddings for the indexing/search\n",
        "      outputs = self.model(**inputs) # forward pass [1, sequence_length, 768]\n",
        "\n",
        "\n",
        "    # using [CLS] token embedding as doc representation\n",
        "    cls_embedding = outputs.last_hidden_state[:,0,:] # [batch_size, seq_length, hidden_size] -> [batch_size, hidden_size] so it is [1, 768]\n",
        "\n",
        "    print(f\"Encoded '{text[:30]}...': shape={cls_embedding.shape}\")\n",
        "    return cls_embedding.cpu()  # Move to CPU for storage\n",
        "\n",
        "\n",
        "  def index_documents(self, documents: List[Document]):\n",
        "    \"\"\"Pre-compute and cache doc embeddings\"\"\"\n",
        "\n",
        "    print(f\"\\nIndexing {len(documents)} documents with BERT...\")\n",
        "\n",
        "    for doc in documents:\n",
        "      embedding = self.encode_text(doc.content)\n",
        "      self.doc_embeddings[doc.doc_id] = embedding # cache embedding\n",
        "\n",
        "\n",
        "    # stack all the embeddings for efficient similarity computation\n",
        "    self.doc_matrix = torch.stack(list(self.doc_embeddings.values())).squeeze(1) # squeeze(1) removes dimension at index 1 so if it has size 1: [n_docs, 1, 768] -> [n_docs, 768]\n",
        "    print(f\"Document matrix shape: {self.doc_matrix.shape}\")\n",
        "    print(f\"Document matrix:\\n {self.doc_matrix[:,:20]}\")\n",
        "\n",
        "\n",
        "  def search(self, query: Query, top_k:int = 3) -> List[Tuple[str,float]]:\n",
        "    \"\"\"Search using the dense vector similarity\"\"\"\n",
        "\n",
        "    # Encode query\n",
        "    query_embedding = self.encode_text(query.text).squeeze(0) # output shape [hidden_size]\n",
        "\n",
        "    # compute cos sim\n",
        "    # norm vectors\n",
        "    query_norm = query_embedding / query_embedding.norm() # L2 normalization\n",
        "    doc_norms = self.doc_matrix / self.doc_matrix.norm(dim=1, keepdim=True) # normalize each doc\n",
        "\n",
        "    # compute dot product\n",
        "    similarities = torch.matmul(doc_norms, query_norm) # [n_docs]\n",
        "\n",
        "    print(f\"\\nDense search for: '{query.text}'\")\n",
        "    print(f\"Query embedding shape: {query_embedding.shape}\")\n",
        "    print(f\"Similarities: {similarities}\")\n",
        "\n",
        "    # get top-k indices\n",
        "    top_scores, top_indices = torch.topk(similarities, min(top_k, len(similarities)))\n",
        "\n",
        "    results = []\n",
        "    doc_ids = list(self.doc_embeddings.keys())\n",
        "    for idx, score in zip(top_indices, top_scores):\n",
        "      doc_id = doc_ids[idx]\n",
        "      results.append((doc_id, score.item()))\n",
        "      print(f\"  {doc_id}: {score.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "6jXFUvD8Uzuy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Dense Retrieval\n"
      ],
      "metadata": {
        "id": "uXiIwZki_k6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_retriever = DenseBERTRetriever(use_peft=torch.cuda.is_available())\n",
        "dense_retriever.index_documents(collection.documents)\n",
        "\n",
        "query = Query(query_id=\"q3\", text=\"war battle\")\n",
        "dense_results = dense_retriever.search(query, top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38OY6f19UzqL",
        "outputId": "6114f71e-38ef-473e-c2cc-e49dd8017e94"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indexing 4 documents with BERT...\n",
            "Encoded 'battle good fool wit love fore...': shape=torch.Size([1, 768])\n",
            "Encoded 'good fool wit love comedy mist...': shape=torch.Size([1, 768])\n",
            "Encoded 'battle battle battle good fool...': shape=torch.Size([1, 768])\n",
            "Encoded 'battle battle battle battle go...': shape=torch.Size([1, 768])\n",
            "Document matrix shape: torch.Size([4, 768])\n",
            "Document matrix:\n",
            " tensor([[-0.6397,  0.1917, -0.1856, -0.0429, -0.6057, -0.2334,  0.6254,  0.0160,\n",
            "         -0.2607, -0.2735,  0.1154,  0.1545,  0.3871,  0.2930,  0.1245,  0.1331,\n",
            "         -0.1775,  0.2469,  0.3730, -0.2732],\n",
            "        [-0.5598,  0.0561, -0.1338,  0.0238, -0.0677,  0.0127,  0.7196,  0.4464,\n",
            "         -0.3710,  0.0573,  0.2302,  0.0439,  0.2988,  0.4506, -0.0326,  0.1620,\n",
            "         -0.2610,  0.3622,  0.5300, -0.2685],\n",
            "        [-0.5756, -0.0466, -0.2201,  0.1625, -0.3577, -0.1111,  0.0881,  0.5446,\n",
            "         -0.3810, -0.3346,  0.0512,  0.1658,  0.0133,  0.5183,  0.1651,  0.1395,\n",
            "         -0.3084,  0.3998,  0.4595, -0.0961],\n",
            "        [-0.6548,  0.0731, -0.2220,  0.0399, -0.3843,  0.0043,  0.3855,  0.0692,\n",
            "         -0.4522, -0.2236,  0.0214,  0.1052, -0.0829,  0.2451, -0.0917,  0.2575,\n",
            "         -0.4492,  0.2584,  0.4318, -0.3824]])\n",
            "Encoded 'war battle...': shape=torch.Size([1, 768])\n",
            "\n",
            "Dense search for: 'war battle'\n",
            "Query embedding shape: torch.Size([768])\n",
            "Similarities: tensor([0.8853, 0.8714, 0.9142, 0.9167])\n",
            "  henry_v: 0.9167\n",
            "  julius_caesar: 0.9142\n",
            "  as_you_like_it: 0.8853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2zdGGfH_v8I"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}