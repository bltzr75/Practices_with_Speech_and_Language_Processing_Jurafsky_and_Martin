{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Setups"
      ],
      "metadata": {
        "id": "2RA6mP-CTb4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic evaluate transformers datasets accelerate bitsandbytes peft trl wandb\n",
        "!pip install -q torch torchinfo sentence-transformers faiss-cpu chromadb whoosh\n",
        "!pip install -q scikit-learn matplotlib seaborn pandas numpy\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyE_tbo_TlmZ",
        "outputId": "4de4d596-71f0-4cc0-ee67-11a022abe710"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOxZzI9xSbwl",
        "outputId": "af709621-7f4d-4296-a150-d7e06e682f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Tuple, Optional, Any, Union\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Pydantic for data validation\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "\n",
        "# HuggingFace ecosystem\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "import evaluate  # HF evaluate library for metrics\n",
        "\n",
        "# PEFT for parameter-efficient fine-tuning\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# TRL for RLHF and instruction tuning\n",
        "from trl import SFTTrainer, DPOTrainer\n",
        "\n",
        "# Accelerate for distributed training\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# 8-bit optimization\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# IR libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss  # Facebook AI Similarity Search for efficient nearest neighbor\n",
        "\n",
        "# Experiment tracking\n",
        "import wandb\n",
        "\n",
        "# Set device and seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Structures and Pydantic Validation"
      ],
      "metadata": {
        "id": "UU3vggkjVCDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Document(BaseModel):\n",
        "  \"\"\"Document with validation using pydantic v2\"\"\"\n",
        "\n",
        "  model_config = ConfigDict(\n",
        "      extra='forbid',  # no extra fields allowed\n",
        "      validate_assignment=True # validate on reassingment\n",
        "  )\n",
        "\n",
        "\n",
        "  doc_id: str = Field(..., min_length = 1, description=\"Document identifier\")\n",
        "  title: str = Field(..., min_length = 1, description=\"Document title\")\n",
        "  content: str = Field(..., min_length = 1, description=\"Document text content\")\n",
        "  metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n",
        "\n",
        "  @field_validator('content')\n",
        "  @classmethod\n",
        "  def validate_content(cls, v:str) -> str:\n",
        "    \"\"\"Ensure content is non-empty after stripping\"\"\"\n",
        "\n",
        "    if not v.strip():\n",
        "      raise ValueError(\"Content cannot be empty\")\n",
        "    return v.strip()\n",
        "\n",
        "\n",
        "  def get_tokens(self) -> List[str]:\n",
        "    \"\"\"Simple whitespace tokenization\"\"\"\n",
        "\n",
        "    return self.content.lower().split()\n",
        "\n",
        "\n",
        "class Query(BaseModel):\n",
        "  \"\"\"Query with validation\"\"\"\n",
        "\n",
        "  query_id: str = Field(..., min_length=1)\n",
        "  text: str = Field(..., min_length=1)\n",
        "  metadata: Dict[str, Any] = Field(default_factory=dict )\n",
        "\n",
        "  def get_tokens(self) -> List[str]:\n",
        "    \"\"\"Tokenize query text\"\"\"\n",
        "\n",
        "    return self.text.lower().split()\n",
        "\n",
        "\n",
        "class DocumentCollection(BaseModel):\n",
        "  \"\"\"Collection of documents with indexing capabilities\"\"\"\n",
        "\n",
        "  documents: List[Document] = Field(defualt_factory=list)\n",
        "\n",
        "  def add_document(self, doc: Document):\n",
        "    \"\"\"Add validated document to collection\"\"\"\n",
        "\n",
        "    self.documents.append(doc)\n",
        "\n",
        "\n",
        "  def get_vocabulary(self) -> List[str]:\n",
        "    \"\"\"Extract unique vocabulary from all the documents\"\"\"\n",
        "\n",
        "    vocab = set()\n",
        "    for doc in self.documents:\n",
        "      vocab.update(doc.get_tokens()) # Adding all the tokens to the set\n",
        "\n",
        "    return sorted(list(vocab))\n",
        "\n"
      ],
      "metadata": {
        "id": "I4DrB1W7Tot4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple test with Shakespeare corpus from the book"
      ],
      "metadata": {
        "id": "1Vv4tMqRYM4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_docs = [\n",
        "  Document(\n",
        "    doc_id=\"as_you_like_it\",\n",
        "    title=\"As You Like It\",\n",
        "    content=\"battle good fool wit love forest magic\",\n",
        "    metadata={\"genre\": \"comedy\", \"year\": 1599}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"twelfth_night\",\n",
        "    title=\"Twelfth Night\",\n",
        "    content=\"good fool wit love comedy mistaken identity\",\n",
        "    metadata={\"genre\": \"comedy\", \"year\": 1602}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"julius_caesar\",\n",
        "    title=\"Julius Caesar\",\n",
        "    content=\"battle battle battle good fool war rome politics\",\n",
        "    metadata={\"genre\": \"tragedy\", \"year\": 1599}\n",
        "  ),\n",
        "  Document(\n",
        "    doc_id=\"henry_v\",\n",
        "    title=\"Henry V\",\n",
        "    content=\"battle battle battle battle good wit war king england\",\n",
        "    metadata={\"genre\": \"history\", \"year\": 1599}\n",
        "  )\n",
        "]\n",
        "\n",
        "collection = DocumentCollection(documents=shakespeare_docs)\n",
        "print(f\"Created collection with {len(collection.documents)} documents\")\n",
        "print(f\"Vocabulary size: {len(collection.get_vocabulary())} unique terms\")\n",
        "print(f\"Sample vocabulary: {collection.get_vocabulary()[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olSdWCgwUkU8",
        "outputId": "26b6ce5d-58bb-429d-8c21-64b22c5517df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created collection with 4 documents\n",
            "Vocabulary size: 15 unique terms\n",
            "Sample vocabulary: ['battle', 'comedy', 'england', 'fool', 'forest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF with scikit-learn"
      ],
      "metadata": {
        "id": "Y5Z-hIxXaSu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFRetriever:\n",
        "  \"\"\"TF-idf retriever with pydantic validation and sklearn\"\"\"\n",
        "\n",
        "  def __init__(self, collection: DocumentCollection):\n",
        "    self.collection = collection\n",
        "    self.vectorizer = TfidfVectorizer(\n",
        "      lowercase=True,  #convert to lowercase\n",
        "      max_features=1000, # max vocab size\n",
        "      ngram_range=(1,2), # use unigrams and bigrams\n",
        "      sublinear_tf=True, # log(tf) instead of just raw tf\n",
        "      smooth_idf=True, # Laplace smoothing\n",
        "      norm='l2' # normalize with ridge for cos sim\n",
        "    )\n",
        "\n",
        "    self.tfidf_matrix = None\n",
        "    self.doc_ids = []\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    \"\"\"Build TF-IDF matrix from doc collection\"\"\"\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for doc in self.collection.documents:\n",
        "      texts.append(doc.content)\n",
        "      self.doc_ids.append(doc.doc_id) # storing doc ids\n",
        "\n",
        "\n",
        "    print(\"Documents from test\")\n",
        "    print(\"Row 0: battle good fool wit love forest magic\")\n",
        "    print(\"Row 1: good fool wit love comedy mistaken identity\")\n",
        "    print(\"Row 2: battle battle battle good fool war rome politics\")\n",
        "    print(\"Row 3: battle battle battle battle good wit war king england\")\n",
        "\n",
        "\n",
        "    # fit and transform docs to tf idf matrix\n",
        "    self.tfidf_matrix = self.vectorizer.fit_transform(texts) # [n_docs, n_features]  n_docs x vocab per doc\n",
        "    print(f\"\\nself.tfidf_matrix.shape: {self.tfidf_matrix.shape}\\n\")\n",
        "    print(f\"\\nself.tfidf_matrix:\\n {self.tfidf_matrix}\\n\")\n",
        "\n",
        "\n",
        "  def search(self,query: Query, top_k: int=3) -> List[Tuple[Document,float]]:\n",
        "    \"\"\"Search doc using tf-idf cos similarity\"\"\"\n",
        "\n",
        "    # Transform qry to tf-idf vector\n",
        "    query_vector = self.vectorizer.transform([query.text]) # [1, n_features]  single vector with all the features/words\n",
        "\n",
        "    # Compute cos similarity\n",
        "    similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "\n",
        "    print(f\"\\nQuery: '{query.text}'\")\n",
        "    print(f\"Query vector shape: {query_vector.shape}\")\n",
        "    print(f\"Similarities: {similarities}\")\n",
        "\n",
        "    # Top-k doc indices\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1] # sort desc\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "      doc = self.collection.documents[idx]\n",
        "      score = similarities[idx]\n",
        "      results.append((doc, score))\n",
        "      print(f\"  Rank {len(results)}: '{doc.title}' (score={score:.4f})\")\n",
        "\n",
        "    return results\n",
        ""
      ],
      "metadata": {
        "id": "TgY8OgY7YPkN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple test for the TF-IDF Retriever"
      ],
      "metadata": {
        "id": "meI2znRJeVII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = TFIDFRetriever(collection)\n",
        "retriever.fit()\n",
        "\n",
        "\n",
        "# Get the feature names (what each column represents)\n",
        "print(\"\\nUnigrams and bigrams:\\n\")\n",
        "feature_names = retriever.vectorizer.get_feature_names_out()\n",
        "for idx, feature in enumerate(feature_names):\n",
        "    print(f\"Column {idx}: '{feature}'\")\n",
        "\n",
        "\n",
        "test_query = Query(query_id=\"q1\", text=\"battle war\")\n",
        "results = retriever.search(test_query, top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCQ0GDetbpk6",
        "outputId": "a11ed16e-fd5d-4734-9867-013570289bcb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents from test\n",
            "Row 0: battle good fool wit love forest magic\n",
            "Row 1: good fool wit love comedy mistaken identity\n",
            "Row 2: battle battle battle good fool war rome politics\n",
            "Row 3: battle battle battle battle good wit war king england\n",
            "\n",
            "self.tfidf_matrix.shape: (4, 32)\n",
            "\n",
            "\n",
            "self.tfidf_matrix:\n",
            " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 50 stored elements and shape (4, 32)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.22325169970289915\n",
            "  (0, 11)\t0.18252289313304706\n",
            "  (0, 6)\t0.22325169970289915\n",
            "  (0, 29)\t0.22325169970289915\n",
            "  (0, 17)\t0.2757602638693091\n",
            "  (0, 9)\t0.34976692846571494\n",
            "  (0, 20)\t0.34976692846571494\n",
            "  (0, 2)\t0.22325169970289915\n",
            "  (0, 12)\t0.22325169970289915\n",
            "  (0, 8)\t0.2757602638693091\n",
            "  (0, 30)\t0.2757602638693091\n",
            "  (0, 19)\t0.34976692846571494\n",
            "  (0, 10)\t0.34976692846571494\n",
            "  (1, 11)\t0.17057535199597096\n",
            "  (1, 6)\t0.20863814180702328\n",
            "  (1, 29)\t0.20863814180702328\n",
            "  (1, 17)\t0.25770961257841624\n",
            "  (1, 12)\t0.20863814180702328\n",
            "  (1, 8)\t0.25770961257841624\n",
            "  (1, 30)\t0.25770961257841624\n",
            "  (1, 3)\t0.3268719661160508\n",
            "  (1, 21)\t0.3268719661160508\n",
            "  (1, 14)\t0.3268719661160508\n",
            "  (1, 18)\t0.3268719661160508\n",
            "  (1, 4)\t0.3268719661160508\n",
            "  (1, 22)\t0.3268719661160508\n",
            "  (2, 0)\t0.40964786723904145\n",
            "  (2, 11)\t0.15958830297517893\n",
            "  (2, 6)\t0.1951994036492684\n",
            "  (2, 2)\t0.1951994036492684\n",
            "  (2, 12)\t0.1951994036492684\n",
            "  (2, 26)\t0.2411100973882306\n",
            "  (2, 24)\t0.30581758590686003\n",
            "  (2, 23)\t0.30581758590686003\n",
            "  (2, 1)\t0.4082348815974165\n",
            "  (2, 7)\t0.30581758590686003\n",
            "  (2, 28)\t0.30581758590686003\n",
            "  (2, 25)\t0.30581758590686003\n",
            "  (3, 0)\t0.4262924719209452\n",
            "  (3, 11)\t0.14605156614348416\n",
            "  (3, 29)\t0.17864203128774342\n",
            "  (3, 2)\t0.17864203128774342\n",
            "  (3, 26)\t0.22065844852072933\n",
            "  (3, 1)\t0.4630765316640421\n",
            "  (3, 15)\t0.27987726257646384\n",
            "  (3, 5)\t0.27987726257646384\n",
            "  (3, 13)\t0.27987726257646384\n",
            "  (3, 31)\t0.27987726257646384\n",
            "  (3, 27)\t0.27987726257646384\n",
            "  (3, 16)\t0.27987726257646384\n",
            "\n",
            "\n",
            "Unigrams and bigrams:\n",
            "\n",
            "Column 0: 'battle'\n",
            "Column 1: 'battle battle'\n",
            "Column 2: 'battle good'\n",
            "Column 3: 'comedy'\n",
            "Column 4: 'comedy mistaken'\n",
            "Column 5: 'england'\n",
            "Column 6: 'fool'\n",
            "Column 7: 'fool war'\n",
            "Column 8: 'fool wit'\n",
            "Column 9: 'forest'\n",
            "Column 10: 'forest magic'\n",
            "Column 11: 'good'\n",
            "Column 12: 'good fool'\n",
            "Column 13: 'good wit'\n",
            "Column 14: 'identity'\n",
            "Column 15: 'king'\n",
            "Column 16: 'king england'\n",
            "Column 17: 'love'\n",
            "Column 18: 'love comedy'\n",
            "Column 19: 'love forest'\n",
            "Column 20: 'magic'\n",
            "Column 21: 'mistaken'\n",
            "Column 22: 'mistaken identity'\n",
            "Column 23: 'politics'\n",
            "Column 24: 'rome'\n",
            "Column 25: 'rome politics'\n",
            "Column 26: 'war'\n",
            "Column 27: 'war king'\n",
            "Column 28: 'war rome'\n",
            "Column 29: 'wit'\n",
            "Column 30: 'wit love'\n",
            "Column 31: 'wit war'\n",
            "\n",
            "Query: 'battle war'\n",
            "Query vector shape: (1, 32)\n",
            "Similarities: [0.14047611 0.         0.44515758 0.43973537]\n",
            "  Rank 1: 'Julius Caesar' (score=0.4452)\n",
            "  Rank 2: 'Henry V' (score=0.4397)\n",
            "  Rank 3: 'As You Like It' (score=0.1405)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0jvF62IebVz"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}