# -*- coding: utf-8 -*-
"""Chapter_13_Information Retrieval and Retrieval Augmented Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMavG78XCUKvfpZe5ECJ4NAm1gRRrAaW

#Initial Setups
"""

!pip install -q pydantic evaluate transformers datasets accelerate bitsandbytes peft trl wandb
!pip install -q torch torchinfo sentence-transformers faiss-cpu chromadb whoosh
!pip install -q scikit-learn matplotlib seaborn pandas numpy
!pip install transformers

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional, Any, Union
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import time
import math

# Pydantic for data validation
from pydantic import BaseModel, Field, field_validator, ConfigDict

from google.colab import userdata
HF_TOKEN= userdata.get('test_hf_1') # HF token


# HuggingFace ecosystem
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from datasets import Dataset as HFDataset
import evaluate  # HF evaluate library for metrics

# PEFT for parameter-efficient fine-tuning
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# TRL for RLHF and instruction tuning
from trl import SFTTrainer, DPOTrainer

# Accelerate for distributed training
from accelerate import Accelerator

# 8-bit optimization
import bitsandbytes as bnb

# IR libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import faiss  # Facebook AI Similarity Search for efficient nearest neighbor

# Experiment tracking
import wandb

# Set device and seeds
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(42)
np.random.seed(42)
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

"""#Data Structures and Pydantic Validation"""

class Document(BaseModel):
  """Document with validation using pydantic v2"""

  model_config = ConfigDict(
      extra='forbid',  # no extra fields allowed
      validate_assignment=True # validate on reassingment
  )


  doc_id: str = Field(..., min_length = 1, description="Document identifier")
  title: str = Field(..., min_length = 1, description="Document title")
  content: str = Field(..., min_length = 1, description="Document text content")
  metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

  @field_validator('content')
  @classmethod
  def validate_content(cls, v:str) -> str:
    """Ensure content is non-empty after stripping"""

    if not v.strip():
      raise ValueError("Content cannot be empty")
    return v.strip()


  def get_tokens(self) -> List[str]:
    """Simple whitespace tokenization"""

    return self.content.lower().split()


class Query(BaseModel):
  """Query with validation"""

  query_id: str = Field(..., min_length=1)
  text: str = Field(..., min_length=1)
  metadata: Dict[str, Any] = Field(default_factory=dict )

  def get_tokens(self) -> List[str]:
    """Tokenize query text"""

    return self.text.lower().split()


class DocumentCollection(BaseModel):
  """Collection of documents with indexing capabilities"""

  documents: List[Document] = Field(defualt_factory=list)

  def add_document(self, doc: Document):
    """Add validated document to collection"""

    self.documents.append(doc)


  def get_vocabulary(self) -> List[str]:
    """Extract unique vocabulary from all the documents"""

    vocab = set()
    for doc in self.documents:
      vocab.update(doc.get_tokens()) # Adding all the tokens to the set

    return sorted(list(vocab))

"""##Simple test with Shakespeare corpus from the book"""

shakespeare_docs = [
  Document(
    doc_id="as_you_like_it",
    title="As You Like It",
    content="battle good fool wit love forest magic",
    metadata={"genre": "comedy", "year": 1599}
  ),
  Document(
    doc_id="twelfth_night",
    title="Twelfth Night",
    content="good fool wit love comedy mistaken identity",
    metadata={"genre": "comedy", "year": 1602}
  ),
  Document(
    doc_id="julius_caesar",
    title="Julius Caesar",
    content="battle battle battle good fool war rome politics",
    metadata={"genre": "tragedy", "year": 1599}
  ),
  Document(
    doc_id="henry_v",
    title="Henry V",
    content="battle battle battle battle good wit war king england",
    metadata={"genre": "history", "year": 1599}
  )
]

collection = DocumentCollection(documents=shakespeare_docs)
print(f"Created collection with {len(collection.documents)} documents")
print(f"Vocabulary size: {len(collection.get_vocabulary())} unique terms")
print(f"Sample vocabulary: {collection.get_vocabulary()[:5]}")

"""# TF-IDF with scikit-learn"""

class TFIDFRetriever:
  """TF-idf retriever with pydantic validation and sklearn"""

  def __init__(self, collection: DocumentCollection):
    self.collection = collection
    self.vectorizer = TfidfVectorizer(
      lowercase=True,  #convert to lowercase
      max_features=1000, # max vocab size
      ngram_range=(1,2), # use unigrams and bigrams
      sublinear_tf=True, # log(tf) instead of just raw tf
      smooth_idf=True, # Laplace smoothing
      norm='l2' # normalize with ridge for cos sim
    )

    self.tfidf_matrix = None
    self.doc_ids = []


  def fit(self):
    """Build TF-IDF matrix from doc collection"""

    texts = []

    for doc in self.collection.documents:
      texts.append(doc.content)
      self.doc_ids.append(doc.doc_id) # storing doc ids


    print("Documents from test")
    print("Row 0: battle good fool wit love forest magic")
    print("Row 1: good fool wit love comedy mistaken identity")
    print("Row 2: battle battle battle good fool war rome politics")
    print("Row 3: battle battle battle battle good wit war king england")


    # fit and transform docs to tf idf matrix
    self.tfidf_matrix = self.vectorizer.fit_transform(texts) # [n_docs, n_features]  n_docs x vocab per doc
    print(f"\nself.tfidf_matrix.shape: {self.tfidf_matrix.shape}\n")
    print(f"\nself.tfidf_matrix:\n {self.tfidf_matrix}\n")


  def search(self,query: Query, top_k: int=3) -> List[Tuple[Document,float]]:
    """Search doc using tf-idf cos similarity"""

    # Transform qry to tf-idf vector
    query_vector = self.vectorizer.transform([query.text]) # [1, n_features]  single vector with all the features/words

    # Compute cos similarity
    similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()

    print(f"\nQuery: '{query.text}'")
    print(f"Query vector shape: {query_vector.shape}")
    print(f"Similarities: {similarities}")

    # Top-k doc indices
    top_indices = np.argsort(similarities)[-top_k:][::-1] # sort desc

    results = []

    for idx in top_indices:
      doc = self.collection.documents[idx]
      score = similarities[idx]
      results.append((doc, score))
      print(f"  Rank {len(results)}: '{doc.title}' (score={score:.4f})")

    return results

"""# Simple test for the TF-IDF Retriever"""

retriever = TFIDFRetriever(collection)
retriever.fit()


# Get the feature names (what each column represents)
print("\nUnigrams and bigrams:\n")
feature_names = retriever.vectorizer.get_feature_names_out()
for idx, feature in enumerate(feature_names):
    print(f"Column {idx}: '{feature}'")

test_query = Query(query_id="q1", text="battle war")
results = retriever.search(test_query, top_k=3)

"""### Display TF-IDF matrix"""

# make df
tfidf_df = pd.DataFrame(
  retriever.tfidf_matrix.toarray(),
  columns=feature_names,
  index=[f"Doc{i}" for i in range(len(shakespeare_docs))]
)
tfidf_df

"""# BM25 Scorer"""

class BM25Scorer:
  """BM25 scoring with parameter tuning"""

  def __init__(self, collection: DocumentCollection, k1: float=1.2, b:float=0.75):
    """
    k1: controls term frequency saturation (typically 1.2-2.0). Enough to give score to a repeated word but not making a word repeated 100 times 100 times more important.
    b: controls length normalization (0=no normalization, 1=full normalization). Regularization over too large docs.
    """

    self.collection = collection
    self.k1 = k1 # Term frequency saturation param
    self.b = b   # Length normalization param
    self.doc_lengths = {} # Store doc lengths
    self.avg_doc_length = 0 # Avg doc length
    self.doc_freqs = defaultdict(int) # doc freqs per term
    self.N = len(collection.documents) # total num of docs
    self.term_freqs = {} # Term freqs per doc

    self._compute_statistics()


  def _compute_statistics(self):
    """Copmute doc statistics for BM25"""

    total_length = 0

    for doc in self.collection.documents:
      tokens = doc.get_tokens()
      doc_length = len(tokens)
      self.doc_lengths[doc.doc_id] = doc_length
      total_length += doc_length

      # Count term freq for this doc
      term_freq = defaultdict(int)
      unique_terms = set()
      for token in tokens:
        term_freq[token] += 1 # add 1 on occurence
        unique_terms.add(token)

      self.term_freqs[doc.doc_id] = term_freq

      # Update doc frequencies
      for term in unique_terms:
        self.doc_freqs[term] += 1 # Increment doc freq per term


    # Compute avg length
    self.avg_doc_length = total_length / self.N


    print(f"BM25 Statistics:")
    print(f"  Documents: {self.N}")
    print(f"  Avg doc length: {self.avg_doc_length:.2f}")
    print(f"  Unique terms: {len(self.doc_freqs)}")


  def score(self, query: Query, doc: Document) -> float:
    """
    Compute BM25 score for query-document pair.

    BM25(q,d) = Σ IDF(t) * (tf(t,d) * (k1 + 1)) / (tf(t,d) + k1 * (1 - b + b * |d|/avgdl))

    Where for each term t in query q:
      - IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5)) — inverse document frequency
      - tf(t,d) = frequency of term t in document d
      - |d| = length of document d (number of terms)
      - avgdl = average document length in collection
      - k1 = term frequency saturation parameter (default 1.2). Keeps repeated more important but not in a linear scale per occureance.
      - b = length normalization parameter (0=none, 1=full, default 0.75). Penalizes too long docs.

    The formula balances term importance (IDF) with normalized term frequency,
    preventing bias toward longer documents while rewarding term repetition up to a limit.
    """

    score = 0.0
    query_terms = query.get_tokens() # query terms
    doc_length = self.doc_lengths[doc.doc_id]

    for term in query_terms:
      if term not in self.doc_freqs:
        continue # skip terms not in the collection


      # Compute IDF: log((N-df + 0.5)/ (df + 0.5) )

      df = self.doc_freqs[term] # Doc freq

      idf = math.log((self.N - df + 0.5) / (df + 0.5)) # Inv Doc Freq with smoothing
      print("\n\n\nidf = math.log((self.N - df + 0.5) / (df + 0.5))\n")
      print(f"{idf:.3f} = math.log(({self.N} - {df} + 0.5) / ({df} + 0.5))\n")


      # Get term freq in doc
      tf = self.term_freqs[doc.doc_id].get(term, 0)
      print(f"tf = {tf}\n\n")

      #Compute normalized term freq
      # tf_normalized (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |d| / avgdl ))

      norm_factor = 1 - self.b + self.b * (doc_length / self.avg_doc_length)
      tf_component = (tf * (self.k1 + 1 )) / (tf + self.k1 * norm_factor) # Normalized TF


      # Add to score
      term_score = idf * tf_component # idf * normalized tf
      score += term_score

      if tf > 0:  # Only print for terms that appear in document
        print(f"    Term '{term}': tf={tf}, df={df}, idf={idf:.3f}, contribution={term_score:.3f}")
        print("\n", "-"*50,"\n")

    return score


  def search(self, query: Query, top_k: int = 3) -> List[Tuple[Document, float]]:
    """Search documents using BM25 scoring"""

    scores = []

    print(f"\nBM25 Search for: '{query.text}'")

    for doc in self.collection.documents:
      score = self.score(query, doc) # compute bm25 score
      scores.append((doc, score))
      print(f"  '{doc.title}': {score:.4f}")

    # Sort desc by score
    scores.sort(key=lambda x: x[1], reverse=True)

    # Return top k
    return scores[:top_k]

# Test BM25 scorer
bm25 = BM25Scorer(collection, k1=1.2, b=0.75)
query = Query(query_id="q2", text="battle fool")
bm25_results = bm25.search(query, top_k=3)



print("\n\n")
print("\nThe negative IDF -0.847 here means these terms are so common they're actually penalizing the scores rather than helping.")

print("\nHigh IDF = term is RARE, appears in few documents -> very distinctive/important")
print("\nLow/Negative IDF = term is COMMON, appears in many documents -> not distinctive\n")


print("\nTop BM25 Results:")
for rank, (doc, score) in enumerate(bm25_results, 1):
    print(f"  {rank}. {doc.title}: {score:.4f}")

"""# Inverted Index Implementation"""

class InvertedIndex:
  """Efficient inverted index"""

  def __init__(self):
    self.index = defaultdict(list) # term -> list of (doc_id, positions)
    self.doc_lengths = {} #  doc_id -> document lenght
    self.doc_norms = {} # dog_id -> L2 norm for cos sim

  def add_document(self, doc: Document):
    """Add doc to inverted index"""

    tokens = doc.get_tokens()
    self.doc_lengths[doc.doc_id] = len(tokens)

    # track term freq for L2 norm
    term_freqs = defaultdict(int)

    # build postings listwith positions
    for position, token in enumerate(tokens):
      self.index[token].append((doc.doc_id, position))
      term_freqs[token] += 1

    # compute l2 norm for doc vector
    norm = math.sqrt(sum(tf**2 for tf in term_freqs.values()))
    self.doc_norms[doc.doc_id] = norm

    print(f"\nIndexed '{doc.title}': {len(tokens)} tokens, {len(term_freqs)} unique terms")


  def search_term(self, term: str) -> List[Tuple[str, List[int]]]:
    """Search for docs containing a term"""

    # if not contained term in docs retrieve empty list
    term = term.lower()
    if term not in self.index:
      return []

    # group positions by doc
    doc_positions = defaultdict(list)
    for doc_id, position in self.index[term]:
      doc_positions[doc_id].append(position) # group positions by doc

    return list(doc_positions.items()) # return (doc_id, positions) pairs


  def boolean_and(self,terms: List[str]) -> List[str]:
    """Find docs containing ALL terms (AND operation)"""
    if not terms:
      return []


    # get doc sets for each term
    doc_sets = []
    for term in terms:
      docs = set(doc_id for doc_id, _ in self.index[term.lower()])
      doc_sets.append(docs)


    # intersect all docs sets
    result = doc_sets[0] # start with tfirst set
    for doc_set in doc_sets[1:]:
      result = result.intersection(doc_set) # intersect subsequents

    return list(result)


  def boolean_or(self, terms: List[str]) -> List[str]:
    """Find documents containing ANY term (OR operation)"""

    result = set()
    for term in terms:
      docs = set( doc_id for doc_id, _ in self.index[term.lower()] )
      result = result.union(docs)

    return list(result)


  def print_index(self):
      """Print inverted index structure"""
      print("\nInverted Index:")
      for term in sorted(self.index.keys()):
          postings = self.index[term]
          doc_freq = len(set(doc_id for doc_id, _ in postings))  # Count unique documents
          print(f"\n  '{term}' -> df={doc_freq}, postings={postings}")  # Show first 5 postings

"""## Test Inverted Index"""

inv_index = InvertedIndex()
for doc in collection.documents:
  inv_index.add_document(doc)


inv_index.print_index()

# Test boolean search
print("\nBoolean Search Tests:")
print(f"  Documents with 'battle': {inv_index.search_term('battle')}")
print(f"  Documents with 'battle' AND 'fool': {inv_index.boolean_and(['battle', 'fool'])}")
print(f"  Documents with 'love' OR 'war': {inv_index.boolean_or(['love', 'war'])}")

"""# Examples of Different types of Embeddings

**Query:**  
"Where is the Louvre?"  

**Document:**  
"The Louvre Museum is located in Paris, France"  

---

## 1. Cross-Encoder (Full BERT)

**Input:**  
`[CLS] Where is the Louvre? [SEP] The Louvre Museum is located in Paris, France [SEP]`

**Process:**  
- Single BERT encoder processes query and document together.  
- Produces a single scalar score.  

**Output:**  
- Shape: `[1]`  
- Storage: Cannot pre-store (must re-encode for each query-document pair).  

---

## 2. Bi-Encoder

**Input:**  
- Query Encoder (BERTQ): `[CLS] Where is the Louvre?`  
- Document Encoder (BERTD): `[CLS] The Louvre Museum is located...`

**Process:**  
- Encode separately into vectors.  

**Output:**  
- Query vector: `[768 dims]`  
- Document vector: `[768 dims]`  
- Example:  
  - Query: `[-0.23, 0.41, ..., 0.87]`  
  - Document: `[0.15, -0.62, ..., 0.34]`  
- Score: `dot_product(query_vec, doc_vec) -> scalar`  
- Shape: `Query=[768], Doc=[768]`  
- Storage: Pre-store all document vectors.  

---

## 3. ColBERT

**Input Tokens:**  
- Query: `[Q] Where is the Louvre [PAD] ...`  
- Document: `[D] The Louvre Museum is located in Paris France`

**Process:**  
- Each token encoded separately with BERT + linear projection.  

**Output:**  
- Query matrix: `[32 × 128]`  
- Document matrix: `[9 × 128]`  
- Example (partial):  
  - Query: `[q_Q], [q_Where], [q_is], [q_the], [q_Louvre], [q_PAD] ...`  
  - Document: `[d_The], [d_Louvre], [d_Museum], [d_is], [d_located], [d_in], [d_Paris], [d_France]`  

**Scoring (MaxSim):**  
- For each query token, take maximum similarity with any document token.  
  - `q_Where -> max(d_located) = 0.82`  
  - `q_is -> max(d_is) = 0.95`  
  - `q_Louvre -> max(d_Louvre) = 0.99`  
- Ignore padding tokens.  
- Final score = sum of max similarities.  

**Storage:**  
- Pre-store all document token matrices.  

---

## Storage Comparison (1M Documents)

- Cross-Encoder: `0 vectors (cannot pre-store)`  
- Bi-Encoder: `1M × 768 floats ≈ 3 GB`  
- ColBERT: `1M × ~60 tokens × 128 floats ≈ 30 GB`  
<br>

# Dense Retrieval with BERT and PEFT
"""

class DenseBERTRetriever:
  """Dense retrieval using BERT embeddings with PEFT for efficiency"""

  def __init__(self, model_name:str = 'bert-base-uncased', use_peft: bool = True):
    self.device = device
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Load model with optional 8-bit quantization
    if use_peft and torch.cuda.is_available():
      bnb_config = BitsAndBytesConfig(
          load_in_8bit = True,
          bnb_8bit_compute_dtype=torch.float16,
          bnb_8bit_quant_type="nf8", # Quantization type selected: NF8 (Normal Float 8-bit)
          bnb_8bit_use_double_quant=True # # Double quantization for more memory savings
          )

      self.model = AutoModel.from_pretrained(
          model_name,
          quantization_config = bnb_config,
          device_map="auto" # It intelligently distributes the model's parameters across your system's resources, prioritizing faster devices like GPUs.
      )

      # prepare for k-bit training
      self.model = prepare_model_for_kbit_training(self.model)

      # Add LoRA adapters for fine-tuning
      peft_config = LoraConfig(
          r=8,
          lora_alpha=32, # scaling param
          target_modules=["query", "value"],  # Attention Layers
          lora_dropout=0.1,
          bias = "none",
          task_type = TaskType.FEATURE_EXTRACTION, # task type for embeddings
      )

      self.model = get_peft_model(self.model, peft_config)  # Returns a Peft model object from a model and a config

      self.model.print_trainable_parameters() # show parameter efficiency

    else:
      self.model = AutoModel.from_pretrained(model_name).to(self.device)


    self.doc_embeddings = {} # Cache for doc embeddings



  def encode_text(self, text:str) -> torch.Tensor:
    """Encode text to dense vector using BERT [CLS] special token"""

    # Tokenize text
    inputs = self.tokenizer(
        text,
        padding=True, # pad to max length in batch
        truncation=True,  # truncate to max length
        max_length=512, # max length
        return_tensors="pt"
    ).to(self.device)


    # get BERT outputs
    with torch.no_grad(): # we will do inference to only extract the embeddings for the indexing/search
      outputs = self.model(**inputs) # forward pass [1, sequence_length, 768]


    # using [CLS] token embedding as doc representation
    cls_embedding = outputs.last_hidden_state[:,0,:] # [batch_size, seq_length, hidden_size] -> [batch_size, hidden_size] so it is [1, 768]

    print(f"Encoded '{text[:30]}...': shape={cls_embedding.shape}")
    return cls_embedding.cpu()  # Move to CPU for storage


  def index_documents(self, documents: List[Document]):
    """Pre-compute and cache doc embeddings"""

    print(f"\nIndexing {len(documents)} documents with BERT...")

    for doc in documents:
      embedding = self.encode_text(doc.content)
      self.doc_embeddings[doc.doc_id] = embedding # cache embedding


    # stack all the embeddings for efficient similarity computation
    self.doc_matrix = torch.stack(list(self.doc_embeddings.values())).squeeze(1) # squeeze(1) removes dimension at index 1 so if it has size 1: [n_docs, 1, 768] -> [n_docs, 768]
    print(f"Document matrix shape: {self.doc_matrix.shape}")
    print(f"Document matrix:\n {self.doc_matrix[:,:20]}")


  def search(self, query: Query, top_k:int = 3) -> List[Tuple[str,float]]:
    """Search using the dense vector similarity"""

    # Encode query
    query_embedding = self.encode_text(query.text).squeeze(0) # output shape [hidden_size]

    # compute cos sim
    # norm vectors
    query_norm = query_embedding / query_embedding.norm() # L2 normalization
    doc_norms = self.doc_matrix / self.doc_matrix.norm(dim=1, keepdim=True) # normalize each doc

    # compute dot product
    similarities = torch.matmul(doc_norms, query_norm) # [n_docs]

    print(f"\nDense search for: '{query.text}'")
    print(f"Query embedding shape: {query_embedding.shape}")
    print(f"Similarities: {similarities}")

    # get top-k indices
    top_scores, top_indices = torch.topk(similarities, min(top_k, len(similarities)))

    results = []
    doc_ids = list(self.doc_embeddings.keys())
    for idx, score in zip(top_indices, top_scores):
      doc_id = doc_ids[idx]
      results.append((doc_id, score.item()))
      print(f"  {doc_id}: {score.item():.4f}")

"""## Testing Dense Retrieval

"""

dense_retriever = DenseBERTRetriever(use_peft=torch.cuda.is_available())
dense_retriever.index_documents(collection.documents)

query = Query(query_id="q3", text="war battle")
dense_results = dense_retriever.search(query, top_k=3)

"""#ColBERT Implementation"""

class ColBERTRetriever:
  """ColBERT: Efficient retireval via late interation over BERT with multi-vector embeddings of 128 dim per each token"""

  def __init__(self, model_name: str = "bert-base-uncased", dim:int= 128):
    self.device = device
    self.dim = dim
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
    self.bert = AutoModel.from_pretrained(model_name).to(self.device)

    # Linear layer from BERT outputs to lower dim for the embeddings (128)
    self.linear = nn.Linear(self.bert.config.hidden_size, dim).to(self.device)
    self.doc_embeddings = {} # Cache embeddings

  def encode_document(self, text:str, doc_id: str) -> torch.Tensor:
    """Encode document to token-level embeddings"""

    # Adding [D] special token for docs
    text = "[D] " + text # doc marker

    inputs = self.tokenizer(
      text,
      padding='max_length',
      truncation=True,
      max_length=128,
      return_tensors='pt'
    ).to(self.device)

    with torch.no_grad():
      outputs = self.bert(**inputs) # get bert outputs
      token_embeddings = outputs.last_hidden_state # [1,seq len, hiddn size]

      # Project to selected dimensionality (128)
      token_embeddings = self.linear(token_embeddings) # [1,seq_len, 128]

      # L2 Normalize each token embedding
      token_embeddings = F.normalize(token_embeddings, dim=-1, p=2)

    print(f"Encoded document '{doc_id}': shape={token_embeddings.shape}")

    return token_embeddings.cpu() # moving to cou for storage


  def encode_query(self, text:str) -> torch.Tensor:
    """Encode query to token-level embeddings"""

    # Adding [Q] special token for qrys
    text = "[Q] " + text # qry marker


    inputs = self.tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=32,
        return_tensors='pt'
    ).to(device)

    # get attention mask to identify real vs padding tokens
    attention_mask = inputs['attention_mask'] # [1, seq_len]

    with torch.no_grad():
      outputs = self.bert(**inputs) # get bert outputs
      token_embeddings = outputs.last_hidden_state # [1,seq_len, hiddn_size]

      # project to dim 128
      token_embeddings = self.linear(token_embeddings) # [1, seq_len, 128]

      # L2 norm
      token_embeddings = F.normalize(token_embeddings, p=2, dim=-1)

      # mask padding tokens
      token_embeddings = token_embeddings * attention_mask.unsqueeze(-1) # zero out padding


    return token_embeddings # keeping on device for cos sim


  def index_documents(self,documents:List[Document]):
    """Index all documents"""

    print(f"\nIndexing {len(documents)} documents with ColBERT...")

    for doc in documents:
      embeddings = self.encode_document(doc.content, doc.doc_id)
      self.doc_embeddings[doc.doc_id] = embeddings # cache embeddings

  def compute_maxsim(self, query_embeddings: torch.Tensor, doc_embeddings:torch.Tensor) -> float:
    """
    Compute MaxSim score between query and document
    Score(q,d) = Σ_i max_j (q_i · d_j) for all query tokens i and doc tokens j
    """

    query_embeddings = query_embeddings.to(self.device)
    doc_embeddings = doc_embeddings.to(self.device)


    # Compute all pairwise dot prod
    # [1, q_len, dim] @ [1, d_len, dim]^T = [1, q_len, d_len]
    similarity_matrix = torch.bmm(
        query_embeddings,              # [1, q_len, dim]  = [1, 32, 128]
        doc_embeddings.transpose(1,2)  # [1, dim, d_len]  = [1, 128, 128]

    )                          # Result: [1, q_len, d_len] = [1, 32, 128]

    # find max sim wit hany doc token for each query token
    max_similarities, _ =  similarity_matrix.max(dim=2) # [1, q_len] = [1, 32]

    # sum over all query tokens
    score = max_similarities.sum().item() # scalar

    return score

  def search(self, query: Query, top_k: int=3) -> List[Tuple[str, float]]:
    """Search using ColBERT MaxSim"""

    # encode qry
    query_embeddings = self.encode_query(query.text) # [1, q_len, dim]
    print(f"\nColBERT search for: '{query.text}'")
    print(f"Query embedding shape: {query_embeddings.shape}")


    scores = []

    for doc_id, doc_embeddings in self.doc_embeddings.items():
      score = self.compute_maxsim(query_embeddings, doc_embeddings)
      scores.append((doc_id, score))
      print(f"  {doc_id}: MaxSim={score:.4f}")


    # sort by score desc
    scores.sort(key=lambda x: x[1], reverse=True) # sort by score desc

    return scores[:top_k] # return only top k results

"""## Test ColBERT

"""

colbert = ColBERTRetriever(dim=128)
colbert.index_documents(collection.documents)

query = Query(query_id="q4", text="fool love")
colbert_results = colbert.search(query, top_k=3)

print("The ranking (julius_caesar > twelfth_night > as_you_like_it) appears essentially random given the score similarities. For this to work properly, it is needed to either use a pre-trained ColBERT model or fine-tune the encoders and linear projection on retrieval data with proper contrastive loss.")

