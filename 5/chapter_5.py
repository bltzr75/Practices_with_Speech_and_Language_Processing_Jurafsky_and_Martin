# -*- coding: utf-8 -*-
"""Chapter_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZtpdZLRFmcsZRHIwA7fxxlHcGm8TV8Y
"""

import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

"""## Sigmoid Function"""

def sigmoid(z):
  """
  Sigmoid Function: σ(z) = 1 / (1 + e^(-z))
  Maps any real value to the (0,1) range
  """

  return 1 / (1 + np.exp(-z))

# Visualization
z = np.linspace(-10,10,100) # 100 evenly spaced points between -10 and 10
plt.figure(figsize=(8,5))
plt.plot(z,sigmoid(z))
plt.grid(True, alpha=0.3) # alpha is the transparency
plt.xlabel("z")
plt.ylabel("σ(z)")
plt.title("Sigmoid Function")
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5) # Adding horizontal line
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5) # Adding vertical line
plt.show()

"""### Testing the property: 1 - σ(x) = σ(-x)"""

x = 3.14168
print(f"1 - σ({x}) = {1 - sigmoid(x):.6f}")
print(f"σ(-{x}) = {sigmoid(-x):.6f}")
print(f"Property holds: {np.isclose(1 - sigmoid(x), sigmoid(-x))}")  # Check if values are approximately equal

"""##Sentiment Classification Example"""

class LogisticRegression:
  def __init__(self, n_features):
    """ Initializing weights and bias to zero""" ### In LogReg we initialize to 0 usually, but in deep learning to randomly close to 0 values breaking simmetry (if all weights started at zero, neurons would learn the same features)
    self.w = np.zeros(n_features)
    self.b = 0

  def predict_proba(self,x):
    """
    Compute P(y=1|x) with logistic regression
    x: feature vector
    """

    z = np.dot(self.w, x) + self.b # Linear combination: wx + b
    return(sigmoid(z))

  def predict(self,x):
    """Make binary the prediction using a .5 threshold"""
    return 1 if self.predict_proba(x) > 0.5 else 0

  def __call__(self):
    return("Logistic Regression Model with Weights: ", self.w, "Intercept:", self.b)

# Example from the book (sentiment analysis features)
features = {
    'positive_words': 3,      # Count of positive sentiment words
    'negative_words': 2,      # Count of negative sentiment words
    'has_no': 1,             # Binary: contains word "no"
    'pronouns': 3,           # Count of 1st/2nd person pronouns
    'has_exclamation': 0,    # Binary: contains "!"
    'log_word_count': 4.19   # Log of document length
}

# Converting dic to numpy array (order matters)
x = np.array([features['positive_words'],
              features['negative_words'],
              features['has_no'],
              features['pronouns'],
              features['has_exclamation'],
              features['log_word_count']
])

# Creating model with weights from the book
model = LogisticRegression(n_features=6)
model.w = np.array([2.5, -5.0, -1.2, 0.5, 2.0, 0.7])  # Learned weights from the book
model.b = 0.1

print(model())

# Calculate probabilities
prob_positive = model.predict_proba(x)
prob_negative = 1 - prob_positive

print(f"Feature vector x: {x}")
print(f"Weights w: {model.w}")
print(f"Bias b: {model.b}")
print(f"z = w·x + b = {np.dot(model.w, x) + model.b:.3f}")
print(f"P(positive|x) = {prob_positive:.2f}")
print(f"P(negative|x) = {prob_negative:.2f}")
print(f"Prediction: {'positive' if model.predict(x) == 1 else 'negative'}")

"""##Softmax Function for Multinomial Classification"""

def softmax(z):
  """
  Softmax Function for a vector z
  softmax(z_i) = exp(z_i) / Σ_j exp(z_j)
  """
  # Applying normalization to the vector to avoid overflow
  exp_z = np.exp(z - np.max(z)) # Substracting the top value to apply later a np.exp and use the ratios instead
  return (exp_z / np.sum(exp_z) ) # Relative distances stay the same

# Example from the textbook
z = np.array([0.6, 1.1, -1.5, 1.2, 3.2, -1.1])  # Raw scores (logits) for 6 classes
probabilities = softmax(z)

print("Input z:", z)
print("Softmax output:", np.round(probabilities, 2))
print("Sum of probabilities:", np.sum(probabilities))



### Visualization for 3 classes: both variable values and static
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))

### ax1 left plot: one input variates the rest stay fixed
z_values = np.linspace(-5, 5, 100)

# Matix like [z_value, 0,0]
z_matrix = np.array([z_values, np.zeros_like(z_values), np.zeros_like(z_values)]).T
print("z_matrix[:3]")
print(z_matrix[:3])

probs = np.array([softmax(z_row) for z_row in z_matrix])
print("probs[:3]:")
print(probs[:3])

ax1.plot(z_values, probs[:,0], label= "Class 0")
ax1.plot(z_values, probs[:,1], label= "Class 1")
ax1.plot(z_values, probs[:,2], label= "Class 2")

ax1.set_xlabel('z[0] value')
ax1.set_ylabel('Probability')
ax1.set_title('Softmax: Varying z[0], z[1]=z[2]=0')
ax1.legend()
ax1.grid(True, alpha=0.3)


### ax2 right plot of constant logitss
classes = ['Positive', 'Negative', 'Neutral']
example_z = np.array([2.5, 1.0, -0.5])  # Logits for 3-way sentiment
example_probs = softmax(example_z)

ax2.bar(classes, example_probs)
ax2.set_ylabel('Probability')
ax2.set_title(f'Softmax Example: z = {example_z}')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""## Cross-Entropy Loss Function

"""

def cross_entropy_loss(y_true, y_pred):
  """
  Binary Cross-Entropy loss
  When y=1: only first term matters, when y=0: only second term matters
  L = -[       y * (log(ŷ) )     +     ( (1-y) * log(1-ŷ)     )]
  """

  epsilon = 1e-15 # for preventing log(0), which outputs as undefined
  y_pred = np.clip(y_pred, epsilon, 1 -epsilon) # limitting the value so it is defined


  return -(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))

"""## Example from the book"""

y_pred = 0.7
y_true_positive = 1 # + sentiment
y_true_negative = 0 # - sentiment

loss_correct = cross_entropy_loss(y_true_positive, y_pred) # Loss when prediction is correct
loss_incorrect = cross_entropy_loss(y_true_negative, y_pred) # Loss when prediction is incorrect

print(f"Model prediction: {y_pred}")
print(f"Loss when true label is positive (y=1): {loss_correct:.4f}")
print(f"Loss when true label is negative (y=0): {loss_incorrect:.4f}")

### Vis

y_pred_range = np.linspace(0.01, 0.99, 100)  # Avoid 0 and 1 for numerical stability, log(0) not def
loss_when_y1 = cross_entropy_loss(1, y_pred_range)  # Loss curve when true label is 1
loss_when_y0 = cross_entropy_loss(0, y_pred_range)  # Loss curve when true label is 0


plt.figure(figsize=(12,5))

plt.xlabel('Predicted Probability (ŷ)')
plt.ylabel('Cross-Entropy Loss')


plt.plot(y_pred_range, loss_when_y1, label="Loss with y=1", linewidth=2)
plt.plot(y_pred_range, loss_when_y0, label="Loss with y=0", linewidth=2)



plt.title('Cross-Entropy Loss Function')
plt.legend()

plt.show()

"""## Gradient Descent"""

class LogisticRegressionWithGD:
  def __init__(self, n_features, learning_rate=0.1):
    self.w = np.zeros(n_features)
    self.b = 0
    self.learning_rate = learning_rate
    self.losses = []

  def forward(self, X):
    """Compute predictions for batch X"""
    z = np.dot(X, self.w) + self.b
    return sigmoid(z) # applying sigmoid element-wise

  def compute_loss(self, X, y):
    """Compute average cross-entropy loss over batch"""
    m = len(y) # num of examples
    y_pred = self.forward(X)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1-epsilon)

    # Average loss: (1/m) * Σ[y*log(ŷ) + (1-y)*log(1-ŷ)]
    loss = -(1/m) * np.sum( y*np.log(y_pred) + (1-y) * np.log(1-y_pred) )
    return(loss)

  def compute_gradients(self, X, y):
    """Compute gradients using vectorized operations."""
    m = len(y) # Batch size
    y_pred = self.forward(X) # predictions

    # Gradient of weights: (1/m) * (ŷ - y)ᵀ X
    # Shape: (n_features,) = (m,) @ (m, n_features)
    dw = (1/m) *  np.dot((y_pred-y), X)

    # Gradient of bias: (1/m) * Σ(ŷ - y)
    db = (1/m) * np.sum(y_pred-y)

    print( "dw:", dw, "db: ", db)
    return( dw,db)



  def train_step(self, X, y):
    """One step of gradient descent"""

    # Calc gradients
    dw,db = self.compute_gradients(X,y)

    # Update params in opposite direction to gradients
    self.w -= self.learning_rate * dw
    self.b -= self.learning_rate * db

    # Track loss
    loss = self.compute_loss(X, y)
    self.losses.append(loss)

    return loss

# Create synthetic dataset for binary classification
np.random.seed(42)  # For reproducibility
n_samples = 1000
n_features = 2

# Generating 2 classes with diff centers
X_class0 = np.random.randn(n_samples//2, n_features) - 1.5  # Centered at (-1.5, -1.5)
X_class1 = np.random.randn(n_samples//2, n_features) + 1.5  # Centered at (1.5, 1.5)

X = np.vstack([X_class0, X_class1]) # Vertical stack
print( "Vertical Stack: ", X[:5], "\n...", X[-5:])

y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])  # Labels: 0s then 1s
print( "Horizontal Stack: ", y[:5], "...", y[-5:])


# Shuffle data to mix classes
indices = np.random.permutation(n_samples)
X, y = X[indices], y[indices]

print( "X: ", X[:5], "...", X[-5:])
print( "y: ", y[:5], "...", y[-5:])


# Train model
model = LogisticRegressionWithGD(n_features=2, learning_rate=.1)

# Training
n_epochs = 100
for epoch in range(n_epochs):
  loss = model.train_step(X, y)

  if epoch % 10 == 0:
    print(f"\n\nEpoch {epoch}, Loss: {loss:.4f}")
    print(f"Current weights: {model.w}")
    print(f"Current bias: {model.b}\n\n")

# Plot results
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot decision boundary
h = .02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = model.forward(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

ax1.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')
ax1.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6)
ax1.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6)
ax1.set_xlabel('Feature 1')
ax1.set_ylabel('Feature 2')
ax1.set_title('Decision Boundary')
ax1.legend()

# Plot loss curve
ax2.plot(model.losses)
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Loss')
ax2.set_title('Training Loss')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n\nFinal weights: {model.w}")
print(f"Final bias: {model.b}\n\n")

"""## Mini-batch Gradient Descent"""

def create_mini_batches(X,y, batch_size):
  """Mini batches for training"""

  m=len(y)
  mini_batches = []

  # Shuffling
  indices = np.random.permutation(m)
  print("indices: ", str(indices[5]))

  X_shuffled = X[indices]
  y_shuffled = y[indices]


  # Creating mini batches with the batch_size as limit
  for i in range(0, m, batch_size):
    X_batch = X_shuffled[i:i+batch_size]
    y_batch = y_shuffled[i:i+batch_size]

    mini_batches.append((X_batch, y_batch))

  # print(type(mini_batches))
  return(mini_batches)

indices_sample = np.random.permutation(20)
X_sample, y_sample = X[indices_sample], y[indices_sample]

mini_batches = create_mini_batches(X_sample,y_sample,5)
mini_batches

# Comparing diff batch sizes
batch_sizes = [1,32,128,1000]  # 1 = SGD, 1000 = full batch
colors = ['red', 'green', 'blue', 'purple']

plt.figure(figsize=(10,6))

for batch_size, color in zip(batch_sizes, colors):
  model = LogisticRegressionWithGD(n_features=2, learning_rate=.1)
  losses = []


  for epoch in range(20):
    mini_batches = create_mini_batches(X,y,batch_size)
    epoch_loss = 0

    for X_batch, y_batch in mini_batches:
      loss = model.train_step(X_batch, y_batch)
      epoch_loss += loss * len(y_batch) # Weight by batch size

    avg_loss = epoch_loss / len(y) # Average over all the examples size
    losses.append(avg_loss)

  plt.plot(losses, label=f"Batch size = {batch_size}", color=color, linewidth=2)

plt.xlabel("Epoch")
plt.ylabel("Avg Loss")
plt.title("Comparison over Different Batch Sizes on Training")
plt.legend()
plt.show()

"""#### The batch size comparison shows mini-batch gradient descent with batch size 32 achieving optimal performance, converging rapidly to the lowest final loss (aprox 0.04) with smooth training dynamics. SGD (batch=1) exhibits fastest initial descent but plateaus at higher loss (aprox 0.05) due to gradient noise. Batch size 128 follows similar convergence pattern to batch 32 with slightly slower speed. Full batch gradient descent (batch=1000) demonstrates poorest performance, starting at highest initial loss (aprox 0.58) and remaining at aprox 0.17 after 20 epochs, indicating suboptimal learning rate for large batch updates and insufficient training duration. The experiment illustrates that moderate batch sizes outperform extremes by balancing gradient accuracy with update frequency. Batch size 32 provides optimal trade-off between convergence speed and final performance. Results confirm that mini-batch gradient descent often superior to both stochastic and full batch methods in practical applications.

# L2 Regularization: Ridge
"""

class LogisticRegressionL2:
  def __init__(self, n_features, learning_rate=0.1, alpha=0.01):
    self.w = np.zeros(n_features)
    self.b = 0
    self.learning_rate = learning_rate
    self.alpha = alpha

  def compute_loss_with_regularization(self, X, y):
    m = len(y)
    y_pred = sigmoid( np.dot(X, self.w) + self.b  )

    # Cross-Entropy Loss
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1-epsilon) # y_pred clipped to avoid log(o) errors
    ce_loss = (-1/m) * np.sum( ( y * np.log(y_pred))   + ( (1-y) * np.log(1-y_pred) ) )

    # L2 regularization term: (α/2m) * Σw²
    l2_term = (self.alpha/(2*m)) * np.sum((self.w ** 2)) # Does not regularize bias


    return ce_loss + l2_term

  def train_step(self, X,y):
    """ Gradient descent with L2 regularization"""

    m = len(y)
    y_pred = sigmoid((np.dot(X, self.w)) + self.b)

    # Gradient with reg.
    # dw has extra term: (α/m) * w
    dw = (1/m) * np.dot((y_pred - y), X) + (self.alpha/m) * self.w
    db = np.sum(y_pred - y) # Bias is not regularized


    # Update params
    self.w -= self.learning_rate * dw
    self.b -= self.learning_rate * db

# Generate data set
np.random.seed(42)
n_samples = 200
n_features = 20 # Using many feature for regularization effect

# Generate data, only 5 features matter
X = np.random.randn(n_samples, n_features)
true_weights = np.zeros(n_features)
true_weights[:5] = np.array([3, -2, 1.5, -1, .5]) # Only 5 features have non-zero weights

# Adding noise to make classif imperfect
y = (sigmoid(np.dot(X, true_weights) +np.random.randn(n_samples) * .1 ) > .5 ).astype(int)

# Train models with idff regularization strengths
alphas = [0, .01, 0.1, 1.0] # No reg, light reg, medium reg, heavy regularization
models = []


# Modified training loop that uses compute_loss_with_regularization
for alpha in alphas:
  model = LogisticRegressionL2(n_features, learning_rate=0.001, alpha=alpha)
  losses = []  # Track losses

  # Train for 2000 iterations
  for iteration in range(2000):
    # Compute and store loss before update
    loss = model.compute_loss_with_regularization(X, y)
    losses.append(loss)

    # Perform gradient update
    model.train_step(X, y)

    # Optionally print progress
    if iteration % 100 == 0:
      print(f"Alpha={alpha}, Iter={iteration}, Loss={loss:.4f}")


  models.append(model)



# Visualize weight magnitudes
fig, axes = plt.subplots(2,2, figsize=(12,10))
axes = axes.ravel() #Flatten 2x2 array into 1dim for indexing

for i, (alpha,model) in enumerate(zip(alphas,models)):
  axes[i].bar(range(n_features), np.abs(model.w))  # Plot absolute weight values
  axes[i].set_xlabel('Feature Index')
  axes[i].set_ylabel('|Weight|')
  axes[i].set_title(f'α = {alpha}')
  axes[i].set_ylim(0, 3.5)  # Same scale for comparison

  # Important features go red
  for j in range(5):
    axes[i].axvline(x=j, color='red', linestyle='--', alpha=0.3)

plt.tight_layout()
plt.show()

# Print weight statistics
for alpha, model in zip(alphas, models):
  print(f"\nα = {alpha}:")
  print(f"  Sum of squared weights: {np.sum(model.w**2):.4f}")
  print(f"  Number of 'small' weights (|w| < 0.1): {np.sum(np.abs(model.w) < 0.1)}")

"""#### With learning rate 0.001, the L2 regularization results demonstrate expected behavior for logistic regression. Sum of squared weights remains reasonable around 0.39 across all regularization strengths, decreasing slightly from 0.3889 (α=0) to 0.3853 (α=1.0). The model consistently identifies exactly 15 weights as small (|w| < 0.1), correctly distinguishing the 15 irrelevant features from the 5 true predictive features in the synthetic dataset. This stability across regularization parameters indicates proper convergence and successful feature identification. The modest decrease in weight magnitudes with increasing α reflects appropriate L2 regularization behavior - providing weight shrinkage without forcing weights to zero. The lower learning rate enables convergence to optimal weights where the sparse structure of the true model is preserved regardless of regularization strength. These results confirm the implementation correctly balances fitting the data with regularization constraints, achieving both accurate classification and appropriate weight penalization.

##Integrated Example: Complete Text Classification System
"""

class TextClassifier:
  """
  Complete text classifier using log regression
  Integrates all concepts: features, sigmoid/softmax, loss, gradient descent, regularization
  """

  def __init__(self, n_classes = 2, learning_rate=0.001, batch_size = 32, alpha = 0.01, max_features=1000):
    self.n_classes = n_classes
    self.learning_rate = learning_rate
    self.batch_size = batch_size
    self.alpha = alpha
    self.max_features = max_features
    self.vocabulary = {}
    self.W = None # Weight matrix
    self.b = None # Bias vector
    self.losses = []

  def extract_features(self, texts):
    """Conversion of texts to feature vectors"""

    # Build vocabulary if does not exist
    if not self.vocabulary:
      word_counts = Counter()
      for text in texts:
        words = text.lower().split()
        word_counts.update(words)

      # Keep most common words
      most_common = word_counts.most_common(self.max_features)
      self.vocabulary = {word: idx for idx, (word, _) in enumerate(most_common)}

    # Converting texts to feature vectors
    n_texts = len(texts)
    n_features = len(self.vocabulary) + 3 # +3 special features
    X = np.zeros((n_texts, n_features))

    for i, text in enumerate(texts):
      words = text.lower().split()

      # Bag of words features
      for word in words:
        if word in self.vocabulary:
          X[i, self.vocabulary[word]] += 1


      # Special features from the book
      X[i, -3] = 1 if '!' in text else 0  # Has exclamation
      X[i, -2] = 1 if 'no' in text.lower() or 'not' in text.lower() else 0  # Negation
      X[i, -1] = np.log(len(words) + 1)  # Log word count

    return  X


  def softmax(self, z):
    """Softmax for multiclass and sigmoid for binary"""

    # Sigmoid for binary
    if self.n_classes == 2:
      return sigmoid(z[:,1]) # Prob of class 1

    else:
    # Softmax for multi-classes
      exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
      return(exp_z/np.sum(exp_z, axis=1, keepdims=True))

  def forward(self, X):
    """Forward pass, computes probs"""
    # Z = XW + b, shape: (batch_size, n_classes)
    Z = np.dot(X, self.W) + self.b
    return self.softmax(Z)

  def compute_loss(self, X, y):
    """Cross-entropy loss with L2 reg"""
    m = len(y) # Examples size
    probs = self.forward(X)

    if self.n_classes == 2:
      # Binary Cross-Entropy
      epsilon = 1e-15
      probs = np.clip(probs, epsilon, 1-epsilon) # Avoiding issues with log(0) or log(1) being undefined
      ce_loss = -(1/m) * np.sum(y * np.log(probs) + (1-y) * np.log(1-probs))

    else:
      # Multi-class cross-entropy
      # Creating one-hot encoding of y
      y_onehot = np.zeros((m, self.n_classes))
      y_onehot[np.arange(m), y] = 1
      # print("y_onehot: ", str(y_onehot[:5]))

      epsilon = 1e-15
      probs = np.clip(probs, epsilon, 1-epsilon) # Avoiding issues with log(0) or log(1) being undefined
      ce_loss = -(1/m) * np.sum(y_onehot * np.log(probs)) # We just need the left side of the binary case

    # L2 Regularization
    l2_loss = (self.alpha/(2*m)) * np.sum(self.W**2)

    return ce_loss + l2_loss


  def compute_gradients(self, X, y):
    """ Compute gradients for weights and bias params"""
    m = len(y)


    if self.n_classes == 2:
      # Binary case
      probs = self.forward(X)
      # Gradient for class 1
      dW1 = (1/m) * np.dot(X.T, (probs - y)) + (self.alpha/m) * self.W[:,1]
      # Gradietn for class 0
      dW0 = (1/m) * np.dot(X.T, (y - probs)) + (self.alpha/m) * self.W[:,0]

      dW = np.column_stack([dW0, dW1])

      # Bias gradients
      db1 = (1/m) * np.sum(probs-y)
      db = np.array([-db1, db1])

    else:
      # Multi-class case
      probs = self.forward(X)

      # One-hot encoded labels
      y_onehot = np.zeros((m, self.n_classes))
      y_onehot[np.arange(m), y] = 1

      #Gradients
      diff = probs - y_onehot # Shape (m, n_classes)
      dW = (1/m) * np.dot(X.T, diff) + (self.alpha/m) * self.W
      db = (1/m) * np.sum(diff, axis=0)

    return dW, db


  def fit(self, texts, labels, epochs=50, verbose=True):
    """Train the classifier on texts and labels"""

    #Extract features
    X = self.extract_features(texts)
    y = np.array(labels)

    # Initialize weights

    n_features = X.shape[1]
    if self.W is None:
      self.W = np.random.randn(n_features, self.n_classes) * 0.01 # Small random init
      self.b = np.zeros(self.n_classes)

    # Training
    n_samples = len(y)

    for epoch in range(epochs):
      # Shuffle
      indices = np.random.permutation(n_samples)
      X_shuffled = X[indices]
      y_shuffled = y[indices]

      epoch_loss = 0


      # Mini-batch training
      for i in range(0, n_samples, self.batch_size):
        # Get batch
        batch_end = min(i + self.batch_size, n_samples)
        X_batch = X_shuffled[i: batch_end]
        y_batch = y_shuffled[i: batch_end]


        # Compute gradients
        dW, db = self.compute_gradients(X_batch, y_batch)

        # Update parameters
        self.W -= self.learning_rate * dW
        self.b -= self.learning_rate * db


        # Track Loss
        batch_loss = self.compute_loss(X_batch, y_batch)
        epoch_loss += batch_loss * len(y_batch)

      # Avrge epoch loss
      avg_loss = epoch_loss / n_samples
      self.losses.append(avg_loss)

      if verbose and epoch % 10 == 0:
        acc = self.score(texts, labels)
        print(f"Epoch {epoch}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2%}")


  def predict_proba(self, texts):
      """Get probability predictions for texts"""
      X = self.extract_features(texts)
      return self.forward(X)


  def predict(self, texts):
      """Get class predictions for texts"""
      probs = self.predict_proba(texts)

      if self.n_classes == 2:
          return (probs > 0.5).astype(int) # Converts to 1s or 0s with .5 threshold
      else:
          return np.argmax(probs, axis=1)


  def score(self, texts, labels):
      """Compute accuracy"""
      predictions = self.predict(texts)
      return np.mean(predictions == labels)

# Example 1: Binary sentiment classification
print("=== Binary Sentiment Classification ===")

# Expanded movie review dataset to avoid overfitting
binary_texts = [
    # Strong positive reviews
    "This movie was absolutely fantastic! Best film I've seen all year.",
    "Loved every minute! Highly recommend to everyone!",
    "Amazing cinematography and stellar performances!",
    "Brilliant storytelling! A masterpiece of modern cinema.",
    "Outstanding! This film deserves every award it gets.",
    "Phenomenal acting and breathtaking visuals. A must-see!",
    "Incredible movie! I was captivated from start to finish.",
    "Absolutely wonderful! This director is a genius.",
    "Best movie experience I've had in years. Truly exceptional!",
    "Magnificent! Everything about this film is perfect.",
    "A triumph! This movie will be remembered for decades.",
    "Spectacular achievement in filmmaking. Bravo!",

    # Moderate positive reviews
    "Pretty good movie. Worth watching on a weekend.",
    "Enjoyable film with some great moments.",
    "Nice story and decent acting throughout.",
    "Good entertainment value. Recommended.",
    "Solid film with strong performances.",
    "Well-made movie that delivers on its promises.",

    # Strong negative reviews
    "Terrible waste of time. Do not watch this garbage.",
    "Boring and predictable. I fell asleep halfway through.",
    "What a disappointment. Expected so much more.",
    "Awful movie. One of the worst I've ever seen.",
    "Complete disaster. Save your money and time.",
    "Horrible acting and nonsensical plot. Avoid at all costs!",
    "Painfully bad. I walked out after 30 minutes.",
    "Absolute trash. How did this get made?",
    "Dreadful experience. Want my money back!",
    "Worst movie of the year. Completely unwatchable.",
    "Total failure on every level. Embarrassingly bad.",
    "Mind-numbingly boring. A complete waste.",

    # Moderate negative reviews
    "Not bad, but not great either. Just okay.",
    "Decent story but poor execution. Could have been better.",
    "Mediocre at best. Nothing special here.",
    "Disappointing film that fails to deliver.",
    "Below average. Many better options available.",
    "Forgettable movie with weak storyline.",

    # Mixed/neutral reviews (labeled based on overall sentiment)
    "Has its moments but overall disappointing.",  # negative
    "Some good parts but mostly boring.",  # negative
    "Great acting saved an otherwise poor script.",  # positive
    "Terrible plot but amazing cinematography.",  # negative
    "Started strong but fell apart in the end.",  # negative
    "Despite flaws, still entertaining enough.",  # positive
]

binary_labels = [
    # Strong positive (12)
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    # Moderate positive (6)
    1, 1, 1, 1, 1, 1,
    # Strong negative (12)
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    # Moderate negative (6)
    0, 0, 0, 0, 0, 0,
    # Mixed reviews (6)
    0, 0, 1, 0, 0, 1
]

# Split into train/test sets as textbook recommends
from sklearn.model_selection import train_test_split

X_train_texts, X_test_texts, y_train, y_test = train_test_split(
    binary_texts, binary_labels, test_size=0.25, random_state=42, stratify=binary_labels
)

print(f"Training set size: {len(X_train_texts)} examples")
print(f"Test set size: {len(X_test_texts)} examples")
print(f"Class distribution - Positive: {sum(binary_labels)}, Negative: {len(binary_labels) - sum(binary_labels)}")

# Train binary classifier with regularization to prevent overfitting
binary_clf = TextClassifier(
    n_classes=2,
    learning_rate=0.1,  # Lower learning rate for stability
    alpha=0.1,  # Stronger L2 regularization as per Section 5.7
    batch_size=8  # Smaller batch size for small dataset
)

# Train on training set only
binary_clf.fit(X_train_texts, y_train, epochs=100, verbose=True)

# Evaluate on test set
test_predictions = binary_clf.predict(X_test_texts)
test_accuracy = np.mean(test_predictions == y_test)
print(f"\nTest Set Accuracy: {test_accuracy:.2%}")

# Additional evaluation metrics
train_accuracy = binary_clf.score(X_train_texts, y_train)
print(f"Training Set Accuracy: {train_accuracy:.2%}")

# Check for overfitting
if train_accuracy - test_accuracy > 0.1:
    print("Warning: Model may be overfitting (large train-test gap)")
else:
    print("Good generalization (small train-test gap)")

# Test on completely new examples
new_test_texts = [
    "This film is an absolute masterpiece! Stunning!",
    "Completely boring. Fell asleep twice.",
    "It's okay. Nothing special but watchable."
]

predictions = binary_clf.predict(new_test_texts)
probs = binary_clf.predict_proba(new_test_texts)

print("\n=== Predictions on New Examples ===")
for text, pred, prob in zip(new_test_texts, predictions, probs):
    sentiment = "Positive" if pred == 1 else "Negative"
    confidence = prob if pred == 1 else (1 - prob)
    print(f"Text: '{text}'")
    print(f"  Prediction: {sentiment} (confidence: {confidence:.2%})\n")

# Multi-class sentiment classification implementation
print("\n=== Multi-class Sentiment Classification ===")

# Movie review dataset with three sentiment categories
multi_texts = [
   # Positive reviews
   "This movie was absolutely fantastic! Best film ever!",
   "Loved it! Amazing from start to finish!",
   "Incredible! A true masterpiece!",
   "Superb acting and direction. Highly recommended!",
   "A stunning achievement in cinema. Unforgettable experience!",
   "Brilliant screenplay with perfect execution!",
   "One of the best films I've seen this decade!",
   "Exceptional in every way. A modern classic!",
   "Breathtaking visuals and powerful storytelling!",
   "An absolute triumph that exceeds all expectations!",
   "Masterfully crafted with outstanding performances!",
   "Pure cinematic magic from beginning to end!",

   # Negative reviews
   "Terrible waste of time. Absolutely horrible.",
   "Boring and dull. Fell asleep.",
   "Awful. One of the worst I've seen.",
   "Complete disaster from start to finish.",
   "Painfully bad acting and terrible script.",
   "An embarrassment to cinema. Avoid this mess.",
   "Utterly disappointing and poorly executed.",
   "Waste of talent and resources. Dreadful.",
   "Incoherent plot with wooden performances.",
   "A tedious slog that never improves.",
   "Laughably bad. Not even entertaining as a failure.",
   "Completely misses the mark on every level.",

   # Neutral reviews
   "It was okay. Nothing special but watchable.",
   "Average movie. Some good parts, some bad.",
   "Decent enough. Worth a watch if you're bored.",
   "Neither great nor terrible. Just adequate.",
   "Passable entertainment for a slow evening.",
   "Has its moments but overall unremarkable.",
   "Standard fare. Nothing particularly memorable.",
   "Competent but uninspiring filmmaking.",
   "A mixed bag with equal strengths and weaknesses.",
   "Serviceable but forgettable. Middle of the road.",
   "Fair attempt that neither excels nor fails.",
   "Acceptable viewing but nothing groundbreaking."
]

# Labels: 0=negative, 1=neutral, 2=positive
multi_labels = [
   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # positive
   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # negative
   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1   # neutral
]

# Data splitting for proper evaluation
from sklearn.model_selection import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
   multi_texts, multi_labels, test_size=0.3, random_state=42, stratify=multi_labels
)

print(f"Dataset size: {len(multi_texts)} reviews")
print(f"Training set: {len(train_texts)} reviews")
print(f"Test set: {len(test_texts)} reviews")
print(f"Class distribution: Positive={multi_labels.count(2)}, Negative={multi_labels.count(0)}, Neutral={multi_labels.count(1)}")

# Initialize and train classifier
multi_clf = TextClassifier(
   n_classes=3,
   learning_rate=0.2,
   alpha=0.05,
   batch_size=12
)

print("\nTraining multi-class classifier...")
multi_clf.fit(train_texts, train_labels, epochs=150, verbose=True)

# Model evaluation on test set
test_predictions = multi_clf.predict(test_texts)
test_accuracy = np.mean(test_predictions == test_labels)
train_accuracy = multi_clf.score(train_texts, train_labels)

print(f"\nModel Performance:")
print(f"Training Accuracy: {train_accuracy:.2%}")
print(f"Test Accuracy: {test_accuracy:.2%}")
print(f"Generalization Gap: {(train_accuracy - test_accuracy):.2%}")

# Confusion matrix analysis
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_labels, test_predictions)
print("\nConfusion Matrix:")
print("       Neg  Neu  Pos")
for i, row in enumerate(cm):
   label = ["Neg", "Neu", "Pos"][i]
   print(f"{label}:    {row}")

# Test on new examples
evaluation_texts = [
   "Outstanding performance! Brilliant cinematography and acting!",
   "Terrible acting and plot. Complete waste of time.",
   "Fine for what it is. Not amazing but not terrible either.",
   "A masterpiece of modern cinema! Absolutely loved it!",
   "Boring beyond belief. Couldn't finish watching.",
   "Has good and bad elements. Overall pretty average."
]

predictions = multi_clf.predict(evaluation_texts)
probabilities = multi_clf.predict_proba(evaluation_texts)

print("\nClassification Results on New Reviews:")
sentiment_map = {0: "Negative", 1: "Neutral", 2: "Positive"}

for idx, (text, pred, probs) in enumerate(zip(evaluation_texts, predictions, probabilities)):
   print(f"\nReview {idx+1}: {text}")
   print(f"Classification: {sentiment_map[pred]}")
   print(f"Confidence scores: Negative={probs[0]:.3f}, Neutral={probs[1]:.3f}, Positive={probs[2]:.3f}")

# Visualization of training dynamics
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss curve
axes[0].plot(multi_clf.losses, linewidth=2, color='darkblue')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Multi-class Training Loss', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Class probability distribution for test examples
bar_width = 0.25
x_pos = np.arange(len(evaluation_texts[:3]))

axes[1].bar(x_pos - bar_width, probabilities[:3, 0], bar_width, label='Negative', color='crimson', alpha=0.8)
axes[1].bar(x_pos, probabilities[:3, 1], bar_width, label='Neutral', color='gray', alpha=0.8)
axes[1].bar(x_pos + bar_width, probabilities[:3, 2], bar_width, label='Positive', color='forestgreen', alpha=0.8)

axes[1].set_xlabel('Test Example', fontsize=12)
axes[1].set_ylabel('Probability', fontsize=12)
axes[1].set_title('Class Probability Distribution', fontsize=14)
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(['Example 1', 'Example 2', 'Example 3'])
axes[1].legend()
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Feature importance analysis
print("\n=== Feature Importance Analysis ===")

# Extract weights for each class
vocab_size = len(multi_clf.vocabulary)
feature_names = list(multi_clf.vocabulary.keys()) + ['exclamation', 'negation', 'log_length']

for class_idx, class_name in enumerate(['Negative', 'Neutral', 'Positive']):
   print(f"\n{class_name} Class Indicators:")

   # Get weights for this class
   class_weights = multi_clf.W[:, class_idx]

   # Create feature-weight pairs
   feature_importance = [(feature_names[i], class_weights[i])
                        for i in range(len(class_weights))]

   # Sort by absolute weight
   feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

   # Display top features
   print("  Top contributing features:")
   for feature, weight in feature_importance[:8]:
       if abs(weight) > 0.01:  # Only show meaningful weights
           print(f"    {feature}: {weight:+.3f}")

# Statistical summary
print("\nWeight Statistics:")
print(f"Total parameters: {multi_clf.W.size + multi_clf.b.size}")
print(f"Average weight magnitude: {np.mean(np.abs(multi_clf.W)):.4f}")
print(f"Weight standard deviation: {np.std(multi_clf.W):.4f}")

"""#### The implementation is working correctly - the 100% accuracy is expected with such a small dataset. The model is memorizing rather than generalizing. It is overfitting the small data, it should be trained with larger datasets."""