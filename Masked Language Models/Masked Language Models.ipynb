{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMH_lt4DxQf1",
        "outputId": "9a6d19db-9871-47f8-dff3-ae7b8441d73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Any, List, Tuple, Dict\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "# Seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using\", device )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NumPy Indexing in Multi-Dim: Implicit Rules\n",
        "\n",
        "Consider a tensor:\n",
        "```python\n",
        "x.shape == (2, 3, 4) # [seq_len, n_heads, d_head]\n",
        "````\n",
        "\n",
        "Visual layout:\n",
        "\n",
        "```\n",
        "[\n",
        "  [   # first axis-0 element (token 0)\n",
        "    [x000, x001, x002, x003],   # head 0\n",
        "    [x010, x011, x012, x013],   # head 1\n",
        "    [x020, x021, x022, x023]    # head 2\n",
        "  ],\n",
        "\n",
        "  [   # second axis-0 element (token 1)\n",
        "    [x100, x101, x102, x103],   # head 0\n",
        "    [x110, x111, x112, x113],   # head 1\n",
        "    [x120, x121, x122, x123]    # head 2\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Missing slices are implied\n",
        "\n",
        "If fewer slices are provided than the number of dimensions, NumPy assumes full slices (`:`) for the remaining axes.\n",
        "\n",
        "```python\n",
        "x[:, 1]        # (2, 4), equivalent to x[:, 1, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x010, x011, x012, x013],   # token 0, head 1\n",
        "  [x110, x111, x112, x113]    # token 1, head 1\n",
        "]\n",
        "```\n",
        "\n",
        "```python\n",
        "x[0]          # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],   # head 0\n",
        "  [x010, x011, x012, x013],   # head 1\n",
        "  [x020, x021, x022, x023]    # head 2\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Integer indices remove dimensions\n",
        "\n",
        "An integer index selects a single element along that axis and drops the dimension.\n",
        "\n",
        "```python\n",
        "x[:, :, 2]    # (2, 3), axis 2 removed\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x002, x012, x022],   # token 0, all heads\n",
        "  [x102, x112, x122]    # token 1, all heads\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Slice (`:`) keeps dimensions\n",
        "\n",
        "Using a slice preserves the axis length.\n",
        "\n",
        "```python\n",
        "x[:, 1:2, :].shape   # (2, 1, 4)\n",
        "```\n",
        "\n",
        "Result (note the extra axis of length 1):\n",
        "\n",
        "```\n",
        "[\n",
        "  [\n",
        "    [x010, x011, x012, x013]   # head 1 kept as axis length 1\n",
        "  ],\n",
        "  [\n",
        "    [x110, x111, x112, x113]\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Trailing axes can be omitted\n",
        "\n",
        "If only the first axes are specified, NumPy assumes `:` for all remaining axes.\n",
        "\n",
        "```python\n",
        "x[0]       # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result (same as above for token 0):\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],\n",
        "  [x010, x011, x012, x013],\n",
        "  [x020, x021, x022, x023]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Ellipsis (`...`)\n",
        "\n",
        "Ellipsis expands to the required number of `:` to cover missing dimensions.\n",
        "\n",
        "```python\n",
        "x[..., 2]   # (2, 3), equivalent to x[:, :, 2]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x002, x012, x022],   # token 0\n",
        "  [x102, x112, x122]    # token 1\n",
        "]\n",
        "```\n",
        "\n",
        "```python\n",
        "x[0, ...]   # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],\n",
        "  [x010, x011, x012, x013],\n",
        "  [x020, x021, x022, x023]\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "rSkgGdYh66Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Bidirectional Attention"
      ],
      "metadata": {
        "id": "w95PQt9GygkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalAttention:\n",
        "  \"\"\"Bidirectional attention without causal mask\"\"\"\n",
        "\n",
        "  def __init__(self, d_model:int, n_heads:int):\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "\n",
        "    self.d_head = d_model//n_heads\n",
        "\n",
        "    # Weight matrices\n",
        "    self.W_Q = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_K = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_V = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_O = np.random.randn(d_model, d_model) * 0.02\n",
        "\n",
        "    print(f\"Bidirectional attention: d_model={d_model}, n_heads={n_heads}, d_head={self.d_head}\")\n",
        "\n",
        "  def forward(self, x:np.ndarray ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    x: [seq_len, d_model]\n",
        "    Returns: sequence output [seq_len, d_model], attention [seq_len, seq_len]\n",
        "    \"\"\"\n",
        "\n",
        "    seq_len = x.shape[0]\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = x @ self.W_Q # [seq_len, d_model]\n",
        "    K = x @ self.W_K # [seq_len, d_model]\n",
        "    V = x @ self.W_V # [seq_len, d_model]\n",
        "    print(f\"\\nQ,K,V shapes: {Q.shape}\")\n",
        "\n",
        "    # Reshape for the multi-head, needs [L, h, d_k]\n",
        "    Q = Q.reshape(seq_len, self.n_heads, self.d_head)  #  n_heads * d_head = d_model\n",
        "    K = K.reshape(seq_len, self.n_heads, self.d_head)\n",
        "    V = V.reshape(seq_len, self.n_heads, self.d_head)\n",
        "\n",
        "    # Compute attention without causal masking\n",
        "    scores = np.zeros((self.n_heads, seq_len, seq_len))\n",
        "\n",
        "    for head in range(self.n_heads):\n",
        "      QK = Q[:, head] @ K[:,head].T # [L, d_k] @ [d_k, L] = [L, L]\n",
        "      scores[head] = QK / np.sqrt(self.d_head) # normalization by sqrt of d_k\n",
        "      print(f\"Head {head}: QK^T range [{QK.min():.2f}, {QK.max():.2f}]\")\n",
        "\n",
        "\n",
        "    # Without Masking\n",
        "    # Sofmax directly\n",
        "    attention = np.exp(scores - scores.max(axis=-1, keepdims=True)) # Stability and avoid overflow for large values\n",
        "    attention = attention / attention.sum(axis=-1, keepdims=True)\n",
        "    print(f\"Attention shape: {attention.shape}, no causal mask applied\")\n",
        "\n",
        "    # Attention to Values Matrix\n",
        "    output = np.zeros((seq_len, self.d_model))\n",
        "    for head in range(self.n_heads):\n",
        "      head_out = attention[head] @ V[:,head] # [L, L] @ [L, d_k] = [L, d_k]\n",
        "      start_idx = head * self.d_head\n",
        "      end_idx = (head+1) * self.d_head\n",
        "      output[:, start_idx :end_idx] = head_out\n",
        "\n",
        "    # Output Proj\n",
        "    output = output @ self.W_O\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    return output, attention.mean(axis=0) # Average attention over heads"
      ],
      "metadata": {
        "id": "112LwtKEyIQi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Bidirectional vs Causal"
      ],
      "metadata": {
        "id": "D4k6vDkpiSXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len, d_model = 6, 64\n",
        "x = np.random.randn(seq_len, d_model)\n",
        "\n",
        "bidirectional_attn = BidirectionalAttention(d_model, n_heads=4)\n",
        "output, attn_weights = bidirectional_attn.forward(x)\n",
        "\n",
        "\n",
        "# Visualize bidirectional attention pattern\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(attn_weights, cmap='Blues')\n",
        "plt.colorbar()\n",
        "plt.title('Bidirectional Attention\\n(can attend to all positions)')\n",
        "plt.xlabel('Keys')\n",
        "plt.ylabel('Queries')\n",
        "\n",
        "# Show causal mask for comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
        "plt.imshow(causal_mask, cmap='Blues')\n",
        "plt.colorbar()\n",
        "plt.title('Causal Mask Pattern\\n(only attend to past)')\n",
        "plt.xlabel('Keys')\n",
        "plt.ylabel('Queries')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nKey difference: Position 0 can attend to position 5: {attn_weights[0, 5]:.3f}\")\n",
        "print(f\"In causal attention, this would be 0 (masked)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "Oue3D_S68w1f",
        "outputId": "3cfd54cf-6c2b-4af0-c89f-82042a82ffed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional attention: d_model=64, n_heads=4, d_head=16\n",
            "\n",
            "Q,K,V shapes: (6, 64)\n",
            "Head 0: QK^T range [-0.20, 0.11]\n",
            "Head 1: QK^T range [-0.17, 0.37]\n",
            "Head 2: QK^T range [-0.19, 0.21]\n",
            "Head 3: QK^T range [-0.25, 0.25]\n",
            "Attention shape: (4, 6, 6), no causal mask applied\n",
            "Output shape: (6, 64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Key difference: Position 0 can attend to position 5: 0.165\n",
            "In causal attention, this would be 0 (masked)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Bidirectional Transformer Block"
      ],
      "metadata": {
        "id": "io289D4pkSjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalTransformerBlock(nn.Module):\n",
        "  \"\"\"BERT-style transformer block without causal masking\"\"\"\n",
        "\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ff: int = None, dropout:float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # Multi-head attention wihout maskin\n",
        "    self.attention = nn.MultiheadAttention(self.d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "    # Feedforward Layer\n",
        "    d_ff = d_ff or 4 * d_model # Usually FFN = 4 * hidden state\n",
        "\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.GELU(),    # Standard activation function in BERT, not ReLU\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    # Layer Norm (post-norm)\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, padding_mask: Optional[torch.Tensor] = None):\n",
        "    \"\"\"\n",
        "    x:  [batch, seq_len, d_model]\n",
        "    padding_mask: [batch,seq_len] - True for padded positions\n",
        "    \"\"\"\n",
        "    print(f\"\\nInput: {x.shape}\")\n",
        "\n",
        "    # Self attention without Causal Mask\n",
        "    attn_out, attn_weights = self.attention(x, x, x,    # Q,K,V\n",
        "                                            key_padding_mask = padding_mask, # Shape: [batch, seq_len]. tells the attention which tokens are padding and should not contribute to the attention output. Example: padding_mask = torch.tensor([[False, False, True, True]])  # batch=1, seq_len=4\n",
        "                                            need_weights = True,  # whether the function should return the attention weights along with the output\n",
        "                                            attn_mask = None # No causal mask\n",
        "                                            )\n",
        "    print(f\"Attention output: {attn_out.shape}, weights: {attn_weights.shape}\")\n",
        "\n",
        "    # Residual connection + layer norm\n",
        "    x = self.ln1(x + self.dropout(attn_out))\n",
        "    print(f\"After LN1: mean={x.mean():.3f}, std={x.std():.3f}\")\n",
        "\n",
        "    # Feedforward\n",
        "    ffn_out = self.ffn(x)\n",
        "\n",
        "    # Residual connection + layer norm\n",
        "    x = self.ln2(x + self.dropout(ffn_out))\n",
        "    print(f\"After LN2: mean={x.mean():.3f}, std={x.std():.3f}\")\n",
        "\n",
        "    return x, attn_weights"
      ],
      "metadata": {
        "id": "ZZVZ71TijA-1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with mixed precision and torch.compile"
      ],
      "metadata": {
        "id": "X_i1E8o3p5K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BidirectionalTransformerBlock(d_model=256, n_heads=8).to(device)\n",
        "print(f\"Parameters: {sum(param.numel() for param in model.parameters()):,}\")\n",
        "\n",
        "# Mixed precision\n",
        "from torch.cuda.amp import autocast\n",
        "x = torch.randn(2,20,256).to(device)\n",
        "\n",
        "with autocast(dtype=torch.bfloat16) if device.type == \"cuda\" else torch.no_grad():\n",
        "  output, attn = model(x)\n",
        "  print(f\"\\nWith bfloat16: output dtype={output.dtype}\")\n",
        "\n",
        "\n",
        "# Compiling\n",
        "if hasattr(torch, 'compile'):\n",
        "  model_compiled = torch.compile(model, mode='reduce-overhead') # Best option for small models\n",
        "  print(\"Model compiled with torch.compile\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWW0T0RxpSK-",
        "outputId": "5ac8f4df-1a19-4d73-daba-df7cb6c41fb1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 789,760\n",
            "\n",
            "Input: torch.Size([2, 20, 256])\n",
            "Attention output: torch.Size([2, 20, 256]), weights: torch.Size([2, 20, 20])\n",
            "After LN1: mean=-0.000, std=1.000\n",
            "After LN2: mean=-0.000, std=1.000\n",
            "\n",
            "With bfloat16: output dtype=torch.float32\n",
            "Model compiled with torch.compile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1716660855.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(dtype=torch.bfloat16) if device.type == \"cuda\" else torch.no_grad():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace BERT MLM Training"
      ],
      "metadata": {
        "id": "ePHHa-uyXa4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BERTPretrainingBatch:\n",
        "  \"\"\"Batch for BERT pretraining with MLM + NSP\"\"\"\n",
        "\n",
        "  input_ids: torch.Tensor\n",
        "  attention_mask: torch.Tensor\n",
        "  token_type_ids: torch.Tensor # segment for embeddings\n",
        "  mlm_labels: torch.Tensor\n",
        "  nsp_labels: torch.Tensor\n",
        "\n",
        "class BERTPretrainer(nn.Module):\n",
        "  \"\"\"BERT model with MLM and NSP heads\"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # Token embeddings + position + segment\n",
        "    self.embeddings = nn.ModuleDict({\n",
        "        'token':nn.Embedding(config['vocab_size'], config['d_model']),\n",
        "        'position': nn.Embedding(config['max_position'], config['d_model']),\n",
        "        'segment': nn.Embedding(2, config['d_model']), # 2 segments for Next Sent Pred\n",
        "    })\n",
        "\n",
        "    # Transformer blocks\n",
        "    self.blocks = nn.ModuleList([\n",
        "        BidirectionalTransformerBlock(\n",
        "            config['d_model'],\n",
        "            config['n_heads'],\n",
        "            config['d_ff']\n",
        "        ) for _ in range(config['n_layers'])\n",
        "    ])\n",
        "\n",
        "    # MLM head\n",
        "    self.mlm_head = nn.Sequential(\n",
        "        nn.Linear(config['d_model'], config['d_model']),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(config['d_model']),\n",
        "        nn.Linear(config['d_model'], config['vocab_size'])\n",
        "    )\n",
        "\n",
        "    # NSP head (binary classification)\n",
        "    self.nsp_head = nn.Sequential(\n",
        "        nn.Linear(config['d_model'], config['d_model']),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(config['d_model'], 2)\n",
        "    )\n",
        "\n",
        "    print(f\"BERT Pretrainer initialized: {sum(p.numel() for p in self.parameters()):,} params\")\n",
        "\n",
        "\n",
        "  def forward(self, batch: BERTPretrainingBatch):\n",
        "\n",
        "    # Combine embeddings\n",
        "    x = self.embeddings['token'](batch.input_ids)\n",
        "    positions = torch.arange(batch.input_ids.size(1), device=batch.input_ids.device)\n",
        "    x = x + self.embeddings['position'](positions)\n",
        "    x = x + self.embeddings['segment'](batch.token_type_ids)\n",
        "\n",
        "    print(f\"Embeddings: {x.shape}, norm={x.norm(dim=-1).mean():.3f}\")\n",
        "\n",
        "\n",
        "    # Pass through transformer blocks\n",
        "    for i, block in enumerate(self.blocks):\n",
        "      x, _ = block(x)\n",
        "      if i == 0:\n",
        "        print(f\"After block {i}: {x.shape}\")\n",
        "\n",
        "    # MLM prediction for all the tokens\n",
        "    mlm_logits = self.mlm_head(x) # shape [batch, seq_len, vocab_size]\n",
        "    print(f\"MLM logits: {mlm_logits.shape}\")\n",
        "\n",
        "    # NSP pred from [CLS] token (postion 0)\n",
        "    cls_hidden = x[:, 0] # [batch, d_model]\n",
        "    nsp_logits = self.nsp_head(cls_hidden) # [batch, 2]\n",
        "    print(f\"NSP logits: {nsp_logits.shape}\")\n",
        "\n",
        "    # Compute losses\n",
        "    mlm_loss = F.cross_entropy(\n",
        "        mlm_logits.view(-1, self.config['vocab_size']),\n",
        "        batch.mlm_labels.view(-1),\n",
        "        ignore_index = -100\n",
        "    )\n",
        "\n",
        "    nsp_loss = F.cross_entropy(nsp_logits, batch.nsp_labels)\n",
        "\n",
        "\n",
        "    total_loss = mlm_loss + nsp_loss\n",
        "    print(f\"MLM loss: {mlm_loss:.3f}, NSP loss: {nsp_loss:.3f}, Total: {total_loss:.3f}\")\n",
        "\n",
        "\n",
        "    return {'loss': total_loss, 'mlm_loss': mlm_loss, 'nsp_loss': nsp_loss}\n",
        "\n"
      ],
      "metadata": {
        "id": "l1ear20GpPQt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined training test"
      ],
      "metadata": {
        "id": "H61kSrLSkm60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  'vocab_size': 1000,\n",
        "  'd_model': 256,\n",
        "  'n_heads': 8,\n",
        "  'd_ff': 1024,\n",
        "  'n_layers': 2,\n",
        "  'max_position': 128\n",
        "}\n",
        "\n",
        "\n",
        "model = BERTPretrainer(config).to(device)\n",
        "\n",
        "# Create dummy batch\n",
        "batch = BERTPretrainingBatch(\n",
        "  input_ids=torch.randint(0, 1000, (4, 32)).to(device),\n",
        "  attention_mask=torch.ones(4, 32).to(device),\n",
        "  token_type_ids=torch.cat([torch.zeros(4, 16), torch.ones(4, 16)], dim=1).long().to(device),\n",
        "  mlm_labels=torch.full((4, 32), -100).to(device),\n",
        "  nsp_labels=torch.randint(0, 2, (4,)).to(device)\n",
        ")\n",
        "\n",
        "# Set some MLM labels\n",
        "batch.mlm_labels[:, [5, 10, 15]] = torch.randint(0, 1000, (4, 3)).to(device)\n",
        "\n",
        "# Forward pass with gradient accumulation\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "for step in range(2):\n",
        "  print(f\"\\nStep {step}:\")\n",
        "  output = model(batch)\n",
        "  loss = output['loss'] / 2  # Gradient accumulation\n",
        "  loss.backward()\n",
        "\n",
        "  if step % 2 == 1:  # Accumulate 2 steps\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(\"Weights updated after gradient accumulation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t74zhJ76c_y7",
        "outputId": "534ec81e-abae-48d4-f473-32f81b25c7d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Pretrainer initialized: 2,258,410 params\n",
            "\n",
            "Step 0:\n",
            "Embeddings: torch.Size([4, 32, 256]), norm=27.867\n",
            "\n",
            "Input: torch.Size([4, 32, 256])\n",
            "Attention output: torch.Size([4, 32, 256]), weights: torch.Size([4, 32, 32])\n",
            "After LN1: mean=0.000, std=1.000\n",
            "After LN2: mean=-0.000, std=1.000\n",
            "After block 0: torch.Size([4, 32, 256])\n",
            "\n",
            "Input: torch.Size([4, 32, 256])\n",
            "Attention output: torch.Size([4, 32, 256]), weights: torch.Size([4, 32, 32])\n",
            "After LN1: mean=0.000, std=1.000\n",
            "After LN2: mean=0.000, std=1.000\n",
            "MLM logits: torch.Size([4, 32, 1000])\n",
            "NSP logits: torch.Size([4, 2])\n",
            "MLM loss: 6.920, NSP loss: 0.638, Total: 7.557\n",
            "\n",
            "Step 1:\n",
            "Embeddings: torch.Size([4, 32, 256]), norm=27.867\n",
            "\n",
            "Input: torch.Size([4, 32, 256])\n",
            "Attention output: torch.Size([4, 32, 256]), weights: torch.Size([4, 32, 32])\n",
            "After LN1: mean=-0.000, std=1.000\n",
            "After LN2: mean=-0.000, std=1.000\n",
            "After block 0: torch.Size([4, 32, 256])\n",
            "\n",
            "Input: torch.Size([4, 32, 256])\n",
            "Attention output: torch.Size([4, 32, 256]), weights: torch.Size([4, 32, 32])\n",
            "After LN1: mean=0.000, std=1.000\n",
            "After LN2: mean=0.000, std=1.000\n",
            "MLM logits: torch.Size([4, 32, 1000])\n",
            "NSP logits: torch.Size([4, 2])\n",
            "MLM loss: 6.910, NSP loss: 0.616, Total: 7.526\n",
            "Weights updated after gradient accumulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual Embeddings and Anisotropy Computation"
      ],
      "metadata": {
        "id": "ZK_dMdquw0I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextualEmbeddingExtractor:\n",
        "  \"\"\"Extract and analyze contexttual embeddings\"\"\"\n",
        "\n",
        "  def __init__(self, n_layers: int, d_model: int):\n",
        "    self.n_layers = n_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def extract_token_embeddings(self, hidden_states: List[np.ndarray], layer_indices: List[int] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract contextual embeddings from specified layers\n",
        "    hidden_states: List of [seq_len, d_model] arrays from each layer\n",
        "    \"\"\"\n",
        "\n",
        "    if layer_indices is None:\n",
        "      # Defualt: using the average of the 4 last layers (common for bert), this creates a more robust representation, leveraging the hierarchical nature of the transformer representation, where diff layers capture diff semantic and linguistic properties :)\n",
        "      layer_indices = list(range(max(0, len(hidden_states) -4), len(hidden_states)))\n",
        "\n",
        "    print(f\"Extracting from layers: {layer_indices}\")\n",
        "\n",
        "    # Stack selected layers\n",
        "    selected = [hidden_states[i] for i in layer_indices]\n",
        "    stacked = np.stack(selected, axis=0) # [n_layers, seq_len, d_model]\n",
        "    print(f\"Stacked shape: {stacked.shape}\")\n",
        "\n",
        "\n",
        "    # Average across layers\n",
        "    avg_embedding = stacked.mean(axis=0) # [seq_len, d_model]\n",
        "    print(f\"Averaged embedding: {avg_embedding.shape}\")\n",
        "\n",
        "    return avg_embedding\n",
        "\n",
        "\n",
        "  def compute_anisotropy(self, embeddings: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute anisotropy (average cosine simiarity between random pairs)\n",
        "    High anisotropy is the same as having the vectors pointing to the same direction in embeding space.\n",
        "    Having isotropy instead is what we need, so that the vectors are pointing in all the directions, and not a single one (a single token).\n",
        "    Anisotropy occurs bcs in all the embeddings there are always some few dimensions that dominate and for which every token will have a very high value, so they end up pointing to similar points\n",
        "    The solution is to standardize the embeddings: substract the mean and divide by the stdev\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples = min(100, len(embeddings))\n",
        "    indices= np.random.choice(len(embeddings), n_samples, replace=False)\n",
        "    sampled = embeddings[indices] # extracted the sampled embeddings\n",
        "\n",
        "    # compute pairwise cosines\n",
        "    norms = np.linalg.norm(sampled, axis=1, keepdims=True)\n",
        "\n",
        "    normalized = sampled/(norms + 1e-10) # [n_samples, d_model]\n",
        "    print(\"normalized: \\n\", normalized ,\"\\n\")\n",
        "\n",
        "\n",
        "    # For normalized vectors, dot product = cosine similarity\n",
        "    cosine_matrix = normalized @ normalized.T  # [n_samples, n_samples]  A symmetric matrix where each entry is the cosine similarity between two vectors (diagonal = 1.0 for self-similarity).\n",
        "    print(\"cosine_matrix: \\n\", cosine_matrix ,\"\\n\")\n",
        "\n",
        "    # Averaging off-diagonal elements\n",
        "    mask = 1 - np.eye(n_samples) # Create mask: 1s everywhere except diagonal (which has 0s). The np.eye creates identity matrix with 1s on diagonal\n",
        "    print(\"mask: \\n\", mask ,\"\\n\")\n",
        "\n",
        "\n",
        "    # Element-wise multiplying the cosine_matrix with the mask, zeroing out diagonal\n",
        "    # Then sum all remaining values and divide by count of 1s in mask.\n",
        "    avg_cosine = (cosine_matrix * mask).sum()/mask.sum() # This measures how similar vectors are on average\n",
        "\n",
        "    print(f\"Anisotropy: {avg_cosine:.3f} (0=isotropic, 1=anisotropic)\")\n",
        "    return avg_cosine\n",
        "\n",
        "\n",
        "  def standardize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Standardize embeddings to reduce the anisotropy\"\"\"\n",
        "\n",
        "    # Compute mean and std across al embeddings\n",
        "    mu = embeddings.mean(axis = 0)  # [d_model]\n",
        "    sigma = embeddings.std(axis=0) # [d_model]\n",
        "\n",
        "    print(f\"Mean norm: {np.linalg.norm(mu):.3f}\")\n",
        "    print(f\"Std range: [{sigma.min():.3f}, {sigma.max():.3f}]\")\n",
        "\n",
        "    # Standardize: z = (x - mean) / std\n",
        "    standardized = (embeddings -mu) / (sigma + 1e-10)\n",
        "    print(f\"After standardization: mean={standardized.mean():.3f}, std={standardized.std():.3f}\")\n",
        "\n",
        "    return standardized"
      ],
      "metadata": {
        "id": "8hMgYv0fZnkk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test contextual embedding extraction"
      ],
      "metadata": {
        "id": "ZPZ3KgwC1l9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = ContextualEmbeddingExtractor(n_layers=12, d_model=256)\n",
        "\n",
        "# Simulate hidden states from 12 layers\n",
        "seq_len = 20\n",
        "hidden_states = [np.random.randn(seq_len, 256) for _ in range(12)]\n",
        "\n",
        "# Extract embeddings\n",
        "embeddings = extractor.extract_token_embeddings(hidden_states)\n",
        "\n",
        "# Check anisotropy\n",
        "anisotropy_before = extractor.compute_anisotropy(embeddings)\n",
        "\n",
        "# Standardize to reduce anisotropy\n",
        "standardized = extractor.standardize_embeddings(embeddings)\n",
        "\n",
        "\n",
        "anisotropy_after = extractor.compute_anisotropy(standardized)\n",
        "\n",
        "print(f\"\\nAnisotropy reduction: {anisotropy_before:.3f} -> {anisotropy_after:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TQpfGuZLas",
        "outputId": "cb90892a-030e-4c27-bc73-070b68fbb4eb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting from layers: [8, 9, 10, 11]\n",
            "Stacked shape: (4, 20, 256)\n",
            "Averaged embedding: (20, 256)\n",
            "normalized: \n",
            " [[-0.03714116  0.04121864 -0.07629346 ... -0.0026631  -0.02167092\n",
            "   0.08247494]\n",
            " [-0.0083402   0.05623053  0.0131521  ... -0.08994952  0.0088227\n",
            "  -0.0017647 ]\n",
            " [-0.0367745   0.0245072  -0.10050575 ...  0.09012986 -0.04420178\n",
            "   0.04303218]\n",
            " ...\n",
            " [-0.01275236  0.00478228  0.12889326 ... -0.07680034 -0.05229417\n",
            "   0.02715317]\n",
            " [ 0.0584198  -0.0285082  -0.0487586  ... -0.08791045 -0.04122035\n",
            "   0.1187615 ]\n",
            " [ 0.03408966  0.04960175  0.06084637 ...  0.00421643  0.03884217\n",
            "  -0.09126748]] \n",
            "\n",
            "cosine_matrix: \n",
            " [[ 1.00000000e+00 -9.03290489e-02  4.97244613e-02 -1.14357126e-01\n",
            "   4.94366675e-02  6.00536476e-03  1.72311988e-02  1.18724164e-01\n",
            "   6.14260930e-02  9.38761180e-03  1.96901883e-04  6.89800964e-02\n",
            "  -3.06378943e-02  3.02534363e-02 -6.73803700e-02 -6.77691999e-02\n",
            "  -7.47836230e-02  8.23603721e-02 -1.77794344e-02 -1.04085399e-02]\n",
            " [-9.03290489e-02  1.00000000e+00  5.40173426e-02  3.84152106e-02\n",
            "  -8.14853901e-02 -1.02690516e-01 -2.28794835e-02 -1.02575074e-01\n",
            "   1.30228960e-02  4.81515961e-02  7.42633659e-02  5.99672928e-02\n",
            "   3.13575279e-02 -1.42694815e-01 -5.86738334e-03  5.09853550e-02\n",
            "   1.30713025e-03  8.13618705e-02  1.22419549e-01  2.90118519e-02]\n",
            " [ 4.97244613e-02  5.40173426e-02  1.00000000e+00  5.78927052e-03\n",
            "  -4.12572849e-02  5.53283242e-02  1.42789700e-02 -3.65772722e-02\n",
            "   5.45770283e-02  5.33365350e-02 -6.37726628e-02 -6.44430258e-02\n",
            "  -7.42520203e-02  1.02510757e-01  1.54585489e-02 -4.56987860e-02\n",
            "  -4.51654188e-02 -2.76079252e-02 -5.34554569e-03  2.55785865e-02]\n",
            " [-1.14357126e-01  3.84152106e-02  5.78927052e-03  1.00000000e+00\n",
            "   3.55025512e-02  4.53000201e-02 -6.17824928e-02 -1.26423478e-02\n",
            "  -9.82762994e-03 -1.10892845e-01 -3.49945782e-03 -2.05222314e-02\n",
            "   5.91737712e-02 -5.92549206e-02  2.99929872e-02  4.39379163e-02\n",
            "  -6.69308312e-02  1.24521066e-02  5.50718205e-02  7.00651492e-02]\n",
            " [ 4.94366675e-02 -8.14853901e-02 -4.12572849e-02  3.55025512e-02\n",
            "   1.00000000e+00  6.11741085e-02 -3.69690502e-02  2.08784992e-02\n",
            "   4.82936323e-02  5.43221972e-02  1.03202015e-02  1.64170428e-01\n",
            "   5.81859330e-02  6.45724292e-02 -1.09742237e-03  2.59370128e-02\n",
            "   6.38299541e-02  2.08434792e-02  7.73451326e-02  3.08169438e-02]\n",
            " [ 6.00536476e-03 -1.02690516e-01  5.53283242e-02  4.53000201e-02\n",
            "   6.11741085e-02  1.00000000e+00 -3.07549392e-02  6.28146644e-02\n",
            "   7.95922375e-02 -3.49805481e-02  1.15088084e-01  5.12945276e-02\n",
            "   1.92013285e-02  6.61903030e-02  2.14326882e-02 -7.29669013e-02\n",
            "  -1.47997153e-02  6.54901628e-02 -3.08834372e-02  4.47350689e-03]\n",
            " [ 1.72311988e-02 -2.28794835e-02  1.42789700e-02 -6.17824928e-02\n",
            "  -3.69690502e-02 -3.07549392e-02  1.00000000e+00 -3.64839156e-02\n",
            "   3.35151556e-02  6.28120263e-02  3.13255060e-02 -1.04017365e-01\n",
            "  -3.61541017e-02  7.80360116e-02 -2.22552310e-02 -2.54897032e-02\n",
            "  -1.09322688e-01 -2.28113067e-02  4.14249343e-02 -1.12194419e-01]\n",
            " [ 1.18724164e-01 -1.02575074e-01 -3.65772722e-02 -1.26423478e-02\n",
            "   2.08784992e-02  6.28146644e-02 -3.64839156e-02  1.00000000e+00\n",
            "  -4.86570423e-02  6.86751304e-02  1.00792249e-01  5.11424930e-02\n",
            "   4.18491408e-02 -1.79357772e-02 -5.29646325e-02  3.10557069e-02\n",
            "  -8.49131860e-02  1.05522236e-01 -3.54521383e-02  3.39925659e-02]\n",
            " [ 6.14260930e-02  1.30228960e-02  5.45770283e-02 -9.82762994e-03\n",
            "   4.82936323e-02  7.95922375e-02  3.35151556e-02 -4.86570423e-02\n",
            "   1.00000000e+00 -3.82760338e-02  2.80923200e-02  1.47817192e-03\n",
            "   2.51412095e-02 -6.45072729e-02  8.43937749e-02 -7.18747776e-02\n",
            "   9.97670242e-02  1.30609643e-02 -6.49085080e-03  5.84381774e-02]\n",
            " [ 9.38761180e-03  4.81515961e-02  5.33365350e-02 -1.10892845e-01\n",
            "   5.43221972e-02 -3.49805481e-02  6.28120263e-02  6.86751304e-02\n",
            "  -3.82760338e-02  1.00000000e+00 -1.30929901e-02  1.33055990e-01\n",
            "   5.31011514e-02  4.11273446e-02  1.91601142e-03  1.62508042e-02\n",
            "   1.02469568e-02  4.08018404e-02  1.56925719e-02  1.08958455e-02]\n",
            " [ 1.96901883e-04  7.42633659e-02 -6.37726628e-02 -3.49945782e-03\n",
            "   1.03202015e-02  1.15088084e-01  3.13255060e-02  1.00792249e-01\n",
            "   2.80923200e-02 -1.30929901e-02  1.00000000e+00  2.23795811e-02\n",
            "   1.38814601e-01  4.77295032e-02  7.86473520e-02 -1.78370853e-03\n",
            "  -1.06613640e-01 -2.05460866e-04  2.21921016e-02  5.09406884e-02]\n",
            " [ 6.89800964e-02  5.99672928e-02 -6.44430258e-02 -2.05222314e-02\n",
            "   1.64170428e-01  5.12945276e-02 -1.04017365e-01  5.11424930e-02\n",
            "   1.47817192e-03  1.33055990e-01  2.23795811e-02  1.00000000e+00\n",
            "   4.50649735e-02 -4.74802349e-02 -2.75775539e-03  1.00512053e-01\n",
            "   7.46002732e-02  5.01140518e-02  6.73920471e-02 -4.16868106e-02]\n",
            " [-3.06378943e-02  3.13575279e-02 -7.42520203e-02  5.91737712e-02\n",
            "   5.81859330e-02  1.92013285e-02 -3.61541017e-02  4.18491408e-02\n",
            "   2.51412095e-02  5.31011514e-02  1.38814601e-01  4.50649735e-02\n",
            "   1.00000000e+00 -8.75336376e-03 -8.67790154e-02  7.32602624e-02\n",
            "   1.57159857e-01 -1.44083056e-02  7.64330145e-02  9.06316451e-03]\n",
            " [ 3.02534363e-02 -1.42694815e-01  1.02510757e-01 -5.92549206e-02\n",
            "   6.45724292e-02  6.61903030e-02  7.80360116e-02 -1.79357772e-02\n",
            "  -6.45072729e-02  4.11273446e-02  4.77295032e-02 -4.74802349e-02\n",
            "  -8.75336376e-03  1.00000000e+00 -5.71474086e-02 -1.17489343e-02\n",
            "  -6.11487542e-03 -1.26394607e-02 -5.36750693e-02 -1.01646111e-02]\n",
            " [-6.73803700e-02 -5.86738334e-03  1.54585489e-02  2.99929872e-02\n",
            "  -1.09742237e-03  2.14326882e-02 -2.22552310e-02 -5.29646325e-02\n",
            "   8.43937749e-02  1.91601142e-03  7.86473520e-02 -2.75775539e-03\n",
            "  -8.67790154e-02 -5.71474086e-02  1.00000000e+00  6.58454977e-03\n",
            "   4.17456329e-02  2.30279367e-03  8.47999299e-03 -6.41809359e-02]\n",
            " [-6.77691999e-02  5.09853550e-02 -4.56987860e-02  4.39379163e-02\n",
            "   2.59370128e-02 -7.29669013e-02 -2.54897032e-02  3.10557069e-02\n",
            "  -7.18747776e-02  1.62508042e-02 -1.78370853e-03  1.00512053e-01\n",
            "   7.32602624e-02 -1.17489343e-02  6.58454977e-03  1.00000000e+00\n",
            "  -4.41616785e-02  7.07758397e-02  2.59997827e-02  7.37793474e-03]\n",
            " [-7.47836230e-02  1.30713025e-03 -4.51654188e-02 -6.69308312e-02\n",
            "   6.38299541e-02 -1.47997153e-02 -1.09322688e-01 -8.49131860e-02\n",
            "   9.97670242e-02  1.02469568e-02 -1.06613640e-01  7.46002732e-02\n",
            "   1.57159857e-01 -6.11487542e-03  4.17456329e-02 -4.41616785e-02\n",
            "   1.00000000e+00  1.11782439e-02  8.06707806e-02  8.24974846e-03]\n",
            " [ 8.23603721e-02  8.13618705e-02 -2.76079252e-02  1.24521066e-02\n",
            "   2.08434792e-02  6.54901628e-02 -2.28113067e-02  1.05522236e-01\n",
            "   1.30609643e-02  4.08018404e-02 -2.05460866e-04  5.01140518e-02\n",
            "  -1.44083056e-02 -1.26394607e-02  2.30279367e-03  7.07758397e-02\n",
            "   1.11782439e-02  1.00000000e+00  6.78876664e-02  7.59487465e-02]\n",
            " [-1.77794344e-02  1.22419549e-01 -5.34554569e-03  5.50718205e-02\n",
            "   7.73451326e-02 -3.08834372e-02  4.14249343e-02 -3.54521383e-02\n",
            "  -6.49085080e-03  1.56925719e-02  2.21921016e-02  6.73920471e-02\n",
            "   7.64330145e-02 -5.36750693e-02  8.47999299e-03  2.59997827e-02\n",
            "   8.06707806e-02  6.78876664e-02  1.00000000e+00 -1.46681603e-02]\n",
            " [-1.04085399e-02  2.90118519e-02  2.55785865e-02  7.00651492e-02\n",
            "   3.08169438e-02  4.47350689e-03 -1.12194419e-01  3.39925659e-02\n",
            "   5.84381774e-02  1.08958455e-02  5.09406884e-02 -4.16868106e-02\n",
            "   9.06316451e-03 -1.01646111e-02 -6.41809359e-02  7.37793474e-03\n",
            "   8.24974846e-03  7.59487465e-02 -1.46681603e-02  1.00000000e+00]] \n",
            "\n",
            "mask: \n",
            " [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]] \n",
            "\n",
            "Anisotropy: 0.012 (0=isotropic, 1=anisotropic)\n",
            "Mean norm: 1.977\n",
            "Std range: [0.311, 0.780]\n",
            "After standardization: mean=-0.000, std=1.000\n",
            "normalized: \n",
            " [[ 0.08705946 -0.05946574 -0.03139768 ... -0.10013415  0.07319339\n",
            "   0.01228087]\n",
            " [ 0.09295399 -0.00743316  0.06174543 ... -0.02107975 -0.00918204\n",
            "  -0.00780863]\n",
            " [-0.041303    0.04865939 -0.08362944 ...  0.11705667 -0.03722538\n",
            "   0.03911962]\n",
            " ...\n",
            " [ 0.01209513  0.01304585 -0.11624914 ... -0.00829065 -0.01903594\n",
            "  -0.01708072]\n",
            " [-0.02067654 -0.00555612  0.03709681 ... -0.01586949 -0.06634924\n",
            "  -0.00975337]\n",
            " [-0.00870053  0.02563487  0.12530691 ... -0.07294387 -0.04888893\n",
            "   0.02188763]] \n",
            "\n",
            "cosine_matrix: \n",
            " [[ 1.00000000e+00  2.08712205e-02 -1.41136144e-01 -8.28267891e-02\n",
            "  -1.44109509e-01 -8.32918840e-02 -7.64695650e-02  1.20347546e-02\n",
            "  -1.23078794e-01  5.01183347e-02 -4.52309729e-02 -4.09921722e-02\n",
            "  -7.94268557e-02 -8.16938889e-02 -1.48264943e-02 -1.16190386e-01\n",
            "   2.69768953e-03  4.88644712e-02  2.13603397e-02 -9.85316665e-02]\n",
            " [ 2.08712205e-02  1.00000000e+00 -1.09275186e-02 -7.36178859e-02\n",
            "  -8.06210256e-03 -4.54984685e-02 -1.11433888e-01 -1.22335032e-01\n",
            "  -3.23308013e-02 -1.48658882e-01  1.06747625e-02 -1.04165838e-01\n",
            "  -1.31940797e-02 -1.33225777e-02 -4.01003925e-02 -6.83295515e-02\n",
            "  -1.08139082e-01 -5.04504068e-02 -5.22587041e-02 -5.46120165e-02]\n",
            " [-1.41136144e-01 -1.09275186e-02  1.00000000e+00 -1.10718587e-01\n",
            "  -8.52005626e-02 -7.26680379e-02  1.42756932e-02  6.26141400e-02\n",
            "  -6.58362631e-03 -1.46789388e-01 -3.40205560e-02 -8.36158403e-03\n",
            "  -7.42221492e-02 -2.95121416e-02 -3.58813218e-04 -1.49761339e-01\n",
            "  -8.18447615e-02  2.08006305e-02 -3.52111143e-02 -1.25465342e-01]\n",
            " [-8.28267891e-02 -7.36178859e-02 -1.10718587e-01  1.00000000e+00\n",
            "  -6.11807057e-02 -1.83483877e-02 -4.66443103e-02 -1.94232597e-02\n",
            "  -3.32051344e-02 -3.78942961e-02 -1.02780153e-01 -5.36485468e-02\n",
            "  -5.36575746e-02 -4.61228992e-03 -1.35868049e-01  6.18862360e-02\n",
            "  -6.25475238e-02 -4.27266061e-02 -9.75732831e-02 -9.12124500e-02]\n",
            " [-1.44109509e-01 -8.06210256e-03 -8.52005626e-02 -6.11807057e-02\n",
            "   1.00000000e+00  2.91510556e-02 -8.87190865e-02 -3.71338714e-02\n",
            "  -5.68496325e-02  6.72119296e-02  3.18764155e-02 -3.49493110e-02\n",
            "  -9.84270733e-02 -1.11454430e-01 -3.32095956e-02  2.42141799e-02\n",
            "  -1.36444243e-01 -5.03200703e-02 -1.17476134e-01 -4.34576384e-02]\n",
            " [-8.32918840e-02 -4.54984685e-02 -7.26680379e-02 -1.83483877e-02\n",
            "   2.91510556e-02  1.00000000e+00 -8.70284508e-02 -1.23412153e-01\n",
            "  -5.64305907e-02 -3.08561955e-02 -9.43874883e-02 -7.73051502e-02\n",
            "  -3.73311743e-02 -1.55890067e-02  5.73205220e-02 -6.36285867e-02\n",
            "  -1.48660453e-01 -1.02777188e-01  6.92607826e-03 -8.65129188e-03]\n",
            " [-7.64695650e-02 -1.11433888e-01  1.42756932e-02 -4.66443103e-02\n",
            "  -8.87190865e-02 -8.70284508e-02  1.00000000e+00  2.34696256e-03\n",
            "  -5.19086796e-02 -8.69614129e-02 -5.42442067e-02 -8.62651932e-02\n",
            "  -1.18084164e-01 -1.34738999e-01 -1.58421483e-01  5.58360509e-03\n",
            "   6.02439280e-02 -3.97481648e-02  1.32766889e-02 -3.03183302e-02]\n",
            " [ 1.20347546e-02 -1.22335032e-01  6.26141400e-02 -1.94232597e-02\n",
            "  -3.71338714e-02 -1.23412153e-01  2.34696256e-03  1.00000000e+00\n",
            "  -2.56682074e-02 -7.75961140e-02 -1.35971989e-01 -4.86389478e-02\n",
            "  -4.41034114e-02 -9.64460728e-02 -1.63615452e-01 -9.66600390e-02\n",
            "  -8.54674368e-02 -1.01086135e-02  3.54800244e-02 -6.09888965e-02]\n",
            " [-1.23078794e-01 -3.23308013e-02 -6.58362631e-03 -3.32051344e-02\n",
            "  -5.68496325e-02 -5.64305907e-02 -5.19086796e-02 -2.56682074e-02\n",
            "   1.00000000e+00 -4.07352204e-02 -9.23615559e-02 -4.45106738e-02\n",
            "  -9.64496091e-02 -1.75076081e-01 -5.12321356e-02  3.90197002e-02\n",
            "   3.09411547e-03 -1.12080872e-01  3.13713505e-03 -5.15214357e-02]\n",
            " [ 5.01183347e-02 -1.48658882e-01 -1.46789388e-01 -3.78942961e-02\n",
            "   6.72119296e-02 -3.08561955e-02 -8.69614129e-02 -7.75961140e-02\n",
            "  -4.07352204e-02  1.00000000e+00 -3.19792758e-02 -5.66915519e-02\n",
            "  -1.44398512e-02 -2.73345281e-02 -4.74563428e-02 -3.32080069e-02\n",
            "  -5.43828829e-02 -5.84192234e-02 -1.02318072e-01 -1.03137251e-01]\n",
            " [-4.52309729e-02  1.06747625e-02 -3.40205560e-02 -1.02780153e-01\n",
            "   3.18764155e-02 -9.43874883e-02 -5.42442067e-02 -1.35971989e-01\n",
            "  -9.23615559e-02 -3.19792758e-02  1.00000000e+00 -1.38087068e-02\n",
            "  -9.72719581e-02 -6.56018517e-02 -4.72206298e-02 -7.44520699e-02\n",
            "  -1.05863040e-01  1.53346521e-02 -2.96838538e-03 -8.28014020e-02]\n",
            " [-4.09921722e-02 -1.04165838e-01 -8.36158403e-03 -5.36485468e-02\n",
            "  -3.49493110e-02 -7.73051502e-02 -8.62651932e-02 -4.86389478e-02\n",
            "  -4.45106738e-02 -5.66915519e-02 -1.38087068e-02  1.00000000e+00\n",
            "  -3.98014428e-02  1.94194635e-02 -3.46078863e-02 -1.31416441e-01\n",
            "  -2.49016833e-02 -6.25731716e-02 -1.45022762e-01 -1.01176850e-02]\n",
            " [-7.94268557e-02 -1.31940797e-02 -7.42221492e-02 -5.36575746e-02\n",
            "  -9.84270733e-02 -3.73311743e-02 -1.18084164e-01 -4.41034114e-02\n",
            "  -9.64496091e-02 -1.44398512e-02 -9.72719581e-02 -3.98014428e-02\n",
            "   1.00000000e+00  3.07708991e-03  1.81170657e-02  8.90547962e-03\n",
            "  -5.37469188e-02 -1.54764828e-01 -3.33379398e-02 -1.31158386e-02]\n",
            " [-8.16938889e-02 -1.33225777e-02 -2.95121416e-02 -4.61228992e-03\n",
            "  -1.11454430e-01 -1.55890067e-02 -1.34738999e-01 -9.64460728e-02\n",
            "  -1.75076081e-01 -2.73345281e-02 -6.56018517e-02  1.94194635e-02\n",
            "   3.07708991e-03  1.00000000e+00  1.89571337e-03 -8.94972388e-02\n",
            "  -2.18135527e-02 -3.08042707e-02 -9.44071306e-02 -6.69375983e-02]\n",
            " [-1.48264943e-02 -4.01003925e-02 -3.58813218e-04 -1.35868049e-01\n",
            "  -3.32095956e-02  5.73205220e-02 -1.58421483e-01 -1.63615452e-01\n",
            "  -5.12321356e-02 -4.74563428e-02 -4.72206298e-02 -3.46078863e-02\n",
            "   1.81170657e-02  1.89571337e-03  1.00000000e+00 -3.24981080e-03\n",
            "  -1.15949538e-01 -1.64348506e-01 -2.97109788e-02  1.70003176e-02]\n",
            " [-1.16190386e-01 -6.83295515e-02 -1.49761339e-01  6.18862360e-02\n",
            "   2.42141799e-02 -6.36285867e-02  5.58360509e-03 -9.66600390e-02\n",
            "   3.90197002e-02 -3.32080069e-02 -7.44520699e-02 -1.31416441e-01\n",
            "   8.90547962e-03 -8.94972388e-02 -3.24981080e-03  1.00000000e+00\n",
            "  -4.80257909e-02 -5.06645813e-02 -1.27865023e-01 -5.90514791e-02]\n",
            " [ 2.69768953e-03 -1.08139082e-01 -8.18447615e-02 -6.25475238e-02\n",
            "  -1.36444243e-01 -1.48660453e-01  6.02439280e-02 -8.54674368e-02\n",
            "   3.09411547e-03 -5.43828829e-02 -1.05863040e-01 -2.49016833e-02\n",
            "  -5.37469188e-02 -2.18135527e-02 -1.15949538e-01 -4.80257909e-02\n",
            "   1.00000000e+00 -2.13668629e-02 -6.98434965e-02  4.84496164e-02]\n",
            " [ 4.88644712e-02 -5.04504068e-02  2.08006305e-02 -4.27266061e-02\n",
            "  -5.03200703e-02 -1.02777188e-01 -3.97481648e-02 -1.01086135e-02\n",
            "  -1.12080872e-01 -5.84192234e-02  1.53346521e-02 -6.25731716e-02\n",
            "  -1.54764828e-01 -3.08042707e-02 -1.64348506e-01 -5.06645813e-02\n",
            "  -2.13668629e-02  1.00000000e+00 -7.01077670e-02 -3.36227862e-02]\n",
            " [ 2.13603397e-02 -5.22587041e-02 -3.52111143e-02 -9.75732831e-02\n",
            "  -1.17476134e-01  6.92607826e-03  1.32766889e-02  3.54800244e-02\n",
            "   3.13713505e-03 -1.02318072e-01 -2.96838538e-03 -1.45022762e-01\n",
            "  -3.33379398e-02 -9.44071306e-02 -2.97109788e-02 -1.27865023e-01\n",
            "  -6.98434965e-02 -7.01077670e-02  1.00000000e+00 -7.30451962e-02]\n",
            " [-9.85316665e-02 -5.46120165e-02 -1.25465342e-01 -9.12124500e-02\n",
            "  -4.34576384e-02 -8.65129188e-03 -3.03183302e-02 -6.09888965e-02\n",
            "  -5.15214357e-02 -1.03137251e-01 -8.28014020e-02 -1.01176850e-02\n",
            "  -1.31158386e-02 -6.69375983e-02  1.70003176e-02 -5.90514791e-02\n",
            "   4.84496164e-02 -3.36227862e-02 -7.30451962e-02  1.00000000e+00]] \n",
            "\n",
            "mask: \n",
            " [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]] \n",
            "\n",
            "Anisotropy: -0.053 (0=isotropic, 1=anisotropic)\n",
            "\n",
            "Anisotropy reduction: 0.012 -> -0.053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zy-X6O02HXk"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}