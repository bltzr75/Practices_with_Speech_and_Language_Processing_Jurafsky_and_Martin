{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMH_lt4DxQf1",
        "outputId": "ac7e7ec4-ba2b-4a19-89e8-1cc8fc596310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Any, List, Tuple, Dict\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "# Seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using\", device )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NumPy Indexing in Multi-Dim: Implicit Rules\n",
        "\n",
        "Consider a tensor:\n",
        "```python\n",
        "x.shape == (2, 3, 4) # [seq_len, n_heads, d_head]\n",
        "````\n",
        "\n",
        "Visual layout:\n",
        "\n",
        "```\n",
        "[\n",
        "  [   # first axis-0 element (token 0)\n",
        "    [x000, x001, x002, x003],   # head 0\n",
        "    [x010, x011, x012, x013],   # head 1\n",
        "    [x020, x021, x022, x023]    # head 2\n",
        "  ],\n",
        "\n",
        "  [   # second axis-0 element (token 1)\n",
        "    [x100, x101, x102, x103],   # head 0\n",
        "    [x110, x111, x112, x113],   # head 1\n",
        "    [x120, x121, x122, x123]    # head 2\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Missing slices are implied\n",
        "\n",
        "If fewer slices are provided than the number of dimensions, NumPy assumes full slices (`:`) for the remaining axes.\n",
        "\n",
        "```python\n",
        "x[:, 1]        # (2, 4), equivalent to x[:, 1, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x010, x011, x012, x013],   # token 0, head 1\n",
        "  [x110, x111, x112, x113]    # token 1, head 1\n",
        "]\n",
        "```\n",
        "\n",
        "```python\n",
        "x[0]          # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],   # head 0\n",
        "  [x010, x011, x012, x013],   # head 1\n",
        "  [x020, x021, x022, x023]    # head 2\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Integer indices remove dimensions\n",
        "\n",
        "An integer index selects a single element along that axis and drops the dimension.\n",
        "\n",
        "```python\n",
        "x[:, :, 2]    # (2, 3), axis 2 removed\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x002, x012, x022],   # token 0, all heads\n",
        "  [x102, x112, x122]    # token 1, all heads\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Slice (`:`) keeps dimensions\n",
        "\n",
        "Using a slice preserves the axis length.\n",
        "\n",
        "```python\n",
        "x[:, 1:2, :].shape   # (2, 1, 4)\n",
        "```\n",
        "\n",
        "Result (note the extra axis of length 1):\n",
        "\n",
        "```\n",
        "[\n",
        "  [\n",
        "    [x010, x011, x012, x013]   # head 1 kept as axis length 1\n",
        "  ],\n",
        "  [\n",
        "    [x110, x111, x112, x113]\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Trailing axes can be omitted\n",
        "\n",
        "If only the first axes are specified, NumPy assumes `:` for all remaining axes.\n",
        "\n",
        "```python\n",
        "x[0]       # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result (same as above for token 0):\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],\n",
        "  [x010, x011, x012, x013],\n",
        "  [x020, x021, x022, x023]\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Ellipsis (`...`)\n",
        "\n",
        "Ellipsis expands to the required number of `:` to cover missing dimensions.\n",
        "\n",
        "```python\n",
        "x[..., 2]   # (2, 3), equivalent to x[:, :, 2]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x002, x012, x022],   # token 0\n",
        "  [x102, x112, x122]    # token 1\n",
        "]\n",
        "```\n",
        "\n",
        "```python\n",
        "x[0, ...]   # (3, 4), equivalent to x[0, :, :]\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```\n",
        "[\n",
        "  [x000, x001, x002, x003],\n",
        "  [x010, x011, x012, x013],\n",
        "  [x020, x021, x022, x023]\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "rSkgGdYh66Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Bidirectional Attention"
      ],
      "metadata": {
        "id": "w95PQt9GygkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalAttention:\n",
        "  \"\"\"Bidirectional attention without causal mask\"\"\"\n",
        "\n",
        "  def __init__(self, d_model:int, n_heads:int):\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_model//n_heads\n",
        "\n",
        "    # Weight matrices\n",
        "    self.W_Q = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_K = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_V = np.random.randn(d_model, d_model) * 0.02\n",
        "    self.W_O = np.random.randn(d_model, d_model) * 0.02\n",
        "\n",
        "    print(f\"Bidirectional attention: d_model={d_model}, n_heads={n_heads}, d_head={self.d_head}\")\n",
        "\n",
        "  def forward(self, x:np.ndarray ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    x: [seq_len, d_model]\n",
        "    Returns: sequence output [seq_len, d_model], attention [seq_len, seq_len]\n",
        "    \"\"\"\n",
        "\n",
        "    seq_len = x.shape[0]\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = x @ self.W_Q # [seq_len, d_model]\n",
        "    K = x @ self.K_Q # [seq_len, d_model]\n",
        "    V = x @ self.V_Q # [seq_len, d_model]\n",
        "    print(f\"\\nQ,K,V shapes: {Q.shape}\")\n",
        "\n",
        "    # Reshape for the multi-head, needs [L, h, d_k]\n",
        "    Q = Q.reshape(seq_len, self.n_heads, self.d_head)  #  n_heads * d_head = d_model\n",
        "    K = K.reshape(seq_len, self.n_heads, self.d_head)\n",
        "    V = V.reshape(seq_len, self.n_heads, self.d_head)\n",
        "\n",
        "    # Compute attention without causal masking\n",
        "    scores = np.zeros(self.n_heads, seq_len, seq_len)\n",
        "\n",
        "    for head in range(self.n_heads):\n",
        "      QK = Q[:, head] @ K[:,head].T # [L, d_k] @ [d_k, L] = [L, L]\n",
        ""
      ],
      "metadata": {
        "id": "112LwtKEyIQi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oue3D_S68w1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}