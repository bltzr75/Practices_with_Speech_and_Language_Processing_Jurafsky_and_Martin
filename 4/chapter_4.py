# -*- coding: utf-8 -*-
"""Chapter_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t0U0CBNrQfJcCWVn7TzHVyNLQkLfReLx

## Import libs and downloads
"""

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Text to num feature conversion
from sklearn.naive_bayes import MultinomialNB # Standard multinomial naive Bayes described in the chapter - uses word counts/frequencies as features
from sklearn.naive_bayes import BernoulliNB #  "Multivariate Bernoulli naive Bayes" (different from binary multinomial NB) - estimates P(w|c) as the fraction of documents containing a term and includes probability for term absence
from sklearn.model_selection import train_test_split, cross_val_score # Data splitting and validation
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold



import nltk # nat lang tool kit
from nltk.corpus import stopwords

import re

import seaborn as sns
import matplotlib.pyplot as plt

import random
from scipy import stats

# Download required NLTK data (one-time setup)
nltk.download('stopwords')

"""## Text sample with ground truth/gold labels"""

# Each text represents a document to classify - EXPANDED DATASET
texts = [
    "I love this movie! It's sweet, but with satirical humor.",  # Positive sentiment
    "The dialogue is great and the adventure scenes are fun.",   # Positive sentiment
    "It was pathetic. The worst part about it was the boxing scenes.",  # Negative sentiment
    "No plot twists or great scenes. Entirely predictable.",     # Negative sentiment
    "Awesome caramel sauce and sweet toasty almonds. I love this place!",  # Positive sentiment
    "Awful pizza and ridiculously overpriced food.",            # Negative sentiment
    "Very powerful and the most fun film of the summer.",       # Positive sentiment
    "Just plain boring and lacks energy. No surprises.",        # Negative sentiment

    # Additional positive examples
    "Absolutely fantastic! Best movie I've seen this year.",     # Positive sentiment
    "Brilliant acting and wonderful cinematography throughout.",  # Positive sentiment
    "Perfect blend of comedy and drama. Highly recommend!",      # Positive sentiment
    "Outstanding performance by the lead actor. Amazing!",      # Positive sentiment
    "Incredible storyline that kept me engaged until the end.",  # Positive sentiment
    "Beautiful soundtrack and excellent direction. Loved it!",   # Positive sentiment

    # Additional negative examples
    "Terrible acting and a completely boring plot.",            # Negative sentiment
    "Worst restaurant experience ever. Food was disgusting.",   # Negative sentiment
    "Completely disappointing. Waste of time and money.",       # Negative sentiment
    "Poor service and the quality was absolutely horrible.",    # Negative sentiment
    "Annoying characters and a ridiculous storyline.",         # Negative sentiment
    "Frustrating experience. Nothing worked as expected.",      # Negative sentiment

    # Examples with negation for testing negation handling
    "Not bad, but could definitely be much better.",            # Mixed/Negative sentiment
    "I don't hate it, but it's not great either.",            # Mixed/Negative sentiment
    "Can't say I didn't enjoy parts of it.",                   # Mixed/Positive sentiment
    "It's not terrible, actually quite entertaining.",          # Positive sentiment
]

# Corresponding labels for each text (ground truth) - EXPANDED LABELS
labels = ['positive', 'positive', 'negative', 'negative',
          'positive', 'negative', 'positive', 'negative',
          # Additional labels
          'positive', 'positive', 'positive', 'positive',
          'positive', 'positive', 'negative', 'negative',
          'negative', 'negative', 'negative', 'negative',
          # Negation examples - these are subjective, adjust as needed
          'negative', 'negative', 'positive', 'positive']

print(f"Total documents: {len(texts)}")
print(f"Total labels: {len(labels)}")
print(f"Positive examples: {labels.count('positive')}")
print(f"Negative examples: {labels.count('negative')}")

"""## Basic preprocessing"""

# Basic preprocessing like the one from the book
def preprocess_text(text, remove_stopwords=False):
  """Clean and normalize text for processing"""

  text = text.lower()
  # Removing punctuation but keeping spaces and letters/numbers
  text = re.sub(r'[^\w\s]','',text) ## caret = the opposite(in this case: non words, non spaces); \w = words ; \s = whitespaces

  if remove_stopwords:
    stop_words = set(stopwords.words('english'))
    words = text.split()
    text = ' '.join([word for word in words if word not in stop_words])


  return text

processed_texts = [preprocess_text(text) for text in texts]
print("Processed texts: " )
print(processed_texts)

"""## Naive Bayes Algorithm"""

from collections import defaultdict, Counter
import math # for log operations

class NaiveBayesClassifier:
  def __init__(self, smoothing=1):
    self.smoothing = smoothing # default as laplace add-one
    self.class_priors = {} # P(c) Prob of classes
    self.word_likelihoods = defaultdict(dict) # P(w|c) Prob of the word given the class
    self.vocabulary = set() # Cardinality/size of the vocabulary. Unique words

  def train(self, texts, labels):
    """ Train the Naive Bayes classifier"""

    class_counts = Counter(labels) ## for the Prior probability. Would be: Counter({'positive': 4, 'negative': 4})
    total_docs = len(labels)

    # Calc Prior probability count(class)/total docs
    for class_label, count in class_counts.items(): # dict_items([('positive', 4), ('negative', 4)])
      self.class_priors[class_label] = count/total_docs # MLE for the probs of the classes or labels, using just the frequency
      print("self.class_priors: ", str(self.class_priors))

    class_word_counts = defaultdict(Counter) # Counts each word in each class
    class_total_words = defaultdict(int) # Total words in each class

    for text, label in zip(texts, labels):
      words = text.split() # Tokenizing by split on whitespaces
      for word in words:
        self.vocabulary.add(word)
        class_word_counts[label][word] +=1
        class_total_words[label] += 1

    # Calculate the word likelihoods P(w|c) using add-one (laplace) smoothing
    vocab_size = len(self.vocabulary) # |V|
    for class_label in class_counts:
      for word in self.vocabulary:
        count = class_word_counts[class_label][word] # Raw count of word in class
        # (count + 1) / (total_words + |V|): it is the frequency of the word in the specific class, with an adding of smoothing to not have zeros
        self.word_likelihoods[class_label][word] = (
            (count + self.smoothing) /
            (class_total_words[class_label] + vocab_size * self.smoothing)
        )


  def predict(self, text):
    """Predict the class for a given text using Naive Bayes"""

    words = text.split()
    class_scores = {} # For storing log probabilities per class, logs avoid underflow and other benefits

    ## It will go summing up the logs of the probabilities instead of the raw probabilities
    for class_label in self.class_priors:
      score = math.log(self.class_priors[class_label]) # log P(c)

      # Adding log likelihood for each word in the doc
      for word in words:
        if word in self.vocabulary: # Only known words, if not will be a problematic 0
          score += math.log(self.word_likelihoods[class_label][word])

      class_scores[class_label] = score # stores final score for the class

    print("class_scores: ", str(class_scores))
    print("class probs: ")
    print({class_label: math.exp(log_prob) for class_label, log_prob in class_scores.items()} )

    # Predicts/retrieves the class with the max probability scored from the class_scores. Argmax function
    return max(class_scores, key=class_scores.get)

nb_classifier = NaiveBayesClassifier()
nb_classifier.train(processed_texts, labels)

test_text = "This movie... is... great and fun!"
prediction = nb_classifier.predict(preprocess_text(test_text))
print(f"\nPrediction for '{test_text}': {prediction}\n")

test_text_2 = "It was boring."
prediction_2 = nb_classifier.predict(preprocess_text(test_text_2))
print(f"\nPrediction for '{test_text_2}': {prediction_2}\n")

"""## Handling Negation

"""

def handle_negation(text):
  """
  Implementation of negation handling as described in the book, section 4.4
  Add NOT_ prefix to words after negation until punctuation
  """

  # Words that indicate negation (from chapter examples)
  negation_words = ['not', 'no', 'never', 'nothing', 'nowhere', 'nobody',
                    'none', 'neither', 'nor', 'dont', "don't", 'didnt',
                    "didn't", 'wont', "won't", 'cant', "can't"]

  words = text.split() # Simple tonekizer for individual words
  result = []
  negated = False # Flag that sets scope of negation, starts with detected word and closes with punctuation


  for word in words:
    clean_word = re.sub('[^\w]', '', word.lower()) # Cleaned to detect negation from the negation_words: lowercase and remove non-word chars

    if clean_word in negation_words: ## Detected negation words
      print("Detected negation: ", word)
      negated = True # Changing flag
      result.append(word) ## Adding the original negation word as it is

    elif any(char in word for char in '.:,;!?'): ## Detected punctuation: Punctuation chars close the negated flag scope
      print("Detected punctuation: ", word)
      result.append(f"NOT_{word}" if negated else word)
      negated = False # Changing flag


    else: ## Detected common word: applying the _NOT prefic only if the negated flag is true
      result.append(f"NOT_{word}" if negated else word)

  return(' '.join(result))

# Example from the chapter
text = "didn't like this movie , but I"
negated_text = handle_negation(text)
print(f"Original: {text}")
print(f"Negated:  {negated_text}")  # Should show: didn't NOT_like NOT_this NOT_movie , but I

"""## Binary vs. Multinomial Naive Bayes"""

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(
    processed_texts, labels, # Documents and their gold labels
    test_size= 0.3,  # Splits by docs, not by words: 5 docs go to train, and 3 to test
    random_state = 42
    )

"""### Multinomial Naive Bayes (uses actual word counts)

"""

# Multinomial NB
count_vectorizer = CountVectorizer() #  implements the bag of words by ignoring word order, counting frequencies, building a vocabulary, and converting texts to numerical vectors where each dimension represents word counts
X_train_counts = count_vectorizer.fit_transform(X_train) # Creates sparse matrix for train data.  ## Example: "(0, 13) 1 means Document 0, word at vocabulary index 13, appears 1 time
X_test_counts  = count_vectorizer.transform(X_test) # Creates sparse matrix for test data

## sparse matrix
print("X_train_counts:", str(X_train_counts))  ## Example: "(0, 13) 1 means Document 0, word at vocabulary index 13, appears 1 time
print("X_test_counts:", str(X_test_counts))

multinomial_nb = MultinomialNB(alpha=1.0) # Smoothing of 1.0
multinomial_nb.fit(X_train_counts, y_train)

print("multinomial_nb trained:\n")

# Show the key attributes of the trained model
print("Classes:", multinomial_nb.classes_)  # Class labels ['negative', 'positive']

print("\nClass log priors:", multinomial_nb.class_log_prior_)  # log P(c) for each class
print("Class priors (actual):", np.exp(multinomial_nb.class_log_prior_))  # P(c)

print("\nVocabulary size:", len(count_vectorizer.vocabulary_))  # Number of unique words
print("Feature names (first 10):", count_vectorizer.get_feature_names_out()[:10])  # First 10 words

print("\nFeature log probabilities shape:", multinomial_nb.feature_log_prob_.shape)  # (classes, features)
# This is log P(word|class) for each word in each class

# Example: probability of first few words given each class
for i, class_name in enumerate(multinomial_nb.classes_):
    print(f"\n{class_name} class - first word in probability:")
    for j in range(1):
        word = count_vectorizer.get_feature_names_out()[j]
        log_prob = multinomial_nb.feature_log_prob_[i][j]
        actual_prob = np.exp(log_prob)
        print(f"  P('{word}'|{class_name}) = {actual_prob:.6f}")

"""#### Binary Naive Bayes (uses only presence/absence of words)

"""

binary_vectorizer = CountVectorizer(binary=True) # binary=True converts counts to just 0 or 1
X_train_binary = binary_vectorizer.fit_transform(X_train)
X_test_binary = binary_vectorizer.transform(X_test)

print("X_train_binary:", str(X_train_binary))
print("X_test_binary:", str(X_test_binary))

binary_nb = BernoulliNB(alpha=1.0) # Binary variant of NB
binary_nb.fit(X_train_binary, y_train)


print("binary_nb trained:\n")

# Show the key attributes of the trained BernoulliNB model
print("Classes:", binary_nb.classes_)  # Class labels ['negative', 'positive']

print("\nClass log priors:", binary_nb.class_log_prior_)  # log P(c) for each class
print("Class priors (actual):", np.exp(binary_nb.class_log_prior_))  # P(c)

print("\nVocabulary size:", len(binary_vectorizer.vocabulary_))  # Number of unique words
print("Feature names (first word):", binary_vectorizer.get_feature_names_out()[0])  # First word

print("\nFeature log probabilities shape:", binary_nb.feature_log_prob_.shape)  # (classes, features)
# This is log P(word_present|class) for binary features

# Example: probability of first word being present given each class
for i, class_name in enumerate(binary_nb.classes_):
    print(f"\n{class_name} class - first word presence probability:")
    word = binary_vectorizer.get_feature_names_out()[0]
    log_prob = binary_nb.feature_log_prob_[i][0]
    actual_prob = np.exp(log_prob)
    print(f"  P('{word}' present|{class_name}) = {actual_prob:.6f}")



"""### Compare the predictions from both approaches

"""

y_test

print("Multinomial NB predictions:", multinomial_nb.predict(X_test_counts))
print("Binary NB predictions:", binary_nb.predict(X_test_binary))

"""##Evaluation Metrics"""

def evaluate_classifier(classifier, X_test, y_test, class_names):
  """Comprehensive evaluation as described in the book"""

  y_pred = classifier.predict(X_test)

  print("\nclassification_report:")
  print(classification_report(y_test, y_pred, target_names=class_names))
  print("\n\n")

  # Precision, Recall and F1-score, support (Number of observations, helps seeing imbalance)
  precision, recall, f1, support = precision_recall_fscore_support(
    y_test, y_pred, average = None, labels = class_names)

  # Macro and Micro Averages
  macro_f1 = precision_recall_fscore_support(y_test, y_pred, average='macro')[2]
  micro_f1 = precision_recall_fscore_support(y_test, y_pred, average='micro')[2]


  print("\nEvaluation Rsults:\n")
  print("-"*50)
  for i, class_name in enumerate(class_names):
    print(f"{class_name:>10}: Precision={precision[i]:.3f}, "
          f"Recall={recall[i]:.3f}, F1-Score={f1[i]:.3f}")

  print(f"\nMacro-Averaged F1: {macro_f1:.3f}")  # Compute F1 for each class separately, then average. Better reflects performance on smaller classes.
  print(f"\nMicro-Averaged F1: {micro_f1:.3f}")  # Pool all predictions into one confusion matrix, then compute F1. Dominated by larger classes.

  # Confusion Matrix
  cm = confusion_matrix(y_test, y_pred, labels=class_names)
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
              xticklabels=class_names, yticklabels=class_names)
  plt.title('Confusion Matrix')
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')
  plt.show()

  return precision, recall, f1

### Just 24 documents gives a poor result but it is the correct one

if len(X_test) > 0:
  class_names = ['negative', 'positive']
  print("=== MULTINOMIAL NB ===")
  evaluate_classifier(multinomial_nb, X_test_counts, y_test, class_names)

  print("\n=== BINARY NB ===")
  evaluate_classifier(binary_nb, X_test_binary, y_test, class_names)

"""## Cross Validation"""

def perform_cross_validation(texts, labels, k=5):
  """K-fold cross validation as described in the book"""

  vectorizer = CountVectorizer()           # Create vectorizer
  X = vectorizer.fit_transform(texts)      # Convert texts to feature matrix
  print("X: ",str( X[:10]))

  y = np.array(labels)                     # Convert labels to numpy array
  print("Y: ", str(y))

  # Creating stratified k-fold cross-validator
  cv = StratifiedKFold(n_splits = k,
                       shuffle = True,    # Shuffle before sampling
                       random_state = 42)

  print("\n\n")
  # Test different classifiers
  classifiers = {
      'Multinomial NB': MultinomialNB(alpha=1.0 ),
      'Binary NB': BernoulliNB(alpha=1.0)
  }

  for name, classifier in classifiers.items():
    scores = cross_val_score(classifier, X, y,
                             cv = cv,
                             scoring = 'f1_macro')

    print(f"{name}: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")  # Show mean and standard deviation (confidence interval)

# Exaple
perform_cross_validation(processed_texts, labels, k=3)  # Use k=3 for small dataset

"""#####Multinomial NB slightly wins by 0.019 points
#####Both perform similarly
#####Difference is small and likely not statistically significant
##### Adding more data is required

##Statistical Significance Testing
"""

def bootstrap_test(scores_a, scores_b, n_bootstrap=1000):
  """
  Simplified bootstrap test for comparing two classifiers
  Paired bootstrap based on the book
  """

  observed_diff = np.mean(scores_a) - np.mean(scores_b) # δ(x) The Effect Size

  # Bootstrap resampling procedure
  bootstrap_diffs = []
  combined_scores = list(zip(scores_a, scores_b))

  for _ in range(n_bootstrap):
    # Resample with replacement
    resampled = random.choices( combined_scores, k = len(combined_scores))
    resampled_a, resampled_b = zip(*resampled)

    diff = np.mean(resampled_a) - np.mean(resampled_b) ## Checking again the difference to verify if it is still the same δ(x)
    bootstrap_diffs.append(diff)

  # Counting how often bootstrap difference ≥ 2 * observed difference as appears in the book
  extreme_count = sum(1 for diff in bootstrap_diffs if diff >= 2 * observed_diff)
  print("extreme_count: ", str(extreme_count))

  # Calculation of P-Value
  p_value = extreme_count / n_bootstrap # proportion of extreme cases

  return observed_diff, p_value

# Example usage (would need actual score data from multiple test runs)
scores_classifier_a = [0.85, 0.82, 0.88, 0.86, 0.84]  # Performance scores for classifier A
scores_classifier_b = [0.2, 0.82, 0.7, 0.7, 0.34]  # Performance scores for classifier B
diff, p_val = bootstrap_test(scores_classifier_a, scores_classifier_b)
print(f"Observed difference: {diff:.3f}, p-value: {p_val:.3f}")
if p_val <= 0.01: print("There is a statistically significant difference") ## Although some authors recommend even a lower threshold for NLP

"""## Sentiment Lexicons"""

positive_words = {
    'great', 'awesome', 'fantastic', 'excellent', 'wonderful',
    'amazing', 'brilliant', 'outstanding', 'perfect', 'love',
    'beautiful', 'incredible', 'superb', 'marvelous', 'fabulous'
}

negative_words = {
    'terrible', 'awful', 'horrible', 'bad', 'worst',
    'pathetic', 'disgusting', 'hate', 'ridiculous', 'boring',
    'stupid', 'useless', 'disappointing', 'annoying', 'frustrating'
}

def lexicon_features(text):
  """Extract lexicon-based features from text"""

  words = text.lower().split()

  # Count of positive and negative words
  pos_count = sum(1 for word in words if word in positive_words)
  neg_count = sum(1 for word in words if word in negative_words)

  # Retrieves feature dictionary
  return{
      'positive_word_count': pos_count,
      'negative_word_count': neg_count,
      'sentiment_score': pos_count - neg_count
  }

# Example usage
text = "This movie is awesome but the ending was terrible"
features = lexicon_features(text)
print("Lexicon features:", features)  # Should show: positive=1, negative=1, score=0

"""## Complete Pipeline Example"""

class SentimentClassifier:
  """Complete sentiment classification pipeline"""

  def __init__(self, use_negation=True, use_binary=True, remove_stopwords=False):
    self.use_negation = use_negation # Negation handling. Adding prefix NOT_
    self.use_binary = use_binary # Binary NB
    self.remove_stopwords = remove_stopwords

    self.vectorizer = CountVectorizer(binary=use_binary) # Binary vectorizer

    self.classifier = BernoulliNB() if use_binary else MultinomialNB()

    if self.remove_stopwords:
      self.stop_words = set(stopwords.words('english'))


  def preprocess(self, texts):
    """Applying preprocessing with negation handling included"""

    processed = []

    for text in texts:

      if self.use_negation:
        text = handle_negation(text) # Handle negation while punctuation is still present


      text = re.sub('[^\w\s]', '', text.lower()) # Removes the complement of chars and spaces, and lowercases the rest


      if self.remove_stopwords:
        words = text.split()
        text = ' '.join([word for word in words if word not in self.stop_words])

      processed.append(text)

    print("\nprocessed: ", str(processed),"\n\n")
    return(processed)



  def train(self, texts, labels):
    """Train classifier on provided data"""

    processed_texts = self.preprocess(texts)
    X = self.vectorizer.fit_transform(processed_texts)
    self.classifier.fit(X, labels)

  def predict(self, texts):
    """Make predictions on new data"""
    processed_texts = self.preprocess(texts)
    X = self.vectorizer.transform(processed_texts)
    return self.classifier.predict(X)

  def predict_probability(self, texts):
    """Get the prediction probabilities per each class"""
    processed_texts = self.preprocess(texts)
    X = self.vectorizer.transform(processed_texts)
    return self.classifier.predict_proba(X)

"""## Example"""

classifier = SentimentClassifier(use_negation=True, use_binary=True)  # Enable both optimizations
classifier.train(texts, labels)

## Generated complex examples
new_texts = [
    # === NEGATION EXAMPLES ===
    "I don't like this movie",                          # Simple negation - negative
    "This movie is not bad",                            # Negated negative word - positive
    "I can't say I didn't enjoy it",                    # Double negation - positive
    "It's not terrible, actually quite good",           # Negation + positive - positive
    "Never seen anything so boring",                    # Negation with boring - negative
    "Nothing about this film worked",                   # Nothing negation - negative
    "Won't recommend this to anyone",                   # Won't negation - negative

    # === CLEAR POSITIVE EXAMPLES ===
    "This is a great film!",                           # Clear positive
    "Absolutely fantastic movie experience",            # Strong positive
    "Love the amazing cinematography and acting",       # Positive with specific praise
    "Perfect blend of drama and comedy",               # Positive description

    # === CLEAR NEGATIVE EXAMPLES ===
    "Terrible acting and boring plot",                 # Clear negative
    "Waste of time and money",                         # Strong negative
    "Completely disappointing experience",              # Negative experience
    "The worst movie I've ever seen",                 # Superlative negative

    # === MIXED/NEUTRAL EXAMPLES ===
    "Not bad, but could be better",                    # Mixed with negation
    "It was okay, nothing special",                    # Neutral/lukewarm
    "Good acting but terrible story",                  # Mixed positive/negative
    "Started well but ended poorly",                   # Mixed temporal sentiment

    # === DOMAIN VARIETY ===
    "The restaurant food was not good",                # Restaurant + negation
    "Amazing service and delicious food",              # Restaurant positive
    "Hotel room was dirty and overpriced",            # Hotel negative
    "Can't complain about the excellent service",      # Service + double negative

    # === EDGE CASES ===
    "Not not good",                                    # Double negation edge case
    "It's not that I don't like it",                  # Complex negation
    "I love that it's not predictable",               # Positive about negated quality
    "This doesn't suck",                              # Informal negated negative
]

predictions = classifier.predict(new_texts)        # Get class predictions
probabilities = classifier.predict_probability(new_texts)  # Get probability distributions

# Display results with detailed formatting
for text, pred, prob in zip(new_texts, predictions, probabilities):
  print(f"Text: '{text}'")
  print(f"Prediction: {pred}")
  # Create probability dictionary mapping class names to probabilities
  prob_dict = dict(zip(classifier.classifier.classes_, prob))
  print(f"Probabilities: {prob_dict}")
  print("-" * 50)

