# -*- coding: utf-8 -*-
"""Chapter_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t0U0CBNrQfJcCWVn7TzHVyNLQkLfReLx

## Import libs and downloads
"""

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Text to num feature conversion
from sklearn.naive_bayes import MultinomialNB # Standard multinomial naive Bayes described in the chapter - uses word counts/frequencies as features
from sklearn.naive_bayes import BernoulliNB #  "Multivariate Bernoulli naive Bayes" (different from binary multinomial NB) - estimates P(w|c) as the fraction of documents containing a term and includes probability for term absence
from sklearn.model_selection import train_test_split, cross_val_score # Data splitting and validation
from sklearn.metrics import classification_report, confusion_matrix

import nltk # nat lang tool kit
from nltk.corpus import stopwords

import re

# Download required NLTK data (one-time setup)
nltk.download('stopwords')

"""## Text sample with ground truth/gold labels"""

# Each text represents a document to classify
texts = [
    "I love this movie!.... It's sweet, but with satirical humor.",  # Positive sentiment
    "The dialogue is great and the adventure scenes are fun.",   # Positive sentiment
    "It was pathetic. The worst part about it was the boxing scenes.",  # Negative sentiment
    "No plot twists or great scenes. Entirely predictable.",     # Negative sentiment
    "Awesome caramel sauce and sweet toasty almonds. I love this place!",  # Positive sentiment
    "Awful pizza and ridiculously overpriced food.",            # Negative sentiment
    "Very powerful and the most fun film of the summer.",       # Positive sentiment
    "Just plain boring and lacks energy. No surprises."         # Negative sentiment
]

# Corresponding labels for each text (ground truth)
labels = ['positive', 'positive', 'negative', 'negative',
          'positive', 'negative', 'positive', 'negative']

"""## Basic preprocessing"""

# Basic preprocessing like the one from the book
def preprocess_text(text):
  """Clean and normalize text for processing"""

  text = text.lower()
  # Removing punctuation but keeping spaces and letters/numbers
  text = re.sub(r'[^\w\s]','',text) ## caret = the opposite(in this case: non words, non spaces); \w = words ; \s = whitespaces
  return text

processed_texts = [preprocess_text(text) for text in texts]
print("Processed texts: " )
print(processed_texts)

"""## Naive Bayes Algorithm"""

from collections import defaultdict, Counter
import math # for log operations

class NaiveBayesClassifier:
  def __init__(self, smoothing=1):
    self.smoothing = smoothing # default as laplace add-one
    self.class_priors = {} # P(c) Prob of classes
    self.word_likelihoods = defaultdict(dict) # P(w|c) Prob of the word given the class
    self.vocabulary = set() # Cardinality/size of the vocabulary. Unique words

  def train(self, texts, labels):
    """ Train the Naive Bayes classifier"""

    class_counts = Counter(labels) ## for the Prior probability. Would be: Counter({'positive': 4, 'negative': 4})
    total_docs = len(labels)

    # Calc Prior probability count(class)/total docs
    for class_label, count in class_counts.items(): # dict_items([('positive', 4), ('negative', 4)])
      self.class_priors[class_label] = count/total_docs # MLE for the probs of the classes or labels, using just the frequency
      print("self.class_priors: ", str(self.class_priors))

    class_word_counts = defaultdict(Counter) # Counts each word in each class
    class_total_words = defaultdict(int) # Total words in each class

    for text, label in zip(texts, labels):
      words = text.split() # Tokenizing by split on whitespaces
      for word in words:
        self.vocabulary.add(word)
        class_word_counts[label][word] +=1
        class_total_words[label] += 1

    # Calculate the word likelihoods P(w|c) using add-one (laplace) smoothing
    vocab_size = len(self.vocabulary) # |V|
    for class_label in class_counts:
      for word in self.vocabulary:
        count = class_word_counts[class_label][word] # Raw count of word in class
        # (count + 1) / (total_words + |V|): it is the frequency of the word in the specific class, with an adding of smoothing to not have zeros
        self.word_likelihoods[class_label][word] = (
            (count + self.smoothing) /
            (class_total_words[class_label] + vocab_size * self.smoothing)
        )


  def predict(self, text):
    """Predict the class for a given text using Naive Bayes"""

    words = text.split()
    class_scores = {} # For storing log probabilities per class, logs avoid underflow and other benefits

    ## It will go summing up the logs of the probabilities instead of the raw probabilities
    for class_label in self.class_priors:
      score = math.log(self.class_priors[class_label]) # log P(c)

      # Adding log likelihood for each word in the doc
      for word in words:
        if word in self.vocabulary: # Only known words, if not will be a problematic 0
          score += math.log(self.word_likelihoods[class_label][word])

      class_scores[class_label] = score # stores final score for the class

    print("class_scores: ", str(class_scores))
    print("class probs: ")
    print({class_label: math.exp(log_prob) for class_label, log_prob in class_scores.items()} )

    # Predicts/retrieves the class with the max probability scored from the class_scores. Argmax function
    return max(class_scores, key=class_scores.get)

nb_classifier = NaiveBayesClassifier()
nb_classifier.train(processed_texts, labels)

test_text = "This movie... is... great and fun!"
prediction = nb_classifier.predict(preprocess_text(test_text))
print(f"\nPrediction for '{test_text}': {prediction}\n")

test_text_2 = "It was boring."
prediction_2 = nb_classifier.predict(preprocess_text(test_text_2))
print(f"\nPrediction for '{test_text_2}': {prediction_2}\n")

"""## Handling Negation

"""

def handle_negation(text):
  """
  Implementation of negation handling as described in the book, section 4.4
  Add NOT_ prefix to words after negation until punctuation
  """

  # Words that indicate negation (from chapter examples)
  negation_words = ['not', 'no', 'never', 'nothing', 'nowhere', 'nobody',
                    'none', 'neither', 'nor', 'dont', "don't", 'didnt',
                    "didn't", 'wont', "won't", 'cant', "can't"]

  words = text.split() # Simple tonekizer for individual words
  result = []
  negated = False # Flag that sets scope of negation, starts with detected word and closes with punctuation


  for word in words:
    clean_word = re.sub('[^\w]', '', word.lower()) # Cleaned to detect negation from the negation_words: lowercase and remove non-word chars

    if clean_word in negation_words: ## Detected negation words
      print("Detected negation: ", word)
      negated = True # Changing flag
      result.append(word) ## Adding the original negation word as it is

    elif any(char in word for char in '.:,;!?'): ## Detected punctuation: Punctuation chars close the negated flag scope
      print("Detected punctuation: ", word)
      result.append(f"NOT_{word}" if negated else word)
      negated = False # Changing flag


    else: ## Detected common word: applying the _NOT prefic only if the negated flag is true
      result.append(f"NOT_{word}" if negated else word)

  return(' '.join(result))

# Example from the chapter
text = "didn't like this movie , but I"
negated_text = handle_negation(text)
print(f"Original: {text}")
print(f"Negated:  {negated_text}")  # Should show: didn't NOT_like NOT_this NOT_movie , but I

"""## Binary vs. Multinomial Naive Bayes"""

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(
    processed_texts, labels, # Documents and their gold labels
    test_size= 0.3,  # Splits by docs, not by words: 5 docs go to train, and 3 to test
    random_state = 42
    )

"""### Multinomial Naive Bayes (uses actual word counts)

"""

# Multinomial NB
count_vectorizer = CountVectorizer() #  implements the bag of words by ignoring word order, counting frequencies, building a vocabulary, and converting texts to numerical vectors where each dimension represents word counts
X_train_counts = count_vectorizer.fit_transform(X_train) # Creates sparse matrix for train data.  ## Example: "(0, 13) 1 means Document 0, word at vocabulary index 13, appears 1 time
X_test_counts  = count_vectorizer.transform(X_test) # Creates sparse matrix for test data

## sparse matrix
print("X_train_counts:", str(X_train_counts))  ## Example: "(0, 13) 1 means Document 0, word at vocabulary index 13, appears 1 time
print("X_test_counts:", str(X_test_counts))

multinomial_nb = MultinomialNB(alpha=1.0) # Smoothing of 1.0
multinomial_nb.fit(X_train_counts, y_train)

print("multinomial_nb trained:\n")

# Show the key attributes of the trained model
print("Classes:", multinomial_nb.classes_)  # Class labels ['negative', 'positive']

print("\nClass log priors:", multinomial_nb.class_log_prior_)  # log P(c) for each class
print("Class priors (actual):", np.exp(multinomial_nb.class_log_prior_))  # P(c)

print("\nVocabulary size:", len(count_vectorizer.vocabulary_))  # Number of unique words
print("Feature names (first 10):", count_vectorizer.get_feature_names_out()[:10])  # First 10 words

print("\nFeature log probabilities shape:", multinomial_nb.feature_log_prob_.shape)  # (classes, features)
# This is log P(word|class) for each word in each class

# Example: probability of first few words given each class
for i, class_name in enumerate(multinomial_nb.classes_):
    print(f"\n{class_name} class - first word in probability:")
    for j in range(1):
        word = count_vectorizer.get_feature_names_out()[j]
        log_prob = multinomial_nb.feature_log_prob_[i][j]
        actual_prob = np.exp(log_prob)
        print(f"  P('{word}'|{class_name}) = {actual_prob:.6f}")

"""#### Binary Naive Bayes (uses only presence/absence of words)

"""

binary_vectorizer = CountVectorizer(binary=True) # binary=True converts counts to just 0 or 1
X_train_binary = binary_vectorizer.fit_transform(X_train)
X_test_binary = binary_vectorizer.transform(X_test)

print("X_train_binary:", str(X_train_binary))
print("X_test_binary:", str(X_test_binary))

binary_nb = BernoulliNB(alpha=1.0) # Binary variant of NB
binary_nb.fit(X_train_binary, y_train)


print("binary_nb trained:\n")

# Show the key attributes of the trained BernoulliNB model
print("Classes:", binary_nb.classes_)  # Class labels ['negative', 'positive']

print("\nClass log priors:", binary_nb.class_log_prior_)  # log P(c) for each class
print("Class priors (actual):", np.exp(binary_nb.class_log_prior_))  # P(c)

print("\nVocabulary size:", len(binary_vectorizer.vocabulary_))  # Number of unique words
print("Feature names (first word):", binary_vectorizer.get_feature_names_out()[0])  # First word

print("\nFeature log probabilities shape:", binary_nb.feature_log_prob_.shape)  # (classes, features)
# This is log P(word_present|class) for binary features

# Example: probability of first word being present given each class
for i, class_name in enumerate(binary_nb.classes_):
    print(f"\n{class_name} class - first word presence probability:")
    word = binary_vectorizer.get_feature_names_out()[0]
    log_prob = binary_nb.feature_log_prob_[i][0]
    actual_prob = np.exp(log_prob)
    print(f"  P('{word}' present|{class_name}) = {actual_prob:.6f}")



"""### Compare predictions from both approaches

"""

y_test

print("Multinomial NB predictions:", multinomial_nb.predict(X_test_counts))
print("Binary NB predictions:", binary_nb.predict(X_test_binary))

